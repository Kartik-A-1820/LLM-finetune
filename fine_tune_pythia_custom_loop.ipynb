{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1964da6d-706f-4fad-9fa1-6ab523941174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, DataCollatorForLanguageModeling\n",
    "from peft import PeftModel, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "import evaluate \n",
    "import os \n",
    "import numpy as np\n",
    "from transformers import default_data_collator\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "752c651d-cca9-4efb-9131-f67f027b3fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/pythia-410m\"\n",
    "saved_path = \"./pythia-lora-final\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afe646cd-191b-4fd6-99e5-ce1b02042be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    \"train\": [\n",
    "        # \"data/datascience_2000.jsonl\",\n",
    "        \"data/datascience_1000_multistep.jsonl\",\n",
    "        \"data/datascience_4000_multistep.jsonl\",\n",
    "        # \"data/datascience_1000_errors.jsonl\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09236c28-cf06-4318-baf3-de8d9d6e080c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04510c6ec8b4dd2a74078c7b9ba13bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_ds = load_dataset(\"json\", data_files=data_files, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "052f11a7-9c6f-4554-8f90-37ce21c14b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5015dd7-e1ab-462a-a044-6cf76844bc78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': \"Read CSV file 'time_series.csv' into DataFrame data, drop duplicates, fill missing values in 'quantity' with median, group by 'department' and plot average 'speed'.\",\n",
       " 'output': \"import pandas as pd\\nimport matplotlib.pyplot as plt\\ndata = pd.read_csv('time_series.csv')\\ndata = data.drop_duplicates()\\ndata['quantity'] = data['quantity'].fillna(data['quantity'].median())\\navg = data.groupby('department')['speed'].mean()\\navg.plot(kind='bar')\\nplt.title('Average speed by department')\\nplt.show()\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70c3e56c-1964-44fc-8b4b-af0b8d9352a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(saved_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8f7a419-1564-48f3-96d4-28da54ddbc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(ex):\n",
    "    prompt = f\"### Instruction:\\n{ex['instruction']}\\n\\n### Response:\\n{ex['output']}\"\n",
    "    prompt += tokenizer.eos_token\n",
    "    return {\"text\": prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f946b46-e601-4880-ba46-57c9c2d3f7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e95404a1f074c2e89e10bb3355f0cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = raw_ds.map(format_example, remove_columns=raw_ds.column_names)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63f10499-45ea-44d8-9b12-2d584cebbb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"### Instruction:\\nRead CSV file 'time_series.csv' into DataFrame data, drop duplicates, fill missing values in 'quantity' with median, group by 'department' and plot average 'speed'.\\n\\n### Response:\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\ndata = pd.read_csv('time_series.csv')\\ndata = data.drop_duplicates()\\ndata['quantity'] = data['quantity'].fillna(data['quantity'].median())\\navg = data.groupby('department')['speed'].mean()\\navg.plot(kind='bar')\\nplt.title('Average speed by department')\\nplt.show()<|endoftext|>\"}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d14b6579-d1e8-456d-b046-0dd0e95f76a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47b60b0468f409483e83f8d8a441bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id == tokenizer.eos_token_id\n",
    "\n",
    "def tokenize_fn(ex):\n",
    "    return tokenizer(\n",
    "        ex[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=192,\n",
    "        padding=\"max_length\",  # will use pad_token_id that you already set\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "dataloader = DataLoader(tokenized_dataset, batch_size=16, shuffle=True, collate_fn=default_data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27e527ab-efad-470f-b07e-298d41487a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "base_model = prepare_model_for_kbit_training(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c269948-902f-481c-9f97-d3af9b19b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Load time series CSV 'data.csv', set 'date' as index, resample weekly average of 'sales', detect points > mean+2*std and annotate plot.\"\n",
    "prompt_template = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06bbcfc0-dcde-468a-a479-6a51f095ef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopOnKeywords(StoppingCriteria):\n",
    "    def __init__(self, tokenizer, stop_phrases, input_len):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stop_phrases = stop_phrases\n",
    "        self.input_len = input_len\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        generated_text = self.tokenizer.decode(input_ids[0][self.input_len:], skip_special_tokens=True)\n",
    "        for stop_phrase in self.stop_phrases:\n",
    "            if stop_phrase in generated_text:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83833aaf-1844-44c1-a6b9-7c213bc330af",
   "metadata": {},
   "source": [
    "# Base Model Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc417fa7-1a0f-4e19-b88d-8262b11c74f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Load time series CSV 'data.csv', set 'date' as index, resample weekly average of 'sales', detect points > mean+2*std and annotate plot.\n",
      "\n",
      "### Response:\n",
      "\n",
      "```\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stop_phrases = [\"### Response:\", \"\\n\\n\"]\n",
    "inputs = tokenizer(prompt_template, return_tensors=\"pt\").to(\"cuda\")\n",
    "input_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([\n",
    "    StopOnKeywords(tokenizer, stop_phrases, input_len)\n",
    "])\n",
    "tokens = base_model.generate(**inputs, max_new_tokens=192, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.eos_token_id, stopping_criteria=stopping_criteria)\n",
    "print(tokenizer.decode(tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b1e5fd-91cd-4c2e-afcb-26bcc7b89427",
   "metadata": {},
   "source": [
    "# Lora Model V1: Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9bbd1b6-2d4c-4228-94f9-f15489e1531c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Load time series CSV 'data.csv', set 'date' as index, resample weekly average of 'sales', detect points > mean+2*std and annotate plot.\n",
      "\n",
      "### Response:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "df_data = pd.read_csv('data.csv', parse_dates=['date'], index_col='date')\n",
      "weekly = df_data['sales'].resample('W').mean()\n",
      "mean = weekly.mean(); std = weekly.std()\n",
      "signal = weekly[weekly > mean + 2*std]\n",
      "plt.plot(weekly.index, weekly)\n",
      "plt.scatter(signal.index, signal, color='red')\n",
      "plt.title('Weekly sales with Anomalies')\n",
      "plt.show()\n",
      "odf_data.to_csv('omal_data.csv', index=False)\n",
      "plt.show()\n",
      "plt.show()\n",
      "odf_data.to_csv('omal_data.csv', index=False)\n",
      "plt. finish()\n",
      "odf_\n"
     ]
    }
   ],
   "source": [
    "model = PeftModel.from_pretrained(base_model, saved_path)\n",
    "model = model.to(device)\n",
    "\n",
    "model.enable_input_require_grads()\n",
    "model.config.use_cache = False\n",
    "\n",
    "inputs = tokenizer(prompt_template, return_tensors=\"pt\").to(\"cuda\")\n",
    "tokens = base_model.generate(**inputs, max_new_tokens=192, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.eos_token_id, stopping_criteria=stopping_criteria)\n",
    "print(tokenizer.decode(tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d04610-d9ca-4d06-b972-5c755b72cab7",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5215b56-42a7-49d1-b13e-a457c593880d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567.517216\n"
     ]
    }
   ],
   "source": [
    "lora_cfg = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_cfg)\n",
    "print(model.get_memory_footprint()/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c39b3eb-75a4-45ca-921a-6b49d538fb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f17847c-12e5-420e-9083-c9eff1e3f3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████████████████████████████| 313/313 [36:38<00:00,  7.02s/it, loss=0.0907]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Inference Output ===\n",
      "### Instruction:\n",
      "Load time series CSV 'data.csv', set 'date' as index, resample weekly average of 'sales', detect points > mean+2*std and annotate plot.\n",
      "\n",
      "### Response:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "df1 = pd.read_csv('data.csv', parse_dates=['date'], index_col='date')\n",
      "weekly = df1['sales'].resample('W').mean()\n",
      "mean = weekly.mean(); std = weekly.std()\n",
      "signal = weekly[weekly > mean + 2*std]\n",
      "plt.plot(weekly.index, weekly)\n",
      "plt.scatter(signal.index, signal, color='red')\n",
      "plt.title('Weekly sales with Anomalies')\n",
      "plt.show()\n",
      "========================\n",
      "\n",
      "Epoch 1 completed. Avg loss: 0.3139\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████████████████████████████████████████████████████| 313/313 [36:51<00:00,  7.07s/it, loss=0.0845]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Inference Output ===\n",
      "### Instruction:\n",
      "Load time series CSV 'data.csv', set 'date' as index, resample weekly average of 'sales', detect points > mean+2*std and annotate plot.\n",
      "\n",
      "### Response:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "df1 = pd.read_csv('data.csv', parse_dates=['date'], index_col='date')\n",
      "weekly = df1['sales'].resample('W').mean()\n",
      "mean = weekly.mean(); std = weekly.std()\n",
      "signal = weekly[weekly > mean + 2*std]\n",
      "plt.plot(weekly.index, weekly)\n",
      "plt.scatter(signal.index, signal, color='red')\n",
      "plt.title('Weekly sales with Anomalies')\n",
      "plt.show()\n",
      "========================\n",
      "\n",
      "Epoch 2 completed. Avg loss: 0.0879\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████████████████████████████████████████████████████| 313/313 [36:53<00:00,  7.07s/it, loss=0.0801]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Inference Output ===\n",
      "### Instruction:\n",
      "Load time series CSV 'data.csv', set 'date' as index, resample weekly average of 'sales', detect points > mean+2*std and annotate plot.\n",
      "\n",
      "### Response:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "df1 = pd.read_csv('data.csv', parse_dates=['date'], index_col='date')\n",
      "weekly = df1['sales'].resample('W').mean()\n",
      "mean = weekly.mean(); std = weekly.std()\n",
      "signal = weekly[weekly > mean + 2*std]\n",
      "plt.plot(weekly.index, weekly)\n",
      "plt.scatter(signal.index, signal, color='red')\n",
      "plt.title('Weekly sales with Anomalies')\n",
      "plt.show()\n",
      "========================\n",
      "\n",
      "Epoch 3 completed. Avg loss: 0.0816\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████████████████████████████████████████████████████| 313/313 [36:53<00:00,  7.07s/it, loss=0.0855]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Inference Output ===\n",
      "### Instruction:\n",
      "Load time series CSV 'data.csv', set 'date' as index, resample weekly average of 'sales', detect points > mean+2*std and annotate plot.\n",
      "\n",
      "### Response:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "data = pd.read_csv('data.csv', parse_dates=['date'], index_col='date')\n",
      "weekly = data['sales'].resample('W').mean()\n",
      "mean = weekly.mean(); std = weekly.std()\n",
      "signal = weekly[weekly > mean + 2*std]\n",
      "plt.plot(weekly.index, weekly)\n",
      "plt.scatter(signal.index, signal, color='red')\n",
      "plt.title('Weekly sales with Anomalies')\n",
      "plt.show()\n",
      "========================\n",
      "\n",
      "Epoch 4 completed. Avg loss: 0.0793\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|████████████████████████████████████████████████████████| 313/313 [8:13:56<00:00, 94.69s/it, loss=0.0803]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Inference Output ===\n",
      "### Instruction:\n",
      "Load time series CSV 'data.csv', set 'date' as index, resample weekly average of 'sales', detect points > mean+2*std and annotate plot.\n",
      "\n",
      "### Response:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "df = pd.read_csv('data.csv', parse_dates=['date'], index_col='date')\n",
      "weekly = df['sales'].resample('W').mean()\n",
      "mean = weekly.mean(); std = weekly.std()\n",
      "signal = weekly[weekly > mean + 2*std]\n",
      "plt.plot(weekly.index, weekly)\n",
      "plt.scatter(signal.index, signal, color='red')\n",
      "plt.title('Weekly sales with Anomalies')\n",
      "plt.show()\n",
      "========================\n",
      "\n",
      "Epoch 5 completed. Avg loss: 0.0772\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████████████████████████████████████████████████████| 313/313 [21:45<00:00,  4.17s/it, loss=0.0707]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Inference Output ===\n",
      "### Instruction:\n",
      "Load time series CSV 'data.csv', set 'date' as index, resample weekly average of 'sales', detect points > mean+2*std and annotate plot.\n",
      "\n",
      "### Response:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "df1 = pd.read_csv('data.csv', parse_dates=['date'], index_col='date')\n",
      "weekly = df1['sales'].resample('W').mean()\n",
      "mean = weekly.mean(); std = weekly.std()\n",
      "signal = weekly[weekly > mean + 2*std]\n",
      "plt.plot(weekly.index, weekly)\n",
      "plt.scatter(signal.index, signal, color='red')\n",
      "plt.title('Weekly sales with Anomalies')\n",
      "plt.show()\n",
      "========================\n",
      "\n",
      "Epoch 6 completed. Avg loss: 0.0767\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████████████████████████████████████████████████████| 313/313 [21:48<00:00,  4.18s/it, loss=0.0790]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Inference Output ===\n",
      "### Instruction:\n",
      "Load time series CSV 'data.csv', set 'date' as index, resample weekly average of 'sales', detect points > mean+2*std and annotate plot.\n",
      "\n",
      "### Response:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "df_data = pd.read_csv('data.csv', parse_dates=['date'], index_col='date')\n",
      "weekly = df_data['sales'].resample('W').mean()\n",
      "mean = weekly.mean(); std = weekly.std()\n",
      "signal = weekly[weekly > mean + 2*std]\n",
      "plt.plot(weekly.index, weekly)\n",
      "plt.scatter(signal.index, signal, color='red')\n",
      "plt.title('Weekly sales with Anomalies')\n",
      "plt.show()\n",
      "========================\n",
      "\n",
      "Epoch 7 completed. Avg loss: 0.0759\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████████████████████████████████████████████████████| 313/313 [21:47<00:00,  4.18s/it, loss=0.0725]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Inference Output ===\n",
      "### Instruction:\n",
      "Load time series CSV 'data.csv', set 'date' as index, resample weekly average of 'sales', detect points > mean+2*std and annotate plot.\n",
      "\n",
      "### Response:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "df1 = pd.read_csv('data.csv', parse_dates=['date'], index_col='date')\n",
      "weekly = df1['sales'].resample('W').mean()\n",
      "mean = weekly.mean(); std = weekly.std()\n",
      "signal = weekly[weekly > mean + 2*std]\n",
      "plt.plot(weekly.index, weekly)\n",
      "plt.scatter(signal.index, signal, color='red')\n",
      "plt.title('Weekly sales with Anomalies')\n",
      "plt.show()\n",
      "========================\n",
      "\n",
      "Epoch 8 completed. Avg loss: 0.0757\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████████████████████████████████████████████████████| 313/313 [21:46<00:00,  4.17s/it, loss=0.0758]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Inference Output ===\n",
      "### Instruction:\n",
      "Load time series CSV 'data.csv', set 'date' as index, resample weekly average of 'sales', detect points > mean+2*std and annotate plot.\n",
      "\n",
      "### Response:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "data_clean = pd.read_csv('data.csv', parse_dates=['date'], index_col='date')\n",
      "weekly = data_clean['sales'].resample('W').mean()\n",
      "mean = weekly.mean(); std = weekly.std()\n",
      "signal = weekly[weekly > mean + 2*std]\n",
      "plt.plot(weekly.index, weekly)\n",
      "plt.scatter(signal.index, signal, color='red')\n",
      "plt.title('Weekly sales with Anomalies')\n",
      "plt.show()\n",
      "========================\n",
      "\n",
      "Epoch 9 completed. Avg loss: 0.0763\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|█████████████████████████████████████████████████████████| 313/313 [21:43<00:00,  4.17s/it, loss=0.0744]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Inference Output ===\n",
      "### Instruction:\n",
      "Load time series CSV 'data.csv', set 'date' as index, resample weekly average of 'sales', detect points > mean+2*std and annotate plot.\n",
      "\n",
      "### Response:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "dataset = pd.read_csv('data.csv', parse_dates=['date'], index_col='date')\n",
      "weekly = dataset['sales'].resample('W').mean()\n",
      "mean = weekly.mean(); std = weekly.std()\n",
      "signal = weekly[weekly > mean + 2*std]\n",
      "plt.plot(weekly.index, weekly)\n",
      "plt.scatter(signal.index, signal, color='red')\n",
      "plt.title('Weekly sales with Anomalies')\n",
      "plt.show()\n",
      "========================\n",
      "\n",
      "Epoch 10 completed. Avg loss: 0.0749\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    for step, batch in progress_bar:\n",
    "        model.train()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Update tqdm bar with current loss\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt_template, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        tokens = model.generate(**inputs, max_new_tokens=192, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.eos_token_id, stopping_criteria=stopping_criteria)\n",
    "    print(\"=== Inference Output ===\")\n",
    "    print(tokenizer.decode(tokens[0], skip_special_tokens=True))\n",
    "    print(\"========================\")\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"\\nEpoch {epoch+1} completed. Avg loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fe9a734-16a1-4234-85ed-c91ea95d9ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./pythia-lora-V3\\\\tokenizer_config.json',\n",
       " './pythia-lora-V3\\\\special_tokens_map.json',\n",
       " './pythia-lora-V3\\\\tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Save Fine-Tuned Weights\n",
    "model.save_pretrained(\"./pythia-lora-V3\")\n",
    "tokenizer.save_pretrained(\"./pythia-lora-V3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05383224-f72e-4e8d-a139-2a61b20bf248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch_kernel)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "013eca3801f94c03b3d037ee097fff7b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "092a7b057a7a4145bca0be4a4b30905d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0a8632b13eb544849f6d624d6f0cd2be": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0aa65b1a765f412faf6ce36add4b14e5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0c835ad5701048bc9b33167daee40f93": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1a7c327860004954be24583e059a8a3a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3f9ce0cdf04d45a8aaf32d4f849d14e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_46dc7e8963f9462f8b16ecbb267247a1",
       "style": "IPY_MODEL_7b89546ed20e4e03b29f339a0943d460",
       "value": " 5000/5000 [00:01&lt;00:00, 5689.71 examples/s]"
      }
     },
     "43e64c0614a646209c24fe971ea9b365": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "20px"
      }
     },
     "46dc7e8963f9462f8b16ecbb267247a1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "49dd29991d314e64ae45541e45ee0980": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5b1305fd5e1d445ca909e7cc9c9c1dc0",
       "style": "IPY_MODEL_8b76cd932c9c4825821cc3d143fa2ec9",
       "value": "Computing checksums: 100%"
      }
     },
     "4e95404a1f074c2e89e10bb3355f0cba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_5d7b47f116a643f080ba6573e1de99ad",
        "IPY_MODEL_aa6063b5e5184cabb8d47d00d6e4fc3a",
        "IPY_MODEL_ec575a0d18184acd8a10c8808b2c47e8"
       ],
       "layout": "IPY_MODEL_76096f7230cc452089ef6882dee073cd"
      }
     },
     "4ed5ebd07d69415f91a49aef4fe3c5dd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4f876a4e035845eda1cc0e27fbfc118c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "515cb142a2894e3d9fe3a0fc5e680f4d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "53aae80153ab4a8a9e2311392ee48106": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "54a3a0ee81284fd39aad50bdc294c684": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_cdf4ca22b91b485c9d4572ee015e2a83",
       "max": 5000,
       "style": "IPY_MODEL_969993d6413a41068a2b52222bd2a7ca",
       "value": 5000
      }
     },
     "58230166724b43cb8917e8903ebe408b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "589fe3cd80a54661b50c8dabd61acdc8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "5b1305fd5e1d445ca909e7cc9c9c1dc0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5d7b47f116a643f080ba6573e1de99ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_092a7b057a7a4145bca0be4a4b30905d",
       "style": "IPY_MODEL_013eca3801f94c03b3d037ee097fff7b",
       "value": "Map: 100%"
      }
     },
     "667c9ec36cec4b4cb179f42426e9fd8c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_bb964748c3944056a03190bfb4ac6c75",
       "style": "IPY_MODEL_cb9d99ceb72943649fc2f7cd90d06ae9",
       "value": "Map: 100%"
      }
     },
     "76096f7230cc452089ef6882dee073cd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7b89546ed20e4e03b29f339a0943d460": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "852357e4b5044b69a0c4eb7c948fd242": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_0a8632b13eb544849f6d624d6f0cd2be",
       "style": "IPY_MODEL_9f884dae8a1845d582c69b050dc853e2",
       "value": " 5000/0 [00:00&lt;00:00, 4759.93 examples/s]"
      }
     },
     "8595f10ba5dd46e9a1870ca499faf02b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8a9fcf121f2f4992a48b89d47449cce2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_43e64c0614a646209c24fe971ea9b365",
       "max": 1,
       "style": "IPY_MODEL_58230166724b43cb8917e8903ebe408b",
       "value": 1
      }
     },
     "8b76cd932c9c4825821cc3d143fa2ec9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "969993d6413a41068a2b52222bd2a7ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "972cd1d6cef445caad215bd055f2137c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_dbfc286d87ec4ac8b6492b770254f832",
       "style": "IPY_MODEL_8595f10ba5dd46e9a1870ca499faf02b",
       "value": " 2/2 [00:00&lt;00:00, 492.84it/s]"
      }
     },
     "9f884dae8a1845d582c69b050dc853e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a8eb6fcf539642c3a263acafaad8da12": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "aa6063b5e5184cabb8d47d00d6e4fc3a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_a8eb6fcf539642c3a263acafaad8da12",
       "max": 5000,
       "style": "IPY_MODEL_589fe3cd80a54661b50c8dabd61acdc8",
       "value": 5000
      }
     },
     "ab2520420b7644d5bc89941e173ea5af": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b04510c6ec8b4dd2a74078c7b9ba13bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_f4a04b68f95249cdb30f51c725c676a3",
        "IPY_MODEL_8a9fcf121f2f4992a48b89d47449cce2",
        "IPY_MODEL_852357e4b5044b69a0c4eb7c948fd242"
       ],
       "layout": "IPY_MODEL_4ed5ebd07d69415f91a49aef4fe3c5dd"
      }
     },
     "bb964748c3944056a03190bfb4ac6c75": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c47b60b0468f409483e83f8d8a441bf6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_667c9ec36cec4b4cb179f42426e9fd8c",
        "IPY_MODEL_54a3a0ee81284fd39aad50bdc294c684",
        "IPY_MODEL_3f9ce0cdf04d45a8aaf32d4f849d14e6"
       ],
       "layout": "IPY_MODEL_ab2520420b7644d5bc89941e173ea5af"
      }
     },
     "cb9d99ceb72943649fc2f7cd90d06ae9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "cdf4ca22b91b485c9d4572ee015e2a83": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d2caf165cdf841509c3bcfe0a1b7f5a3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_49dd29991d314e64ae45541e45ee0980",
        "IPY_MODEL_f7dbea97d6794071a3fe29f454faeab3",
        "IPY_MODEL_972cd1d6cef445caad215bd055f2137c"
       ],
       "layout": "IPY_MODEL_515cb142a2894e3d9fe3a0fc5e680f4d"
      }
     },
     "dbfc286d87ec4ac8b6492b770254f832": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ec575a0d18184acd8a10c8808b2c47e8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_0c835ad5701048bc9b33167daee40f93",
       "style": "IPY_MODEL_f2adfa4746d54fd3aa7578c56088393c",
       "value": " 5000/5000 [00:00&lt;00:00, 15767.17 examples/s]"
      }
     },
     "f2adfa4746d54fd3aa7578c56088393c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f4a04b68f95249cdb30f51c725c676a3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4f876a4e035845eda1cc0e27fbfc118c",
       "style": "IPY_MODEL_1a7c327860004954be24583e059a8a3a",
       "value": "Generating train split: "
      }
     },
     "f7dbea97d6794071a3fe29f454faeab3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_0aa65b1a765f412faf6ce36add4b14e5",
       "max": 2,
       "style": "IPY_MODEL_53aae80153ab4a8a9e2311392ee48106",
       "value": 2
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
