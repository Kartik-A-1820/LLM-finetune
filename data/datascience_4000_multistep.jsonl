{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Linear Regression model to predict 'species', then handle missing values in 'species' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nprint(df.describe())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['species'].fillna(df['species'].median(), inplace=True)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then calculate the correlation matrix for numeric features, then handle missing values in 'ContractType' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nprint(df.describe())\ncorr = df.corr()\nprint(corr)\ndf['ContractType'].fillna(df['ContractType'].median(), inplace=True)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'timestamp' by removing punctuation and stopwords, then train a Linear Regression model to predict 'sensor_value', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['timestamp_clean'] = df['timestamp'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Close', then handle missing values in 'High' by imputing with median, then plot a histogram of 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['High'].fillna(df['High'].median(), inplace=True)\ndf['Close'].hist()\nplt.xlabel('Close')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then clean text data in column 'Churn' by removing punctuation and stopwords, then handle missing values in 'ContractType' by imputing with median, then detect outliers in 'ContractType' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Churn_clean'] = df['Churn'].apply(clean)\ndf['ContractType'].fillna(df['ContractType'].median(), inplace=True)\nQ1 = df['ContractType'].quantile(0.25)\nQ3 = df['ContractType'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['ContractType'] < (Q1 - 1.5*IQR)) | (df['ContractType'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then create a new feature 'consumption_ratio' as the ratio of 'consumption' to 'consumption', then calculate the correlation matrix for numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['consumption_ratio'] = df['consumption'] / df['consumption']\ncorr = df.corr()\nprint(corr)\nprint(df.describe())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'humidity' by removing punctuation and stopwords, then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['humidity_clean'] = df['humidity'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then normalize the 'distance' column using min-max scaling, then detect outliers in 'arrival_delay' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['distance_scaled'] = (df['distance'] - df['distance'].min()) / (df['distance'].max() - df['distance'].min())\nQ1 = df['arrival_delay'].quantile(0.25)\nQ3 = df['arrival_delay'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['arrival_delay'] < (Q1 - 1.5*IQR)) | (df['arrival_delay'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'device_id', then train a Random Forest Classifier to predict 'sensor_value', then create a new feature 'sensor_value_ratio' as the ratio of 'sensor_value' to 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['device_id'].hist()\nplt.xlabel('device_id')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['sensor_value_ratio'] = df['sensor_value'] / df['sensor_value']"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then clean text data in column 'amount' by removing punctuation and stopwords, then one-hot encode the categorical column 'isFraud', then compute TF-IDF features for column 'isFraud' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['amount_clean'] = df['amount'].apply(clean)\ndf = pd.get_dummies(df, columns=['isFraud'], prefix=['isFraud'])\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['isFraud'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display feature importances from the Random Forest model, then handle missing values in 'date' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['date'].fillna(df['date'].median(), inplace=True)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then calculate the correlation matrix for numeric features, then create a new feature 'genre_ratio' as the ratio of 'genre' to 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['rating'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ncorr = df.corr()\nprint(corr)\ndf['genre_ratio'] = df['genre'] / df['sentiment']"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'species', then train a Random Forest Classifier to predict 'species', then plot a histogram of 'sepal_length'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['sepal_length'].hist()\nplt.xlabel('sepal_length')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then normalize the 'temp' column using min-max scaling, then handle missing values in 'traffic_volume' by imputing with median, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['temp_scaled'] = (df['temp'] - df['temp'].min()) / (df['temp'].max() - df['temp'].min())\ndf['traffic_volume'].fillna(df['traffic_volume'].median(), inplace=True)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['traffic_volume'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sales', then compute TF-IDF features for column 'sales' and display top 10 words, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sales'])\nprint(vect.get_feature_names_out())\nprint(df.describe())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then detect outliers in 'pressure' using the IQR method, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ncorr = df.corr()\nprint(corr)\nQ1 = df['pressure'].quantile(0.25)\nQ3 = df['pressure'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['pressure'] < (Q1 - 1.5*IQR)) | (df['pressure'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then normalize the 'distance' column using min-max scaling, then train a Linear Regression model to predict 'arrival_delay', then compute TF-IDF features for column 'arrival_delay' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['distance_scaled'] = (df['distance'] - df['distance'].min()) / (df['distance'].max() - df['distance'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['arrival_delay'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'ecg_reading' by removing punctuation and stopwords, then detect outliers in 'time' using the IQR method, then plot a histogram of 'time'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['ecg_reading_clean'] = df['ecg_reading'].apply(clean)\nQ1 = df['time'].quantile(0.25)\nQ3 = df['time'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['time'] < (Q1 - 1.5*IQR)) | (df['time'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['time'].hist()\nplt.xlabel('time')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())\ndf['temperature'].hist()\nplt.xlabel('temperature')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'traffic_volume', then train a Random Forest Classifier to predict 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'timestamp' and display top 10 words, then perform time-series forecasting using ARIMA to predict the next 12 periods, then handle missing values in 'location' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['timestamp'])\nprint(vect.get_feature_names_out())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['location'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['location'].fillna(df['location'].median(), inplace=True)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then plot a histogram of 'no2', then handle missing values in 'date' by imputing with median, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['no2'].hist()\nplt.xlabel('no2')\nplt.ylabel('Frequency')\nplt.show()\ndf['date'].fillna(df['date'].median(), inplace=True)\nprint(df.describe())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then clean text data in column 'departure_delay' by removing punctuation and stopwords, then one-hot encode the categorical column 'carrier', then create a new feature 'flight_ratio' as the ratio of 'flight' to 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['departure_delay_clean'] = df['departure_delay'].apply(clean)\ndf = pd.get_dummies(df, columns=['carrier'], prefix=['carrier'])\ndf['flight_ratio'] = df['flight'] / df['arrival_delay']"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then handle missing values in 'distance' by imputing with median, then plot a histogram of 'flight', then train a Random Forest Classifier to predict 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['distance'].fillna(df['distance'].median(), inplace=True)\ndf['flight'].hist()\nplt.xlabel('flight')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'education' and display top 10 words, then create a new feature 'job_ratio' as the ratio of 'job' to 'y', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['education'])\nprint(vect.get_feature_names_out())\ndf['job_ratio'] = df['job'] / df['y']\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['age'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'precipitation' by removing punctuation and stopwords, then evaluate the model performance using RMSE and R\u00b2 score, then create a new feature 'humidity_ratio' as the ratio of 'humidity' to 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['precipitation_clean'] = df['precipitation'].apply(clean)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['humidity_ratio'] = df['humidity'] / df['temperature']"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then plot a histogram of 'tenure', then train a Random Forest Classifier to predict 'Churn', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['tenure'].hist()\nplt.xlabel('tenure')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nprint(df.describe())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then detect outliers in 'review' using the IQR method, then train a Random Forest Classifier to predict 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ncorr = df.corr()\nprint(corr)\nQ1 = df['review'].quantile(0.25)\nQ3 = df['review'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['review'] < (Q1 - 1.5*IQR)) | (df['review'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Random Forest Classifier to predict 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sales'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'humidity', then train a Random Forest Classifier to predict 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['humidity'], prefix=['humidity'])\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then handle missing values in 'day_of_week' by imputing with median, then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['day_of_week'].fillna(df['day_of_week'].median(), inplace=True)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'quality' column using min-max scaling, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['quality_scaled'] = (df['quality'] - df['quality'].min()) / (df['quality'].max() - df['quality'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['ecg_reading'])\ny = df['ecg_reading']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'sepal_length', then train a Linear Regression model to predict 'species', then plot a histogram of 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf = pd.get_dummies(df, columns=['sepal_length'], prefix=['sepal_length'])\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['species'].hist()\nplt.xlabel('species')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then create a new feature 'likes_ratio' as the ratio of 'likes' to 'text', then train a Linear Regression model to predict 'text', then compute TF-IDF features for column 'likes' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['likes_ratio'] = df['likes'] / df['text']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['likes'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Random Forest Classifier to predict 'Churn', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nprint(df.describe())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'arrival_delay' by imputing with median, then plot a histogram of 'carrier'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nprint(df.describe())\ndf['arrival_delay'].fillna(df['arrival_delay'].median(), inplace=True)\ndf['carrier'].hist()\nplt.xlabel('carrier')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then display feature importances from the Random Forest model, then clean text data in column 'tenure' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['tenure_clean'] = df['tenure'].apply(clean)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then one-hot encode the categorical column 'departure_delay', then clean text data in column 'flight' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['departure_delay'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf = pd.get_dummies(df, columns=['departure_delay'], prefix=['departure_delay'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['flight_clean'] = df['flight'].apply(clean)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split, then one-hot encode the categorical column 'likes'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['user_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['text'])\ny = df['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf = pd.get_dummies(df, columns=['likes'], prefix=['likes'])"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then clean text data in column 'Open' by removing punctuation and stopwords, then display feature importances from the Random Forest model, then one-hot encode the categorical column 'Low'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Open_clean'] = df['Open'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf = pd.get_dummies(df, columns=['Low'], prefix=['Low'])"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'temperature', then normalize the 'date' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['date_scaled'] = (df['date'] - df['date'].min()) / (df['date'].max() - df['date'].min())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then normalize the 'date' column using min-max scaling, then train a Linear Regression model to predict 'temperature', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['date_scaled'] = (df['date'] - df['date'].min()) / (df['date'].max() - df['date'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then create a new feature 'newbalanceOrig_ratio' as the ratio of 'newbalanceOrig' to 'isFraud', then train a Linear Regression model to predict 'isFraud', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['newbalanceOrig_ratio'] = df['newbalanceOrig'] / df['isFraud']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Revenue', then detect outliers in 'Revenue' using the IQR method, then compute TF-IDF features for column 'UnitsSold' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['Revenue'].quantile(0.25)\nQ3 = df['Revenue'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Revenue'] < (Q1 - 1.5*IQR)) | (df['Revenue'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['UnitsSold'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'amount', then handle missing values in 'newbalanceOrig' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['amount'], prefix=['amount'])\ndf['newbalanceOrig'].fillna(df['newbalanceOrig'].median(), inplace=True)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then create a new feature 'quantity_ratio' as the ratio of 'quantity' to 'total_amount', then compute TF-IDF features for column 'quantity' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['quantity_ratio'] = df['quantity'] / df['total_amount']\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['quantity'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'date_time' and display top 10 words, then normalize the 'snow_1h' column using min-max scaling, then train a Random Forest Classifier to predict 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date_time'])\nprint(vect.get_feature_names_out())\ndf['snow_1h_scaled'] = (df['snow_1h'] - df['snow_1h'].min()) / (df['snow_1h'].max() - df['snow_1h'].min())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then create a new feature 'customer_id_ratio' as the ratio of 'customer_id' to 'total_amount', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['customer_id_ratio'] = df['customer_id'] / df['total_amount']\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'no2_ratio' as the ratio of 'no2' to 'pm2_5', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['o3'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['no2_ratio'] = df['no2'] / df['pm2_5']\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then handle missing values in 'OverallQual' by imputing with median, then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'LotArea' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['OverallQual'].fillna(df['OverallQual'].median(), inplace=True)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['LotArea'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['LotArea'].quantile(0.25)\nQ3 = df['LotArea'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['LotArea'] < (Q1 - 1.5*IQR)) | (df['LotArea'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then clean text data in column 'Age' by removing punctuation and stopwords, then train a Random Forest Classifier to predict 'Survived', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Age_clean'] = df['Age'].apply(clean)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'pm2_5', then split the data into training and testing sets with an 80-20 split, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['o3'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'humidity' by removing punctuation and stopwords, then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['humidity_clean'] = df['humidity'].apply(clean)\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Pclass', then split the data into training and testing sets with an 80-20 split, then create a new feature 'Age_ratio' as the ratio of 'Age' to 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf = pd.get_dummies(df, columns=['Pclass'], prefix=['Pclass'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['Age_ratio'] = df['Age'] / df['Survived']"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then clean text data in column 'departure_delay' by removing punctuation and stopwords, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['departure_delay_clean'] = df['departure_delay'].apply(clean)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['carrier'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nprint(df.describe())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then display feature importances from the Random Forest model, then one-hot encode the categorical column 'education'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nprint(df.describe())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf = pd.get_dummies(df, columns=['education'], prefix=['education'])"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then compute TF-IDF features for column 'wind_speed' and display top 10 words, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['wind_speed'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['wind_speed'])\nprint(vect.get_feature_names_out())\nprint(df.describe())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nprint(df.describe())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then plot a histogram of 'length', then train a Linear Regression model to predict 'sentiment', then create a new feature 'length_ratio' as the ratio of 'length' to 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['length'].hist()\nplt.xlabel('length')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['length_ratio'] = df['length'] / df['sentiment']"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'temperature', then detect outliers in 'humidity' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nQ1 = df['humidity'].quantile(0.25)\nQ3 = df['humidity'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['humidity'] < (Q1 - 1.5*IQR)) | (df['humidity'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Region', then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'Region' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Region'].hist()\nplt.xlabel('Region')\nplt.ylabel('Frequency')\nplt.show()\nprint(df.describe())\ndf['Region'].fillna(df['Region'].median(), inplace=True)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then handle missing values in 'marital' by imputing with median, then create a new feature 'job_ratio' as the ratio of 'job' to 'y', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['marital'].fillna(df['marital'].median(), inplace=True)\ndf['job_ratio'] = df['job'] / df['y']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score, then compute TF-IDF features for column 'species' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['species'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then handle missing values in 'petal_length' by imputing with median, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['petal_length'].fillna(df['petal_length'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'traffic_volume', then display summary statistics of all numeric columns using df.describe(), then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nprint(df.describe())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['temp'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'Fare' column using min-max scaling, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Fare'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['Fare_scaled'] = (df['Fare'] - df['Fare'].min()) / (df['Fare'].max() - df['Fare'].min())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'consumption', then train a Random Forest Classifier to predict 'consumption', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['consumption'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'consumption', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nprint(df.describe())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'temperature', then train a Linear Regression model to predict 'temperature', then clean text data in column 'precipitation' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['precipitation_clean'] = df['precipitation'].apply(clean)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'UnitsSold' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Revenue'])\ny = df['Revenue']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['UnitsSold'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['UnitsSold'].quantile(0.25)\nQ3 = df['UnitsSold'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['UnitsSold'] < (Q1 - 1.5*IQR)) | (df['UnitsSold'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then split the data into training and testing sets with an 80-20 split, then create a new feature 'UnitsSold_ratio' as the ratio of 'UnitsSold' to 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Revenue'])\ny = df['Revenue']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['UnitsSold_ratio'] = df['UnitsSold'] / df['Revenue']"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'confirmed', then display feature importances from the Random Forest model, then compute TF-IDF features for column 'country' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['country'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Close', then handle missing values in 'Open' by imputing with median, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['Open'].fillna(df['Open'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'temperature' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score, then plot a histogram of 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nQ1 = df['temperature'].quantile(0.25)\nQ3 = df['temperature'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['temperature'] < (Q1 - 1.5*IQR)) | (df['temperature'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['temperature'].hist()\nplt.xlabel('temperature')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'LotArea', then train a Linear Regression model to predict 'SalePrice', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf = pd.get_dummies(df, columns=['LotArea'], prefix=['LotArea'])\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then compute TF-IDF features for column 'consumption' and display top 10 words, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['consumption'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then handle missing values in 'Revenue' by imputing with median, then train a Linear Regression model to predict 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Revenue'])\ny = df['Revenue']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['Revenue'].fillna(df['Revenue'].median(), inplace=True)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then evaluate the model performance using RMSE and R\u00b2 score, then compute TF-IDF features for column 'customer_id' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['total_amount'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['customer_id'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then normalize the 'temperature' column using min-max scaling, then handle missing values in 'wind_speed' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ncorr = df.corr()\nprint(corr)\ndf['temperature_scaled'] = (df['temperature'] - df['temperature'].min()) / (df['temperature'].max() - df['temperature'].min())\ndf['wind_speed'].fillna(df['wind_speed'].median(), inplace=True)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'day_of_week' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['day_of_week'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nprint(df.describe())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then create a new feature 'precipitation_ratio' as the ratio of 'precipitation' to 'temperature', then perform time-series forecasting using ARIMA to predict the next 12 periods, then plot a histogram of 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['precipitation_ratio'] = df['precipitation'] / df['temperature']\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['temperature'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['temperature'].hist()\nplt.xlabel('temperature')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then normalize the 'ContractType' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score, then compute TF-IDF features for column 'Churn' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['ContractType_scaled'] = (df['ContractType'] - df['ContractType'].min()) / (df['ContractType'].max() - df['ContractType'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Churn'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'isFraud', then compute TF-IDF features for column 'newbalanceOrig' and display top 10 words, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['newbalanceOrig'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'SalePrice'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then clean text data in column 'snow_1h' by removing punctuation and stopwords, then train a Linear Regression model to predict 'traffic_volume', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['snow_1h_clean'] = df['snow_1h'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'confirmed', then detect outliers in 'confirmed' using the IQR method, then compute TF-IDF features for column 'deaths' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['confirmed'].quantile(0.25)\nQ3 = df['confirmed'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['confirmed'] < (Q1 - 1.5*IQR)) | (df['confirmed'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['deaths'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then plot a histogram of 'oldbalanceOrg', then clean text data in column 'oldbalanceOrg' by removing punctuation and stopwords, then detect outliers in 'newbalanceOrig' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['oldbalanceOrg'].hist()\nplt.xlabel('oldbalanceOrg')\nplt.ylabel('Frequency')\nplt.show()\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['oldbalanceOrg_clean'] = df['oldbalanceOrg'].apply(clean)\nQ1 = df['newbalanceOrig'].quantile(0.25)\nQ3 = df['newbalanceOrig'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['newbalanceOrig'] < (Q1 - 1.5*IQR)) | (df['newbalanceOrig'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Close', then one-hot encode the categorical column 'Volume', then clean text data in column 'Date' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['Volume'], prefix=['Volume'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Date_clean'] = df['Date'].apply(clean)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then normalize the 'open' column using min-max scaling, then one-hot encode the categorical column 'store'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['open_scaled'] = (df['open'] - df['open'].min()) / (df['open'].max() - df['open'].min())\ndf = pd.get_dummies(df, columns=['store'], prefix=['store'])"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then clean text data in column 'sepal_width' by removing punctuation and stopwords, then handle missing values in 'petal_length' by imputing with median, then train a Random Forest Classifier to predict 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sepal_width_clean'] = df['sepal_width'].apply(clean)\ndf['petal_length'].fillna(df['petal_length'].median(), inplace=True)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then clean text data in column 'job' by removing punctuation and stopwords, then train a Linear Regression model to predict 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['job_clean'] = df['job'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'confirmed', then train a Linear Regression model to predict 'confirmed', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['country'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'distance' and display top 10 words, then display feature importances from the Random Forest model, then normalize the 'departure_delay' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['distance'])\nprint(vect.get_feature_names_out())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['departure_delay_scaled'] = (df['departure_delay'] - df['departure_delay'].min()) / (df['departure_delay'].max() - df['departure_delay'].min())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['humidity'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'Fare' and display top 10 words, then plot a histogram of 'Survived', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Fare'])\nprint(vect.get_feature_names_out())\ndf['Survived'].hist()\nplt.xlabel('Survived')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'OverallQual', then perform time-series forecasting using ARIMA to predict the next 12 periods, then compute TF-IDF features for column 'YearBuilt' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf = pd.get_dummies(df, columns=['OverallQual'], prefix=['OverallQual'])\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['SalePrice'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['YearBuilt'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Revenue', then perform K-Means clustering with k=3 on numeric features, then normalize the 'Region' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['Region_scaled'] = (df['Region'] - df['Region'].min()) / (df['Region'].max() - df['Region'].min())"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods, then plot a histogram of 'timestamp'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sensor_value'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['timestamp'].hist()\nplt.xlabel('timestamp')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then plot a histogram of 'text', then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'shares' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['text'].hist()\nplt.xlabel('text')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['shares_scaled'] = (df['shares'] - df['shares'].min()) / (df['shares'].max() - df['shares'].min())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then normalize the 'date' column using min-max scaling, then detect outliers in 'pressure' using the IQR method, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['date_scaled'] = (df['date'] - df['date'].min()) / (df['date'].max() - df['date'].min())\nQ1 = df['pressure'].quantile(0.25)\nQ3 = df['pressure'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['pressure'] < (Q1 - 1.5*IQR)) | (df['pressure'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'device_id' by removing punctuation and stopwords, then train a Linear Regression model to predict 'sensor_value', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['device_id_clean'] = df['device_id'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sensor_value'])\ny = df['sensor_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'Product' by removing punctuation and stopwords, then display feature importances from the Random Forest model, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Product_clean'] = df['Product'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'newbalanceOrig' and display top 10 words, then handle missing values in 'isFraud' by imputing with median, then detect outliers in 'oldbalanceOrg' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['newbalanceOrig'])\nprint(vect.get_feature_names_out())\ndf['isFraud'].fillna(df['isFraud'].median(), inplace=True)\nQ1 = df['oldbalanceOrg'].quantile(0.25)\nQ3 = df['oldbalanceOrg'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['oldbalanceOrg'] < (Q1 - 1.5*IQR)) | (df['oldbalanceOrg'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'isFraud', then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then create a new feature 'sensor_value_ratio' as the ratio of 'sensor_value' to 'sensor_value', then train a Linear Regression model to predict 'sensor_value', then train a Random Forest Classifier to predict 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['sensor_value_ratio'] = df['sensor_value'] / df['sensor_value']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then one-hot encode the categorical column 'humidity', then plot a histogram of 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf = pd.get_dummies(df, columns=['humidity'], prefix=['humidity'])\ndf['consumption'].hist()\nplt.xlabel('consumption')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then train a Linear Regression model to predict 'arrival_delay', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['arrival_delay'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then one-hot encode the categorical column 'so2', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ncorr = df.corr()\nprint(corr)\ndf = pd.get_dummies(df, columns=['so2'], prefix=['so2'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'SalePrice', then handle missing values in 'Neighborhood' by imputing with median, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['Neighborhood'].fillna(df['Neighborhood'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'status' and display top 10 words, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['status'])\nprint(vect.get_feature_names_out())\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then handle missing values in 'product_id' by imputing with median, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['transaction_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['product_id'].fillna(df['product_id'].median(), inplace=True)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'device_id', then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf = pd.get_dummies(df, columns=['device_id'], prefix=['device_id'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sensor_value'])\ny = df['sensor_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'y', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['marital'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then detect outliers in 'sepal_width' using the IQR method, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['sepal_width'].quantile(0.25)\nQ3 = df['sepal_width'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sepal_width'] < (Q1 - 1.5*IQR)) | (df['sepal_width'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nprint(df.describe())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'Open', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nprint(df.describe())\ndf['Open'].hist()\nplt.xlabel('Open')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then create a new feature 'consumption_ratio' as the ratio of 'consumption' to 'consumption', then split the data into training and testing sets with an 80-20 split, then normalize the 'consumption' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['consumption_ratio'] = df['consumption'] / df['consumption']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['consumption_scaled'] = (df['consumption'] - df['consumption'].min()) / (df['consumption'].max() - df['consumption'].min())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then handle missing values in 'review' by imputing with median, then display summary statistics of all numeric columns using df.describe(), then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['review'].fillna(df['review'].median(), inplace=True)\nprint(df.describe())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'wind_speed', then one-hot encode the categorical column 'date', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['wind_speed'].hist()\nplt.xlabel('wind_speed')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['date'], prefix=['date'])\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then handle missing values in 'traffic_volume' by imputing with median, then clean text data in column 'snow_1h' by removing punctuation and stopwords, then train a Linear Regression model to predict 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['traffic_volume'].fillna(df['traffic_volume'].median(), inplace=True)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['snow_1h_clean'] = df['snow_1h'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'tenure' and display top 10 words, then perform K-Means clustering with k=3 on numeric features, then create a new feature 'tenure_ratio' as the ratio of 'tenure' to 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['tenure'])\nprint(vect.get_feature_names_out())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['tenure_ratio'] = df['tenure'] / df['Churn']"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'sales', then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'customers' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf = pd.get_dummies(df, columns=['sales'], prefix=['sales'])\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['customers'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then handle missing values in 'petal_width' by imputing with median, then create a new feature 'petal_width_ratio' as the ratio of 'petal_width' to 'species', then compute TF-IDF features for column 'petal_width' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['petal_width'].fillna(df['petal_width'].median(), inplace=True)\ndf['petal_width_ratio'] = df['petal_width'] / df['species']\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['petal_width'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then handle missing values in 'confirmed' by imputing with median, then train a Random Forest Classifier to predict 'confirmed', then create a new feature 'deaths_ratio' as the ratio of 'deaths' to 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['confirmed'].fillna(df['confirmed'].median(), inplace=True)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['deaths_ratio'] = df['deaths'] / df['confirmed']"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then detect outliers in 'Region' using the IQR method, then create a new feature 'Revenue_ratio' as the ratio of 'Revenue' to 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ncorr = df.corr()\nprint(corr)\nQ1 = df['Region'].quantile(0.25)\nQ3 = df['Region'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Region'] < (Q1 - 1.5*IQR)) | (df['Region'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['Revenue_ratio'] = df['Revenue'] / df['Revenue']"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display feature importances from the Random Forest model, then detect outliers in 'petal_length' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['petal_width'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['petal_length'].quantile(0.25)\nQ3 = df['petal_length'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['petal_length'] < (Q1 - 1.5*IQR)) | (df['petal_length'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then detect outliers in 'humidity' using the IQR method, then create a new feature 'date_ratio' as the ratio of 'date' to 'consumption', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nQ1 = df['humidity'].quantile(0.25)\nQ3 = df['humidity'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['humidity'] < (Q1 - 1.5*IQR)) | (df['humidity'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['date_ratio'] = df['date'] / df['consumption']\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'species', then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'temperature', then normalize the 'date' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['date_scaled'] = (df['date'] - df['date'].min()) / (df['date'].max() - df['date'].min())"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then create a new feature 'YearBuilt_ratio' as the ratio of 'YearBuilt' to 'SalePrice', then compute TF-IDF features for column 'YearBuilt' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ncorr = df.corr()\nprint(corr)\ndf['YearBuilt_ratio'] = df['YearBuilt'] / df['SalePrice']\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['YearBuilt'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then plot a histogram of 'post_id', then detect outliers in 'likes' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ncorr = df.corr()\nprint(corr)\ndf['post_id'].hist()\nplt.xlabel('post_id')\nplt.ylabel('Frequency')\nplt.show()\nQ1 = df['likes'].quantile(0.25)\nQ3 = df['likes'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['likes'] < (Q1 - 1.5*IQR)) | (df['likes'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then clean text data in column 'job' by removing punctuation and stopwords, then perform time-series forecasting using ARIMA to predict the next 12 periods, then one-hot encode the categorical column 'age'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['job_clean'] = df['job'].apply(clean)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['marital'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf = pd.get_dummies(df, columns=['age'], prefix=['age'])"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'consumption' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['consumption'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nprint(df.describe())"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then create a new feature 'product_id_ratio' as the ratio of 'product_id' to 'total_amount', then one-hot encode the categorical column 'product_id', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['product_id_ratio'] = df['product_id'] / df['total_amount']\ndf = pd.get_dummies(df, columns=['product_id'], prefix=['product_id'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then normalize the 'recovered' column using min-max scaling, then one-hot encode the categorical column 'recovered', then create a new feature 'date_ratio' as the ratio of 'date' to 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['recovered_scaled'] = (df['recovered'] - df['recovered'].min()) / (df['recovered'].max() - df['recovered'].min())\ndf = pd.get_dummies(df, columns=['recovered'], prefix=['recovered'])\ndf['date_ratio'] = df['date'] / df['confirmed']"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display summary statistics of all numeric columns using df.describe(), then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['tenure'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nprint(df.describe())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then handle missing values in 'country' by imputing with median, then create a new feature 'confirmed_ratio' as the ratio of 'confirmed' to 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['country'].fillna(df['country'].median(), inplace=True)\ndf['confirmed_ratio'] = df['confirmed'] / df['confirmed']"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then handle missing values in 'recovered' by imputing with median, then split the data into training and testing sets with an 80-20 split, then create a new feature 'deaths_ratio' as the ratio of 'deaths' to 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['recovered'].fillna(df['recovered'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['deaths_ratio'] = df['deaths'] / df['confirmed']"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'text', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['text'])\ny = df['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then one-hot encode the categorical column 'patient_id', then handle missing values in 'ecg_reading' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf = pd.get_dummies(df, columns=['patient_id'], prefix=['patient_id'])\ndf['ecg_reading'].fillna(df['ecg_reading'].median(), inplace=True)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then normalize the 'Revenue' column using min-max scaling, then compute TF-IDF features for column 'Revenue' and display top 10 words, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Revenue_scaled'] = (df['Revenue'] - df['Revenue'].min()) / (df['Revenue'].max() - df['Revenue'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Revenue'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Revenue'])\ny = df['Revenue']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'pm10' and display top 10 words, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['pm10'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then clean text data in column 'transaction_id' by removing punctuation and stopwords, then detect outliers in 'product_id' using the IQR method, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['transaction_id_clean'] = df['transaction_id'].apply(clean)\nQ1 = df['product_id'].quantile(0.25)\nQ3 = df['product_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['product_id'] < (Q1 - 1.5*IQR)) | (df['product_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nprint(df.describe())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then plot a histogram of 'so2', then clean text data in column 'pm10' by removing punctuation and stopwords, then compute TF-IDF features for column 'so2' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['so2'].hist()\nplt.xlabel('so2')\nplt.ylabel('Frequency')\nplt.show()\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['pm10_clean'] = df['pm10'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['so2'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then clean text data in column 'TotalCharges' by removing punctuation and stopwords, then handle missing values in 'ContractType' by imputing with median, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['TotalCharges_clean'] = df['TotalCharges'].apply(clean)\ndf['ContractType'].fillna(df['ContractType'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then clean text data in column 'pm10' by removing punctuation and stopwords, then normalize the 'no2' column using min-max scaling, then compute TF-IDF features for column 'pm10' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['pm10_clean'] = df['pm10'].apply(clean)\ndf['no2_scaled'] = (df['no2'] - df['no2'].min()) / (df['no2'].max() - df['no2'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['pm10'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Random Forest Classifier to predict 'text', then plot a histogram of 'shares'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['shares'].hist()\nplt.xlabel('shares')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then one-hot encode the categorical column 'likes', then clean text data in column 'shares' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ncorr = df.corr()\nprint(corr)\ndf = pd.get_dummies(df, columns=['likes'], prefix=['likes'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['shares_clean'] = df['shares'].apply(clean)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'total_amount', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['transaction_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then normalize the 'quantity' column using min-max scaling, then create a new feature 'customer_id_ratio' as the ratio of 'customer_id' to 'total_amount', then detect outliers in 'customer_id' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['quantity_scaled'] = (df['quantity'] - df['quantity'].min()) / (df['quantity'].max() - df['quantity'].min())\ndf['customer_id_ratio'] = df['customer_id'] / df['total_amount']\nQ1 = df['customer_id'].quantile(0.25)\nQ3 = df['customer_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['customer_id'] < (Q1 - 1.5*IQR)) | (df['customer_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then handle missing values in 'product_id' by imputing with median, then plot a histogram of 'total_amount', then train a Random Forest Classifier to predict 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['product_id'].fillna(df['product_id'].median(), inplace=True)\ndf['total_amount'].hist()\nplt.xlabel('total_amount')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then normalize the 'rating' column using min-max scaling, then clean text data in column 'sentiment' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['rating_scaled'] = (df['rating'] - df['rating'].min()) / (df['rating'].max() - df['rating'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sentiment_clean'] = df['sentiment'].apply(clean)\nprint(df.describe())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then detect outliers in 'pm10' using the IQR method, then one-hot encode the categorical column 'no2'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['pm10'].quantile(0.25)\nQ3 = df['pm10'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['pm10'] < (Q1 - 1.5*IQR)) | (df['pm10'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['no2'], prefix=['no2'])"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then detect outliers in 'amount' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nQ1 = df['amount'].quantile(0.25)\nQ3 = df['amount'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['amount'] < (Q1 - 1.5*IQR)) | (df['amount'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['oldbalanceOrg'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then handle missing values in 'flight' by imputing with median, then detect outliers in 'arrival_delay' using the IQR method, then one-hot encode the categorical column 'distance'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['flight'].fillna(df['flight'].median(), inplace=True)\nQ1 = df['arrival_delay'].quantile(0.25)\nQ3 = df['arrival_delay'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['arrival_delay'] < (Q1 - 1.5*IQR)) | (df['arrival_delay'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['distance'], prefix=['distance'])"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then split the data into training and testing sets with an 80-20 split, then create a new feature 'temp_ratio' as the ratio of 'temp' to 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['temp_ratio'] = df['temp'] / df['traffic_volume']"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'text', then detect outliers in 'user_id' using the IQR method, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf = pd.get_dummies(df, columns=['text'], prefix=['text'])\nQ1 = df['user_id'].quantile(0.25)\nQ3 = df['user_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['user_id'] < (Q1 - 1.5*IQR)) | (df['user_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nprint(df.describe())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Linear Regression model to predict 'Revenue', then handle missing values in 'Region' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nprint(df.describe())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['Region'].fillna(df['Region'].median(), inplace=True)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then plot a histogram of 'species', then split the data into training and testing sets with an 80-20 split, then create a new feature 'petal_width_ratio' as the ratio of 'petal_width' to 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['species'].hist()\nplt.xlabel('species')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['petal_width_ratio'] = df['petal_width'] / df['species']"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then one-hot encode the categorical column 'snow_1h', then compute TF-IDF features for column 'traffic_volume' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf = pd.get_dummies(df, columns=['snow_1h'], prefix=['snow_1h'])\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['traffic_volume'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model, then plot a histogram of 'review'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['review'].hist()\nplt.xlabel('review')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'ecg_reading', then display summary statistics of all numeric columns using df.describe(), then normalize the 'quality' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())\ndf['quality_scaled'] = (df['quality'] - df['quality'].min()) / (df['quality'].max() - df['quality'].min())"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Sex', then display feature importances from the Random Forest model, then train a Linear Regression model to predict 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Sex'].hist()\nplt.xlabel('Sex')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'Region' by removing punctuation and stopwords, then plot a histogram of 'Region', then one-hot encode the categorical column 'Product'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Region_clean'] = df['Region'].apply(clean)\ndf['Region'].hist()\nplt.xlabel('Region')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['Product'], prefix=['Product'])"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then compute TF-IDF features for column 'temp' and display top 10 words, then one-hot encode the categorical column 'date_time'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['temp'])\nprint(vect.get_feature_names_out())\ndf = pd.get_dummies(df, columns=['date_time'], prefix=['date_time'])"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'wind_speed' by imputing with median, then display feature importances from the Random Forest model, then create a new feature 'temperature_ratio' as the ratio of 'temperature' to 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['wind_speed'].fillna(df['wind_speed'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['temperature_ratio'] = df['temperature'] / df['temperature']"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then train a Linear Regression model to predict 'y', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'arrival_delay', then plot a histogram of 'carrier'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['carrier'].hist()\nplt.xlabel('carrier')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then clean text data in column 'review' by removing punctuation and stopwords, then display feature importances from the Random Forest model, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['review_clean'] = df['review'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display feature importances from the Random Forest model, then train a Linear Regression model to predict 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then display feature importances from the Random Forest model, then normalize the 'UnitsSold' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Revenue'])\ny = df['Revenue']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['UnitsSold_scaled'] = (df['UnitsSold'] - df['UnitsSold'].min()) / (df['UnitsSold'].max() - df['UnitsSold'].min())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'ecg_reading' by removing punctuation and stopwords, then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['ecg_reading_clean'] = df['ecg_reading'].apply(clean)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then normalize the 'species' column using min-max scaling, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nprint(df.describe())\ndf['species_scaled'] = (df['species'] - df['species'].min()) / (df['species'].max() - df['species'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then create a new feature 'no2_ratio' as the ratio of 'no2' to 'pm2_5', then evaluate the model performance using RMSE and R\u00b2 score, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['no2_ratio'] = df['no2'] / df['pm2_5']\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then clean text data in column 'length' by removing punctuation and stopwords, then calculate the correlation matrix for numeric features, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['length_clean'] = df['length'].apply(clean)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'review'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['review'].hist()\nplt.xlabel('review')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then clean text data in column 'distance' by removing punctuation and stopwords, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['distance_clean'] = df['distance'].apply(clean)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'newbalanceOrig' and display top 10 words, then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['newbalanceOrig'])\nprint(vect.get_feature_names_out())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['amount'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then detect outliers in 'so2' using the IQR method, then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nQ1 = df['so2'].quantile(0.25)\nQ3 = df['so2'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['so2'] < (Q1 - 1.5*IQR)) | (df['so2'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then detect outliers in 'sepal_width' using the IQR method, then one-hot encode the categorical column 'petal_length', then compute TF-IDF features for column 'petal_length' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nQ1 = df['sepal_width'].quantile(0.25)\nQ3 = df['sepal_width'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sepal_width'] < (Q1 - 1.5*IQR)) | (df['sepal_width'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['petal_length'], prefix=['petal_length'])\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['petal_length'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Revenue', then create a new feature 'Region_ratio' as the ratio of 'Region' to 'Revenue', then train a Random Forest Classifier to predict 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['Region_ratio'] = df['Region'] / df['Revenue']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then normalize the 'snow_1h' column using min-max scaling, then one-hot encode the categorical column 'snow_1h', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['snow_1h_scaled'] = (df['snow_1h'] - df['snow_1h'].min()) / (df['snow_1h'].max() - df['snow_1h'].min())\ndf = pd.get_dummies(df, columns=['snow_1h'], prefix=['snow_1h'])\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Churn', then create a new feature 'ContractType_ratio' as the ratio of 'ContractType' to 'Churn', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['ContractType_ratio'] = df['ContractType'] / df['Churn']\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'quantity' and display top 10 words, then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['quantity'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then compute TF-IDF features for column 'so2' and display top 10 words, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['pm2_5'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['so2'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'patient_id', then detect outliers in 'heart_rate' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['patient_id'], prefix=['patient_id'])\nQ1 = df['heart_rate'].quantile(0.25)\nQ3 = df['heart_rate'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['heart_rate'] < (Q1 - 1.5*IQR)) | (df['heart_rate'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'date' by removing punctuation and stopwords, then split the data into training and testing sets with an 80-20 split, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then normalize the 'status' column using min-max scaling, then train a Random Forest Classifier to predict 'sensor_value', then clean text data in column 'location' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['status_scaled'] = (df['status'] - df['status'].min()) / (df['status'].max() - df['status'].min())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['location_clean'] = df['location'].apply(clean)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then compute TF-IDF features for column 'y' and display top 10 words, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['y'])\nprint(vect.get_feature_names_out())\nprint(df.describe())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then plot a histogram of 'pressure', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['pressure'].hist()\nplt.xlabel('pressure')\nplt.ylabel('Frequency')\nplt.show()\nprint(df.describe())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then one-hot encode the categorical column 'petal_width', then clean text data in column 'sepal_length' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['petal_length'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf = pd.get_dummies(df, columns=['petal_width'], prefix=['petal_width'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sepal_length_clean'] = df['sepal_length'].apply(clean)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then create a new feature 'TotalCharges_ratio' as the ratio of 'TotalCharges' to 'Churn', then display feature importances from the Random Forest model, then train a Linear Regression model to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['TotalCharges_ratio'] = df['TotalCharges'] / df['Churn']\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then handle missing values in 'customers' by imputing with median, then one-hot encode the categorical column 'sales', then plot a histogram of 'day_of_week'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['customers'].fillna(df['customers'].median(), inplace=True)\ndf = pd.get_dummies(df, columns=['sales'], prefix=['sales'])\ndf['day_of_week'].hist()\nplt.xlabel('day_of_week')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then detect outliers in 'store' using the IQR method, then perform K-Means clustering with k=3 on numeric features, then train a Linear Regression model to predict 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nQ1 = df['store'].quantile(0.25)\nQ3 = df['store'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['store'] < (Q1 - 1.5*IQR)) | (df['store'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then clean text data in column 'snow_1h' by removing punctuation and stopwords, then one-hot encode the categorical column 'temp', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['snow_1h_clean'] = df['snow_1h'].apply(clean)\ndf = pd.get_dummies(df, columns=['temp'], prefix=['temp'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'sales', then train a Linear Regression model to predict 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'ecg_reading', then handle missing values in 'time' by imputing with median, then create a new feature 'patient_id_ratio' as the ratio of 'patient_id' to 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['time'].fillna(df['time'].median(), inplace=True)\ndf['patient_id_ratio'] = df['patient_id'] / df['ecg_reading']"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'ContractType' and display top 10 words, then normalize the 'MonthlyCharges' column using min-max scaling, then train a Linear Regression model to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['ContractType'])\nprint(vect.get_feature_names_out())\ndf['MonthlyCharges_scaled'] = (df['MonthlyCharges'] - df['MonthlyCharges'].min()) / (df['MonthlyCharges'].max() - df['MonthlyCharges'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then clean text data in column 'Low' by removing punctuation and stopwords, then normalize the 'Low' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nprint(df.describe())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Low_clean'] = df['Low'].apply(clean)\ndf['Low_scaled'] = (df['Low'] - df['Low'].min()) / (df['Low'].max() - df['Low'].min())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'TotalCharges', then create a new feature 'ContractType_ratio' as the ratio of 'ContractType' to 'Churn', then train a Linear Regression model to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf = pd.get_dummies(df, columns=['TotalCharges'], prefix=['TotalCharges'])\ndf['ContractType_ratio'] = df['ContractType'] / df['Churn']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'pm2_5', then display feature importances from the Random Forest model, then detect outliers in 'pm2_5' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['pm2_5'].quantile(0.25)\nQ3 = df['pm2_5'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['pm2_5'] < (Q1 - 1.5*IQR)) | (df['pm2_5'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then split the data into training and testing sets with an 80-20 split, then one-hot encode the categorical column 'wind_speed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf = pd.get_dummies(df, columns=['wind_speed'], prefix=['wind_speed'])"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'arrival_delay', then evaluate the model performance using RMSE and R\u00b2 score, then plot a histogram of 'departure_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['departure_delay'].hist()\nplt.xlabel('departure_delay')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features, then normalize the 'MonthlyCharges' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)\ndf['MonthlyCharges_scaled'] = (df['MonthlyCharges'] - df['MonthlyCharges'].min()) / (df['MonthlyCharges'].max() - df['MonthlyCharges'].min())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'arrival_delay', then one-hot encode the categorical column 'departure_delay', then plot a histogram of 'distance'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['departure_delay'], prefix=['departure_delay'])\ndf['distance'].hist()\nplt.xlabel('distance')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'device_id', then handle missing values in 'timestamp' by imputing with median, then create a new feature 'timestamp_ratio' as the ratio of 'timestamp' to 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['device_id'].hist()\nplt.xlabel('device_id')\nplt.ylabel('Frequency')\nplt.show()\ndf['timestamp'].fillna(df['timestamp'].median(), inplace=True)\ndf['timestamp_ratio'] = df['timestamp'] / df['sensor_value']"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'country' and display top 10 words, then detect outliers in 'confirmed' using the IQR method, then train a Random Forest Classifier to predict 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['country'])\nprint(vect.get_feature_names_out())\nQ1 = df['confirmed'].quantile(0.25)\nQ3 = df['confirmed'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['confirmed'] < (Q1 - 1.5*IQR)) | (df['confirmed'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'consumption', then clean text data in column 'humidity' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['consumption'].hist()\nplt.xlabel('consumption')\nplt.ylabel('Frequency')\nplt.show()\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['humidity_clean'] = df['humidity'].apply(clean)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then plot a histogram of 'length', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['length'].hist()\nplt.xlabel('length')\nplt.ylabel('Frequency')\nplt.show()\nprint(df.describe())"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then detect outliers in 'Pclass' using the IQR method, then clean text data in column 'Sex' by removing punctuation and stopwords, then one-hot encode the categorical column 'Age'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nQ1 = df['Pclass'].quantile(0.25)\nQ3 = df['Pclass'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Pclass'] < (Q1 - 1.5*IQR)) | (df['Pclass'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Sex_clean'] = df['Sex'].apply(clean)\ndf = pd.get_dummies(df, columns=['Age'], prefix=['Age'])"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Random Forest Classifier to predict 'sentiment', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nprint(df.describe())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then handle missing values in 'Product' by imputing with median, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['Product'].fillna(df['Product'].median(), inplace=True)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then clean text data in column 'oldbalanceOrg' by removing punctuation and stopwords, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['oldbalanceOrg_clean'] = df['oldbalanceOrg'].apply(clean)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'device_id', then handle missing values in 'location' by imputing with median, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['device_id'].hist()\nplt.xlabel('device_id')\nplt.ylabel('Frequency')\nplt.show()\ndf['location'].fillna(df['location'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sensor_value'])\ny = df['sensor_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then detect outliers in 'heart_rate' using the IQR method, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['ecg_reading'])\ny = df['ecg_reading']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['heart_rate'].quantile(0.25)\nQ3 = df['heart_rate'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['heart_rate'] < (Q1 - 1.5*IQR)) | (df['heart_rate'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then create a new feature 'distance_ratio' as the ratio of 'distance' to 'arrival_delay', then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['distance_ratio'] = df['distance'] / df['arrival_delay']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'Revenue' by imputing with median, then plot a histogram of 'Revenue', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Revenue'].fillna(df['Revenue'].median(), inplace=True)\ndf['Revenue'].hist()\nplt.xlabel('Revenue')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Region'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then clean text data in column 'traffic_volume' by removing punctuation and stopwords, then handle missing values in 'date_time' by imputing with median, then train a Random Forest Classifier to predict 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['traffic_volume_clean'] = df['traffic_volume'].apply(clean)\ndf['date_time'].fillna(df['date_time'].median(), inplace=True)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then detect outliers in 'Fare' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nQ1 = df['Fare'].quantile(0.25)\nQ3 = df['Fare'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Fare'] < (Q1 - 1.5*IQR)) | (df['Fare'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Fare'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'isFraud', then display summary statistics of all numeric columns using df.describe(), then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then plot a histogram of 'carrier', then calculate the correlation matrix for numeric features, then clean text data in column 'departure_delay' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['carrier'].hist()\nplt.xlabel('carrier')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['departure_delay_clean'] = df['departure_delay'].apply(clean)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'review', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['review'], prefix=['review'])\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split, then handle missing values in 'departure_delay' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['departure_delay'].fillna(df['departure_delay'].median(), inplace=True)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'Churn' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['MonthlyCharges'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nprint(df.describe())\ndf['Churn'].fillna(df['Churn'].median(), inplace=True)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'sensor_value', then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['sensor_value'].hist()\nplt.xlabel('sensor_value')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sensor_value'])\ny = df['sensor_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sensor_value', then detect outliers in 'status' using the IQR method, then handle missing values in 'location' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['status'].quantile(0.25)\nQ3 = df['status'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['status'] < (Q1 - 1.5*IQR)) | (df['status'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['location'].fillna(df['location'].median(), inplace=True)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'wind_speed', then normalize the 'date' column using min-max scaling, then train a Random Forest Classifier to predict 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['wind_speed'].hist()\nplt.xlabel('wind_speed')\nplt.ylabel('Frequency')\nplt.show()\ndf['date_scaled'] = (df['date'] - df['date'].min()) / (df['date'].max() - df['date'].min())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then one-hot encode the categorical column 'recovered', then compute TF-IDF features for column 'country' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['deaths'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf = pd.get_dummies(df, columns=['recovered'], prefix=['recovered'])\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['country'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'transaction_id', then clean text data in column 'quantity' by removing punctuation and stopwords, then compute TF-IDF features for column 'quantity' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf = pd.get_dummies(df, columns=['transaction_id'], prefix=['transaction_id'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['quantity_clean'] = df['quantity'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['quantity'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then create a new feature 'date_ratio' as the ratio of 'date' to 'temperature', then train a Random Forest Classifier to predict 'temperature', then compute TF-IDF features for column 'precipitation' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['date_ratio'] = df['date'] / df['temperature']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['precipitation'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Churn', then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then create a new feature 'age_ratio' as the ratio of 'age' to 'y', then train a Random Forest Classifier to predict 'y', then one-hot encode the categorical column 'education'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['age_ratio'] = df['age'] / df['y']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['education'], prefix=['education'])"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then create a new feature 'day_of_week_ratio' as the ratio of 'day_of_week' to 'sales', then perform K-Means clustering with k=3 on numeric features, then clean text data in column 'customers' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['day_of_week_ratio'] = df['day_of_week'] / df['sales']\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['customers_clean'] = df['customers'].apply(clean)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'total_amount', then display feature importances from the Random Forest model, then detect outliers in 'product_id' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['product_id'].quantile(0.25)\nQ3 = df['product_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['product_id'] < (Q1 - 1.5*IQR)) | (df['product_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'ecg_reading', then perform time-series forecasting using ARIMA to predict the next 12 periods, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['ecg_reading'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nprint(df.describe())"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'customer_id', then create a new feature 'total_amount_ratio' as the ratio of 'total_amount' to 'total_amount', then normalize the 'quantity' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf = pd.get_dummies(df, columns=['customer_id'], prefix=['customer_id'])\ndf['total_amount_ratio'] = df['total_amount'] / df['total_amount']\ndf['quantity_scaled'] = (df['quantity'] - df['quantity'].min()) / (df['quantity'].max() - df['quantity'].min())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'temperature' and display top 10 words, then detect outliers in 'temperature' using the IQR method, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['temperature'])\nprint(vect.get_feature_names_out())\nQ1 = df['temperature'].quantile(0.25)\nQ3 = df['temperature'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['temperature'] < (Q1 - 1.5*IQR)) | (df['temperature'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then plot a histogram of 'review', then detect outliers in 'review' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['review'].hist()\nplt.xlabel('review')\nplt.ylabel('Frequency')\nplt.show()\nQ1 = df['review'].quantile(0.25)\nQ3 = df['review'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['review'] < (Q1 - 1.5*IQR)) | (df['review'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sentiment'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Revenue', then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'Revenue' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf = pd.get_dummies(df, columns=['Revenue'], prefix=['Revenue'])\nprint(df.describe())\ndf['Revenue'].fillna(df['Revenue'].median(), inplace=True)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'age' using the IQR method, then train a Linear Regression model to predict 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nprint(df.describe())\nQ1 = df['age'].quantile(0.25)\nQ3 = df['age'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['age'] < (Q1 - 1.5*IQR)) | (df['age'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then display summary statistics of all numeric columns using df.describe(), then train a Linear Regression model to predict 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nprint(df.describe())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then create a new feature 'device_id_ratio' as the ratio of 'device_id' to 'sensor_value', then train a Linear Regression model to predict 'sensor_value', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['device_id_ratio'] = df['device_id'] / df['sensor_value']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['device_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then detect outliers in 'o3' using the IQR method, then one-hot encode the categorical column 'date'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['o3'].quantile(0.25)\nQ3 = df['o3'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['o3'] < (Q1 - 1.5*IQR)) | (df['o3'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['date'], prefix=['date'])"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then compute TF-IDF features for column 'time' and display top 10 words, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['time'])\nprint(vect.get_feature_names_out())\nprint(df.describe())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then handle missing values in 'consumption' by imputing with median, then plot a histogram of 'temperature', then detect outliers in 'date' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['consumption'].fillna(df['consumption'].median(), inplace=True)\ndf['temperature'].hist()\nplt.xlabel('temperature')\nplt.ylabel('Frequency')\nplt.show()\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then plot a histogram of 'arrival_delay', then normalize the 'departure_delay' column using min-max scaling, then clean text data in column 'carrier' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['arrival_delay'].hist()\nplt.xlabel('arrival_delay')\nplt.ylabel('Frequency')\nplt.show()\ndf['departure_delay_scaled'] = (df['departure_delay'] - df['departure_delay'].min()) / (df['departure_delay'].max() - df['departure_delay'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['carrier_clean'] = df['carrier'].apply(clean)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then normalize the 'quantity' column using min-max scaling, then split the data into training and testing sets with an 80-20 split, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['quantity_scaled'] = (df['quantity'] - df['quantity'].min()) / (df['quantity'].max() - df['quantity'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then handle missing values in 'tenure' by imputing with median, then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'TotalCharges' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['tenure'].fillna(df['tenure'].median(), inplace=True)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['ContractType'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['TotalCharges'].quantile(0.25)\nQ3 = df['TotalCharges'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['TotalCharges'] < (Q1 - 1.5*IQR)) | (df['TotalCharges'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'humidity' by removing punctuation and stopwords, then compute TF-IDF features for column 'wind_speed' and display top 10 words, then handle missing values in 'precipitation' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['humidity_clean'] = df['humidity'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['wind_speed'])\nprint(vect.get_feature_names_out())\ndf['precipitation'].fillna(df['precipitation'].median(), inplace=True)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then split the data into training and testing sets with an 80-20 split, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Revenue'])\ny = df['Revenue']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'isFraud', then display feature importances from the Random Forest model, then plot a histogram of 'oldbalanceOrg'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['oldbalanceOrg'].hist()\nplt.xlabel('oldbalanceOrg')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'petal_width', then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'sepal_length' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf = pd.get_dummies(df, columns=['petal_width'], prefix=['petal_width'])\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sepal_length'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'total_amount', then compute TF-IDF features for column 'quantity' and display top 10 words, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['quantity'])\nprint(vect.get_feature_names_out())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model, then one-hot encode the categorical column 'education'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf = pd.get_dummies(df, columns=['education'], prefix=['education'])"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then display summary statistics of all numeric columns using df.describe(), then create a new feature 'Sex_ratio' as the ratio of 'Sex' to 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nprint(df.describe())\ndf['Sex_ratio'] = df['Sex'] / df['Survived']"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'open', then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'day_of_week' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf = pd.get_dummies(df, columns=['open'], prefix=['open'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['day_of_week_scaled'] = (df['day_of_week'] - df['day_of_week'].min()) / (df['day_of_week'].max() - df['day_of_week'].min())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then compute TF-IDF features for column 'likes' and display top 10 words, then detect outliers in 'likes' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['likes'])\nprint(vect.get_feature_names_out())\nQ1 = df['likes'].quantile(0.25)\nQ3 = df['likes'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['likes'] < (Q1 - 1.5*IQR)) | (df['likes'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then create a new feature 'species_ratio' as the ratio of 'species' to 'species', then normalize the 'petal_width' column using min-max scaling, then train a Random Forest Classifier to predict 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['species_ratio'] = df['species'] / df['species']\ndf['petal_width_scaled'] = (df['petal_width'] - df['petal_width'].min()) / (df['petal_width'].max() - df['petal_width'].min())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then compute TF-IDF features for column 'Product' and display top 10 words, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Product'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Product'])\nprint(vect.get_feature_names_out())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'UnitsSold' by removing punctuation and stopwords, then compute TF-IDF features for column 'Revenue' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['UnitsSold_clean'] = df['UnitsSold'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Revenue'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'petal_width' by imputing with median, then one-hot encode the categorical column 'petal_width'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nprint(df.describe())\ndf['petal_width'].fillna(df['petal_width'].median(), inplace=True)\ndf = pd.get_dummies(df, columns=['petal_width'], prefix=['petal_width'])"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then detect outliers in 'date' using the IQR method, then one-hot encode the categorical column 'date', then normalize the 'humidity' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['date'], prefix=['date'])\ndf['humidity_scaled'] = (df['humidity'] - df['humidity'].min()) / (df['humidity'].max() - df['humidity'].min())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'petal_length', then display feature importances from the Random Forest model, then detect outliers in 'petal_length' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf = pd.get_dummies(df, columns=['petal_length'], prefix=['petal_length'])\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['petal_length'].quantile(0.25)\nQ3 = df['petal_length'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['petal_length'] < (Q1 - 1.5*IQR)) | (df['petal_length'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then clean text data in column 'sepal_length' by removing punctuation and stopwords, then detect outliers in 'sepal_length' using the IQR method, then normalize the 'sepal_length' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sepal_length_clean'] = df['sepal_length'].apply(clean)\nQ1 = df['sepal_length'].quantile(0.25)\nQ3 = df['sepal_length'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sepal_length'] < (Q1 - 1.5*IQR)) | (df['sepal_length'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['sepal_length_scaled'] = (df['sepal_length'] - df['sepal_length'].min()) / (df['sepal_length'].max() - df['sepal_length'].min())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then normalize the 'sepal_width' column using min-max scaling, then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['sepal_width_scaled'] = (df['sepal_width'] - df['sepal_width'].min()) / (df['sepal_width'].max() - df['sepal_width'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then normalize the 'newbalanceOrig' column using min-max scaling, then detect outliers in 'time' using the IQR method, then train a Random Forest Classifier to predict 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['newbalanceOrig_scaled'] = (df['newbalanceOrig'] - df['newbalanceOrig'].min()) / (df['newbalanceOrig'].max() - df['newbalanceOrig'].min())\nQ1 = df['time'].quantile(0.25)\nQ3 = df['time'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['time'] < (Q1 - 1.5*IQR)) | (df['time'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then create a new feature 'isFraud_ratio' as the ratio of 'isFraud' to 'isFraud', then handle missing values in 'isFraud' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nprint(df.describe())\ndf['isFraud_ratio'] = df['isFraud'] / df['isFraud']\ndf['isFraud'].fillna(df['isFraud'].median(), inplace=True)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'ecg_reading', then create a new feature 'heart_rate_ratio' as the ratio of 'heart_rate' to 'ecg_reading', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['heart_rate_ratio'] = df['heart_rate'] / df['ecg_reading']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['ecg_reading'])\ny = df['ecg_reading']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then handle missing values in 'open' by imputing with median, then normalize the 'sales' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['open'].fillna(df['open'].median(), inplace=True)\ndf['sales_scaled'] = (df['sales'] - df['sales'].min()) / (df['sales'].max() - df['sales'].min())"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Survived', then handle missing values in 'Pclass' by imputing with median, then compute TF-IDF features for column 'Pclass' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['Pclass'].fillna(df['Pclass'].median(), inplace=True)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Pclass'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'country', then perform K-Means clustering with k=3 on numeric features, then create a new feature 'recovered_ratio' as the ratio of 'recovered' to 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf = pd.get_dummies(df, columns=['country'], prefix=['country'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['recovered_ratio'] = df['recovered'] / df['confirmed']"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'y', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['y'], prefix=['y'])\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'education', then create a new feature 'education_ratio' as the ratio of 'education' to 'y', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf = pd.get_dummies(df, columns=['education'], prefix=['education'])\ndf['education_ratio'] = df['education'] / df['y']\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['education'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then normalize the 'age' column using min-max scaling, then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['age_scaled'] = (df['age'] - df['age'].min()) / (df['age'].max() - df['age'].min())\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'distance' column using min-max scaling, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['distance_scaled'] = (df['distance'] - df['distance'].min()) / (df['distance'].max() - df['distance'].min())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then plot a histogram of 'wind_speed', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['wind_speed'].hist()\nplt.xlabel('wind_speed')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then clean text data in column 'tenure' by removing punctuation and stopwords, then train a Linear Regression model to predict 'Churn', then normalize the 'MonthlyCharges' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['tenure_clean'] = df['tenure'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['MonthlyCharges_scaled'] = (df['MonthlyCharges'] - df['MonthlyCharges'].min()) / (df['MonthlyCharges'].max() - df['MonthlyCharges'].min())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'day_of_week' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['day_of_week'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['day_of_week_scaled'] = (df['day_of_week'] - df['day_of_week'].min()) / (df['day_of_week'].max() - df['day_of_week'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'arrival_delay', then normalize the 'distance' column using min-max scaling, then compute TF-IDF features for column 'distance' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['distance_scaled'] = (df['distance'] - df['distance'].min()) / (df['distance'].max() - df['distance'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['distance'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then plot a histogram of 'Pclass', then one-hot encode the categorical column 'Sex'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Pclass'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['Pclass'].hist()\nplt.xlabel('Pclass')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['Sex'], prefix=['Sex'])"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'oldbalanceOrg' and display top 10 words, then one-hot encode the categorical column 'newbalanceOrig', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['oldbalanceOrg'])\nprint(vect.get_feature_names_out())\ndf = pd.get_dummies(df, columns=['newbalanceOrig'], prefix=['newbalanceOrig'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then detect outliers in 'oldbalanceOrg' using the IQR method, then train a Linear Regression model to predict 'isFraud', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nQ1 = df['oldbalanceOrg'].quantile(0.25)\nQ3 = df['oldbalanceOrg'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['oldbalanceOrg'] < (Q1 - 1.5*IQR)) | (df['oldbalanceOrg'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'Date' by removing punctuation and stopwords, then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Date_clean'] = df['Date'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Region'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'temperature', then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'humidity' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['humidity_scaled'] = (df['humidity'] - df['humidity'].min()) / (df['humidity'].max() - df['humidity'].min())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then handle missing values in 'tenure' by imputing with median, then clean text data in column 'Churn' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['tenure'].fillna(df['tenure'].median(), inplace=True)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Churn_clean'] = df['Churn'].apply(clean)\nprint(df.describe())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'High', then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf = pd.get_dummies(df, columns=['High'], prefix=['High'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then normalize the 'patient_id' column using min-max scaling, then plot a histogram of 'heart_rate', then create a new feature 'time_ratio' as the ratio of 'time' to 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['patient_id_scaled'] = (df['patient_id'] - df['patient_id'].min()) / (df['patient_id'].max() - df['patient_id'].min())\ndf['heart_rate'].hist()\nplt.xlabel('heart_rate')\nplt.ylabel('Frequency')\nplt.show()\ndf['time_ratio'] = df['time'] / df['ecg_reading']"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'Close' and display top 10 words, then calculate the correlation matrix for numeric features, then detect outliers in 'Date' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Close'])\nprint(vect.get_feature_names_out())\ncorr = df.corr()\nprint(corr)\nQ1 = df['Date'].quantile(0.25)\nQ3 = df['Date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Date'] < (Q1 - 1.5*IQR)) | (df['Date'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then handle missing values in 'user_id' by imputing with median, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['user_id'].fillna(df['user_id'].median(), inplace=True)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['post_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then handle missing values in 'petal_length' by imputing with median, then train a Random Forest Classifier to predict 'species', then normalize the 'sepal_width' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['petal_length'].fillna(df['petal_length'].median(), inplace=True)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['sepal_width_scaled'] = (df['sepal_width'] - df['sepal_width'].min()) / (df['sepal_width'].max() - df['sepal_width'].min())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then normalize the 'traffic_volume' column using min-max scaling, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nprint(df.describe())\ndf['traffic_volume_scaled'] = (df['traffic_volume'] - df['traffic_volume'].min()) / (df['traffic_volume'].max() - df['traffic_volume'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then handle missing values in 'High' by imputing with median, then detect outliers in 'High' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ncorr = df.corr()\nprint(corr)\ndf['High'].fillna(df['High'].median(), inplace=True)\nQ1 = df['High'].quantile(0.25)\nQ3 = df['High'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['High'] < (Q1 - 1.5*IQR)) | (df['High'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sensor_value', then calculate the correlation matrix for numeric features, then handle missing values in 'sensor_value' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)\ndf['sensor_value'].fillna(df['sensor_value'].median(), inplace=True)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then plot a histogram of 'newbalanceOrig', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['newbalanceOrig'].hist()\nplt.xlabel('newbalanceOrig')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then display summary statistics of all numeric columns using df.describe(), then normalize the 'sales' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ncorr = df.corr()\nprint(corr)\nprint(df.describe())\ndf['sales_scaled'] = (df['sales'] - df['sales'].min()) / (df['sales'].max() - df['sales'].min())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ncorr = df.corr()\nprint(corr)\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then detect outliers in 'isFraud' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nQ1 = df['isFraud'].quantile(0.25)\nQ3 = df['isFraud'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['isFraud'] < (Q1 - 1.5*IQR)) | (df['isFraud'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then plot a histogram of 'wind_speed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['wind_speed'].hist()\nplt.xlabel('wind_speed')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['customer_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then normalize the 'customer_id' column using min-max scaling, then handle missing values in 'customer_id' by imputing with median, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['customer_id_scaled'] = (df['customer_id'] - df['customer_id'].min()) / (df['customer_id'].max() - df['customer_id'].min())\ndf['customer_id'].fillna(df['customer_id'].median(), inplace=True)\nprint(df.describe())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then one-hot encode the categorical column 'review', then detect outliers in 'rating' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf = pd.get_dummies(df, columns=['review'], prefix=['review'])\nQ1 = df['rating'].quantile(0.25)\nQ3 = df['rating'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['rating'] < (Q1 - 1.5*IQR)) | (df['rating'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'traffic_volume', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'MonthlyCharges' and display top 10 words, then normalize the 'tenure' column using min-max scaling, then detect outliers in 'MonthlyCharges' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['MonthlyCharges'])\nprint(vect.get_feature_names_out())\ndf['tenure_scaled'] = (df['tenure'] - df['tenure'].min()) / (df['tenure'].max() - df['tenure'].min())\nQ1 = df['MonthlyCharges'].quantile(0.25)\nQ3 = df['MonthlyCharges'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['MonthlyCharges'] < (Q1 - 1.5*IQR)) | (df['MonthlyCharges'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then split the data into training and testing sets with an 80-20 split, then detect outliers in 'MonthlyCharges' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nprint(df.describe())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['MonthlyCharges'].quantile(0.25)\nQ3 = df['MonthlyCharges'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['MonthlyCharges'] < (Q1 - 1.5*IQR)) | (df['MonthlyCharges'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split, then normalize the 'flight' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['departure_delay'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['flight_scaled'] = (df['flight'] - df['flight'].min()) / (df['flight'].max() - df['flight'].min())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then detect outliers in 'sepal_width' using the IQR method, then plot a histogram of 'petal_width', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nQ1 = df['sepal_width'].quantile(0.25)\nQ3 = df['sepal_width'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sepal_width'] < (Q1 - 1.5*IQR)) | (df['sepal_width'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['petal_width'].hist()\nplt.xlabel('petal_width')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'total_amount', then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then clean text data in column 'day_of_week' by removing punctuation and stopwords, then normalize the 'day_of_week' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['day_of_week_clean'] = df['day_of_week'].apply(clean)\ndf['day_of_week_scaled'] = (df['day_of_week'] - df['day_of_week'].min()) / (df['day_of_week'].max() - df['day_of_week'].min())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nprint(df.describe())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then create a new feature 'petal_width_ratio' as the ratio of 'petal_width' to 'species', then split the data into training and testing sets with an 80-20 split, then detect outliers in 'sepal_length' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['petal_width_ratio'] = df['petal_width'] / df['species']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['sepal_length'].quantile(0.25)\nQ3 = df['sepal_length'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sepal_length'] < (Q1 - 1.5*IQR)) | (df['sepal_length'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'device_id' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nQ1 = df['device_id'].quantile(0.25)\nQ3 = df['device_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['device_id'] < (Q1 - 1.5*IQR)) | (df['device_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['timestamp'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then plot a histogram of 'oldbalanceOrg', then create a new feature 'oldbalanceOrg_ratio' as the ratio of 'oldbalanceOrg' to 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ncorr = df.corr()\nprint(corr)\ndf['oldbalanceOrg'].hist()\nplt.xlabel('oldbalanceOrg')\nplt.ylabel('Frequency')\nplt.show()\ndf['oldbalanceOrg_ratio'] = df['oldbalanceOrg'] / df['isFraud']"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'text', then clean text data in column 'user_id' by removing punctuation and stopwords, then one-hot encode the categorical column 'shares'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['user_id_clean'] = df['user_id'].apply(clean)\ndf = pd.get_dummies(df, columns=['shares'], prefix=['shares'])"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then handle missing values in 'Age' by imputing with median, then normalize the 'Age' column using min-max scaling, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Age'].fillna(df['Age'].median(), inplace=True)\ndf['Age_scaled'] = (df['Age'] - df['Age'].min()) / (df['Age'].max() - df['Age'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then clean text data in column 'High' by removing punctuation and stopwords, then evaluate the model performance using RMSE and R\u00b2 score, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['High_clean'] = df['High'].apply(clean)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Close'])\ny = df['Close']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then plot a histogram of 'sales', then create a new feature 'store_ratio' as the ratio of 'store' to 'sales', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['sales'].hist()\nplt.xlabel('sales')\nplt.ylabel('Frequency')\nplt.show()\ndf['store_ratio'] = df['store'] / df['sales']\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sales'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then plot a histogram of 'traffic_volume', then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'snow_1h'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['traffic_volume'].hist()\nplt.xlabel('traffic_volume')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['snow_1h'], prefix=['snow_1h'])"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then clean text data in column 'sepal_width' by removing punctuation and stopwords, then handle missing values in 'sepal_length' by imputing with median, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sepal_width_clean'] = df['sepal_width'].apply(clean)\ndf['sepal_length'].fillna(df['sepal_length'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display feature importances from the Random Forest model, then train a Random Forest Classifier to predict 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then create a new feature 'Region_ratio' as the ratio of 'Region' to 'Revenue', then clean text data in column 'Product' by removing punctuation and stopwords, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Region_ratio'] = df['Region'] / df['Revenue']\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Product_clean'] = df['Product'].apply(clean)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then normalize the 'Churn' column using min-max scaling, then perform time-series forecasting using ARIMA to predict the next 12 periods, then compute TF-IDF features for column 'Churn' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['Churn_scaled'] = (df['Churn'] - df['Churn'].min()) / (df['Churn'].max() - df['Churn'].min())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['tenure'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Churn'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then create a new feature 'patient_id_ratio' as the ratio of 'patient_id' to 'ecg_reading', then normalize the 'ecg_reading' column using min-max scaling, then one-hot encode the categorical column 'quality'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['patient_id_ratio'] = df['patient_id'] / df['ecg_reading']\ndf['ecg_reading_scaled'] = (df['ecg_reading'] - df['ecg_reading'].min()) / (df['ecg_reading'].max() - df['ecg_reading'].min())\ndf = pd.get_dummies(df, columns=['quality'], prefix=['quality'])"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'total_amount' and display top 10 words, then perform time-series forecasting using ARIMA to predict the next 12 periods, then clean text data in column 'product_id' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['total_amount'])\nprint(vect.get_feature_names_out())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['transaction_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['product_id_clean'] = df['product_id'].apply(clean)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'Fare_ratio' as the ratio of 'Fare' to 'Survived', then train a Random Forest Classifier to predict 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Fare'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['Fare_ratio'] = df['Fare'] / df['Survived']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'y' and display top 10 words, then display feature importances from the Random Forest model, then clean text data in column 'job' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['y'])\nprint(vect.get_feature_names_out())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['job_clean'] = df['job'].apply(clean)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe(), then clean text data in column 'shares' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['text'])\ny = df['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['shares_clean'] = df['shares'].apply(clean)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then handle missing values in 'sentiment' by imputing with median, then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['sentiment'].fillna(df['sentiment'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then normalize the 'quality' column using min-max scaling, then detect outliers in 'patient_id' using the IQR method, then handle missing values in 'patient_id' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['quality_scaled'] = (df['quality'] - df['quality'].min()) / (df['quality'].max() - df['quality'].min())\nQ1 = df['patient_id'].quantile(0.25)\nQ3 = df['patient_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['patient_id'] < (Q1 - 1.5*IQR)) | (df['patient_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['patient_id'].fillna(df['patient_id'].median(), inplace=True)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then create a new feature 'temperature_ratio' as the ratio of 'temperature' to 'consumption', then compute TF-IDF features for column 'humidity' and display top 10 words, then train a Linear Regression model to predict 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['temperature_ratio'] = df['temperature'] / df['consumption']\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['humidity'])\nprint(vect.get_feature_names_out())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe(), then train a Linear Regression model to predict 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sensor_value'])\ny = df['sensor_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sentiment', then plot a histogram of 'review', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['review'].hist()\nplt.xlabel('review')\nplt.ylabel('Frequency')\nplt.show()\nprint(df.describe())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then handle missing values in 'oldbalanceOrg' by imputing with median, then train a Random Forest Classifier to predict 'isFraud', then one-hot encode the categorical column 'newbalanceOrig'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['oldbalanceOrg'].fillna(df['oldbalanceOrg'].median(), inplace=True)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['newbalanceOrig'], prefix=['newbalanceOrig'])"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform K-Means clustering with k=3 on numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nprint(df.describe())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['genre'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then handle missing values in 'temperature' by imputing with median, then compute TF-IDF features for column 'consumption' and display top 10 words, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['temperature'].fillna(df['temperature'].median(), inplace=True)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['consumption'])\nprint(vect.get_feature_names_out())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Revenue', then clean text data in column 'Region' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Region_clean'] = df['Region'].apply(clean)\nprint(df.describe())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'sepal_width', then create a new feature 'petal_width_ratio' as the ratio of 'petal_width' to 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nprint(df.describe())\ndf['sepal_width'].hist()\nplt.xlabel('sepal_width')\nplt.ylabel('Frequency')\nplt.show()\ndf['petal_width_ratio'] = df['petal_width'] / df['species']"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then handle missing values in 'ecg_reading' by imputing with median, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['ecg_reading'].fillna(df['ecg_reading'].median(), inplace=True)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['time'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Random Forest Classifier to predict 'total_amount', then one-hot encode the categorical column 'quantity'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nprint(df.describe())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['quantity'], prefix=['quantity'])"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Random Forest Classifier to predict 'Revenue', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then plot a histogram of 'sentiment', then handle missing values in 'rating' by imputing with median, then normalize the 'length' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['sentiment'].hist()\nplt.xlabel('sentiment')\nplt.ylabel('Frequency')\nplt.show()\ndf['rating'].fillna(df['rating'].median(), inplace=True)\ndf['length_scaled'] = (df['length'] - df['length'].min()) / (df['length'].max() - df['length'].min())"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'status' by imputing with median, then plot a histogram of 'sensor_value', then create a new feature 'device_id_ratio' as the ratio of 'device_id' to 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['status'].fillna(df['status'].median(), inplace=True)\ndf['sensor_value'].hist()\nplt.xlabel('sensor_value')\nplt.ylabel('Frequency')\nplt.show()\ndf['device_id_ratio'] = df['device_id'] / df['sensor_value']"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then normalize the 'Region' column using min-max scaling, then compute TF-IDF features for column 'Date' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['Region_scaled'] = (df['Region'] - df['Region'].min()) / (df['Region'].max() - df['Region'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Date'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then clean text data in column 'pm10' by removing punctuation and stopwords, then train a Linear Regression model to predict 'pm2_5', then handle missing values in 'date' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['pm10_clean'] = df['pm10'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['date'].fillna(df['date'].median(), inplace=True)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['flight'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then compute TF-IDF features for column 'date' and display top 10 words, then one-hot encode the categorical column 'date'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date'])\nprint(vect.get_feature_names_out())\ndf = pd.get_dummies(df, columns=['date'], prefix=['date'])"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then normalize the 'sentiment' column using min-max scaling, then create a new feature 'review_ratio' as the ratio of 'review' to 'sentiment', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['sentiment_scaled'] = (df['sentiment'] - df['sentiment'].min()) / (df['sentiment'].max() - df['sentiment'].min())\ndf['review_ratio'] = df['review'] / df['sentiment']\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Volume', then train a Linear Regression model to predict 'Close', then normalize the 'Close' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf = pd.get_dummies(df, columns=['Volume'], prefix=['Volume'])\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['Close_scaled'] = (df['Close'] - df['Close'].min()) / (df['Close'].max() - df['Close'].min())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'arrival_delay', then train a Linear Regression model to predict 'arrival_delay', then compute TF-IDF features for column 'flight' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['flight'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Survived', then handle missing values in 'Sex' by imputing with median, then clean text data in column 'Age' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf = pd.get_dummies(df, columns=['Survived'], prefix=['Survived'])\ndf['Sex'].fillna(df['Sex'].median(), inplace=True)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Age_clean'] = df['Age'].apply(clean)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'amount' and display top 10 words, then detect outliers in 'newbalanceOrig' using the IQR method, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['amount'])\nprint(vect.get_feature_names_out())\nQ1 = df['newbalanceOrig'].quantile(0.25)\nQ3 = df['newbalanceOrig'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['newbalanceOrig'] < (Q1 - 1.5*IQR)) | (df['newbalanceOrig'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nprint(df.describe())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'Churn', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then detect outliers in 'user_id' using the IQR method, then one-hot encode the categorical column 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nQ1 = df['user_id'].quantile(0.25)\nQ3 = df['user_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['user_id'] < (Q1 - 1.5*IQR)) | (df['user_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['text'], prefix=['text'])"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'text', then perform time-series forecasting using ARIMA to predict the next 12 periods, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['likes'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then plot a histogram of 'temp', then train a Linear Regression model to predict 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['temp'].hist()\nplt.xlabel('temp')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then create a new feature 'review_ratio' as the ratio of 'review' to 'sentiment', then detect outliers in 'review' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['review_ratio'] = df['review'] / df['sentiment']\nQ1 = df['review'].quantile(0.25)\nQ3 = df['review'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['review'] < (Q1 - 1.5*IQR)) | (df['review'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['review'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Close', then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Volume'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then compute TF-IDF features for column 'location' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['location'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nprint(df.describe())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Close', then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'Volume' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Low'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['Volume'].quantile(0.25)\nQ3 = df['Volume'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Volume'] < (Q1 - 1.5*IQR)) | (df['Volume'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then split the data into training and testing sets with an 80-20 split, then one-hot encode the categorical column 'customers'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nprint(df.describe())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf = pd.get_dummies(df, columns=['customers'], prefix=['customers'])"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'pm2_5', then create a new feature 'so2_ratio' as the ratio of 'so2' to 'pm2_5', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['so2_ratio'] = df['so2'] / df['pm2_5']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then detect outliers in 'flight' using the IQR method, then train a Random Forest Classifier to predict 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nQ1 = df['flight'].quantile(0.25)\nQ3 = df['flight'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['flight'] < (Q1 - 1.5*IQR)) | (df['flight'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'temperature', then evaluate the model performance using RMSE and R\u00b2 score, then handle missing values in 'humidity' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['humidity'].fillna(df['humidity'].median(), inplace=True)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'SalePrice' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'Neighborhood'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['SalePrice'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['Neighborhood'], prefix=['Neighborhood'])"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then compute TF-IDF features for column 'user_id' and display top 10 words, then clean text data in column 'text' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['text'])\ny = df['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['user_id'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['text_clean'] = df['text'].apply(clean)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'y', then train a Linear Regression model to predict 'y', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf = pd.get_dummies(df, columns=['y'], prefix=['y'])\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['device_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then plot a histogram of 'job', then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['job'].hist()\nplt.xlabel('job')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Age', then create a new feature 'Pclass_ratio' as the ratio of 'Pclass' to 'Survived', then normalize the 'Pclass' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf = pd.get_dummies(df, columns=['Age'], prefix=['Age'])\ndf['Pclass_ratio'] = df['Pclass'] / df['Survived']\ndf['Pclass_scaled'] = (df['Pclass'] - df['Pclass'].min()) / (df['Pclass'].max() - df['Pclass'].min())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'y', then normalize the 'education' column using min-max scaling, then compute TF-IDF features for column 'job' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['education_scaled'] = (df['education'] - df['education'].min()) / (df['education'].max() - df['education'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['job'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then normalize the 'date' column using min-max scaling, then compute TF-IDF features for column 'pressure' and display top 10 words, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['date_scaled'] = (df['date'] - df['date'].min()) / (df['date'].max() - df['date'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['pressure'])\nprint(vect.get_feature_names_out())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'date' and display top 10 words, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date'])\nprint(vect.get_feature_names_out())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then split the data into training and testing sets with an 80-20 split, then handle missing values in 'SalePrice' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nprint(df.describe())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['SalePrice'].fillna(df['SalePrice'].median(), inplace=True)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then plot a histogram of 'genre', then one-hot encode the categorical column 'rating'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ncorr = df.corr()\nprint(corr)\ndf['genre'].hist()\nplt.xlabel('genre')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['rating'], prefix=['rating'])"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'user_id' and display top 10 words, then display feature importances from the Random Forest model, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['user_id'])\nprint(vect.get_feature_names_out())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then create a new feature 'Fare_ratio' as the ratio of 'Fare' to 'Survived', then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Fare_ratio'] = df['Fare'] / df['Survived']\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'date', then detect outliers in 'date' using the IQR method, then train a Random Forest Classifier to predict 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['date'].hist()\nplt.xlabel('date')\nplt.ylabel('Frequency')\nplt.show()\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then handle missing values in 'amount' by imputing with median, then train a Linear Regression model to predict 'isFraud', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['amount'].fillna(df['amount'].median(), inplace=True)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'traffic_volume', then one-hot encode the categorical column 'temp', then create a new feature 'date_time_ratio' as the ratio of 'date_time' to 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['temp'], prefix=['temp'])\ndf['date_time_ratio'] = df['date_time'] / df['traffic_volume']"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then create a new feature 'Close_ratio' as the ratio of 'Close' to 'Close', then train a Linear Regression model to predict 'Close', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Close_ratio'] = df['Close'] / df['Close']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['High'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'Churn' and display top 10 words, then create a new feature 'MonthlyCharges_ratio' as the ratio of 'MonthlyCharges' to 'Churn', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Churn'])\nprint(vect.get_feature_names_out())\ndf['MonthlyCharges_ratio'] = df['MonthlyCharges'] / df['Churn']\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'species' and display top 10 words, then one-hot encode the categorical column 'sepal_length', then plot a histogram of 'sepal_width'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['species'])\nprint(vect.get_feature_names_out())\ndf = pd.get_dummies(df, columns=['sepal_length'], prefix=['sepal_length'])\ndf['sepal_width'].hist()\nplt.xlabel('sepal_width')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'total_amount', then clean text data in column 'transaction_id' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['transaction_id_clean'] = df['transaction_id'].apply(clean)\nprint(df.describe())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then handle missing values in 'date' by imputing with median, then detect outliers in 'country' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['date'].fillna(df['date'].median(), inplace=True)\nQ1 = df['country'].quantile(0.25)\nQ3 = df['country'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['country'] < (Q1 - 1.5*IQR)) | (df['country'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['confirmed'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'pm2_5', then calculate the correlation matrix for numeric features, then normalize the 'no2' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)\ndf['no2_scaled'] = (df['no2'] - df['no2'].min()) / (df['no2'].max() - df['no2'].min())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then compute TF-IDF features for column 'date' and display top 10 words, then detect outliers in 'date' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date'])\nprint(vect.get_feature_names_out())\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then compute TF-IDF features for column 'sepal_width' and display top 10 words, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['species'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sepal_width'])\nprint(vect.get_feature_names_out())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'Age' and display top 10 words, then detect outliers in 'Survived' using the IQR method, then plot a histogram of 'Sex'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Age'])\nprint(vect.get_feature_names_out())\nQ1 = df['Survived'].quantile(0.25)\nQ3 = df['Survived'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Survived'] < (Q1 - 1.5*IQR)) | (df['Survived'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['Sex'].hist()\nplt.xlabel('Sex')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then detect outliers in 'YearBuilt' using the IQR method, then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'SalePrice'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nQ1 = df['YearBuilt'].quantile(0.25)\nQ3 = df['YearBuilt'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['YearBuilt'] < (Q1 - 1.5*IQR)) | (df['YearBuilt'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'amount', then plot a histogram of 'newbalanceOrig', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf = pd.get_dummies(df, columns=['amount'], prefix=['amount'])\ndf['newbalanceOrig'].hist()\nplt.xlabel('newbalanceOrig')\nplt.ylabel('Frequency')\nplt.show()\nprint(df.describe())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'text', then one-hot encode the categorical column 'likes'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['text'])\ny = df['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['likes'], prefix=['likes'])"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'Revenue' by removing punctuation and stopwords, then detect outliers in 'Date' using the IQR method, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Revenue_clean'] = df['Revenue'].apply(clean)\nQ1 = df['Date'].quantile(0.25)\nQ3 = df['Date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Date'] < (Q1 - 1.5*IQR)) | (df['Date'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'petal_width' and display top 10 words, then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'sepal_length'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['petal_width'])\nprint(vect.get_feature_names_out())\nprint(df.describe())\ndf['sepal_length'].hist()\nplt.xlabel('sepal_length')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then create a new feature 'temp_ratio' as the ratio of 'temp' to 'traffic_volume', then clean text data in column 'traffic_volume' by removing punctuation and stopwords, then normalize the 'temp' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['temp_ratio'] = df['temp'] / df['traffic_volume']\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['traffic_volume_clean'] = df['traffic_volume'].apply(clean)\ndf['temp_scaled'] = (df['temp'] - df['temp'].min()) / (df['temp'].max() - df['temp'].min())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'ecg_reading' by removing punctuation and stopwords, then handle missing values in 'patient_id' by imputing with median, then plot a histogram of 'patient_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['ecg_reading_clean'] = df['ecg_reading'].apply(clean)\ndf['patient_id'].fillna(df['patient_id'].median(), inplace=True)\ndf['patient_id'].hist()\nplt.xlabel('patient_id')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then plot a histogram of 'sepal_width', then normalize the 'sepal_length' column using min-max scaling, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['sepal_width'].hist()\nplt.xlabel('sepal_width')\nplt.ylabel('Frequency')\nplt.show()\ndf['sepal_length_scaled'] = (df['sepal_length'] - df['sepal_length'].min()) / (df['sepal_length'].max() - df['sepal_length'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then handle missing values in 'Low' by imputing with median, then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'Date' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Low'].fillna(df['Low'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['Date'].quantile(0.25)\nQ3 = df['Date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Date'] < (Q1 - 1.5*IQR)) | (df['Date'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then handle missing values in 'ContractType' by imputing with median, then clean text data in column 'Churn' by removing punctuation and stopwords, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['ContractType'].fillna(df['ContractType'].median(), inplace=True)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Churn_clean'] = df['Churn'].apply(clean)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then compute TF-IDF features for column 'distance' and display top 10 words, then clean text data in column 'departure_delay' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['distance'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['distance'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['departure_delay_clean'] = df['departure_delay'].apply(clean)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'isFraud', then perform K-Means clustering with k=3 on numeric features, then create a new feature 'oldbalanceOrg_ratio' as the ratio of 'oldbalanceOrg' to 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['oldbalanceOrg_ratio'] = df['oldbalanceOrg'] / df['isFraud']"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'so2', then detect outliers in 'o3' using the IQR method, then create a new feature 'o3_ratio' as the ratio of 'o3' to 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf = pd.get_dummies(df, columns=['so2'], prefix=['so2'])\nQ1 = df['o3'].quantile(0.25)\nQ3 = df['o3'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['o3'] < (Q1 - 1.5*IQR)) | (df['o3'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['o3_ratio'] = df['o3'] / df['pm2_5']"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then create a new feature 'High_ratio' as the ratio of 'High' to 'Close', then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['High_ratio'] = df['High'] / df['Close']\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then compute TF-IDF features for column 'species' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['species'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then handle missing values in 'traffic_volume' by imputing with median, then display summary statistics of all numeric columns using df.describe(), then create a new feature 'temp_ratio' as the ratio of 'temp' to 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['traffic_volume'].fillna(df['traffic_volume'].median(), inplace=True)\nprint(df.describe())\ndf['temp_ratio'] = df['temp'] / df['traffic_volume']"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'total_amount', then one-hot encode the categorical column 'customer_id', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['customer_id'], prefix=['customer_id'])\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['product_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features, then handle missing values in 'petal_width' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['petal_width'].fillna(df['petal_width'].median(), inplace=True)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then plot a histogram of 'text', then display summary statistics of all numeric columns using df.describe(), then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['text'].hist()\nplt.xlabel('text')\nplt.ylabel('Frequency')\nplt.show()\nprint(df.describe())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then create a new feature 'humidity_ratio' as the ratio of 'humidity' to 'consumption', then detect outliers in 'date' using the IQR method, then one-hot encode the categorical column 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['humidity_ratio'] = df['humidity'] / df['consumption']\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['consumption'], prefix=['consumption'])"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then plot a histogram of 'isFraud', then perform time-series forecasting using ARIMA to predict the next 12 periods, then one-hot encode the categorical column 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['isFraud'].hist()\nplt.xlabel('isFraud')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['amount'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf = pd.get_dummies(df, columns=['isFraud'], prefix=['isFraud'])"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'SalePrice', then clean text data in column 'OverallQual' by removing punctuation and stopwords, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['OverallQual_clean'] = df['OverallQual'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then compute TF-IDF features for column 'store' and display top 10 words, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['store'])\nprint(vect.get_feature_names_out())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then detect outliers in 'genre' using the IQR method, then calculate the correlation matrix for numeric features, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nQ1 = df['genre'].quantile(0.25)\nQ3 = df['genre'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['genre'] < (Q1 - 1.5*IQR)) | (df['genre'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then create a new feature 'device_id_ratio' as the ratio of 'device_id' to 'sensor_value', then train a Random Forest Classifier to predict 'sensor_value', then normalize the 'status' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['device_id_ratio'] = df['device_id'] / df['sensor_value']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['status_scaled'] = (df['status'] - df['status'].min()) / (df['status'].max() - df['status'].min())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then clean text data in column 'sepal_length' by removing punctuation and stopwords, then handle missing values in 'petal_width' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sepal_length_clean'] = df['sepal_length'].apply(clean)\ndf['petal_width'].fillna(df['petal_width'].median(), inplace=True)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'isFraud', then evaluate the model performance using RMSE and R\u00b2 score, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nprint(df.describe())"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then create a new feature 'transaction_id_ratio' as the ratio of 'transaction_id' to 'total_amount', then train a Random Forest Classifier to predict 'total_amount', then train a Linear Regression model to predict 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['transaction_id_ratio'] = df['transaction_id'] / df['total_amount']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then handle missing values in 'Volume' by imputing with median, then compute TF-IDF features for column 'Volume' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Close'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['Volume'].fillna(df['Volume'].median(), inplace=True)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Volume'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then clean text data in column 'LotArea' by removing punctuation and stopwords, then train a Random Forest Classifier to predict 'SalePrice', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['LotArea_clean'] = df['LotArea'].apply(clean)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nprint(df.describe())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'consumption', then create a new feature 'temperature_ratio' as the ratio of 'temperature' to 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['temperature_ratio'] = df['temperature'] / df['consumption']"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then handle missing values in 'isFraud' by imputing with median, then display summary statistics of all numeric columns using df.describe(), then train a Random Forest Classifier to predict 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['isFraud'].fillna(df['isFraud'].median(), inplace=True)\nprint(df.describe())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then handle missing values in 'temp' by imputing with median, then evaluate the model performance using RMSE and R\u00b2 score, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['temp'].fillna(df['temp'].median(), inplace=True)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'OverallQual' and display top 10 words, then normalize the 'SalePrice' column using min-max scaling, then train a Linear Regression model to predict 'SalePrice'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['OverallQual'])\nprint(vect.get_feature_names_out())\ndf['SalePrice_scaled'] = (df['SalePrice'] - df['SalePrice'].min()) / (df['SalePrice'].max() - df['SalePrice'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then one-hot encode the categorical column 'quality', then plot a histogram of 'quality'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nprint(df.describe())\ndf = pd.get_dummies(df, columns=['quality'], prefix=['quality'])\ndf['quality'].hist()\nplt.xlabel('quality')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then create a new feature 'quantity_ratio' as the ratio of 'quantity' to 'total_amount', then display feature importances from the Random Forest model, then handle missing values in 'customer_id' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['quantity_ratio'] = df['quantity'] / df['total_amount']\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['customer_id'].fillna(df['customer_id'].median(), inplace=True)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then handle missing values in 'Close' by imputing with median, then normalize the 'Close' column using min-max scaling, then train a Random Forest Classifier to predict 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Close'].fillna(df['Close'].median(), inplace=True)\ndf['Close_scaled'] = (df['Close'] - df['Close'].min()) / (df['Close'].max() - df['Close'].min())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'total_amount', then plot a histogram of 'customer_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['customer_id'].hist()\nplt.xlabel('customer_id')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features, then clean text data in column 'petal_length' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['petal_length_clean'] = df['petal_length'].apply(clean)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'temperature', then display feature importances from the Random Forest model, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf = pd.get_dummies(df, columns=['temperature'], prefix=['temperature'])\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'sensor_value', then detect outliers in 'location' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['device_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nQ1 = df['location'].quantile(0.25)\nQ3 = df['location'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['location'] < (Q1 - 1.5*IQR)) | (df['location'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then compute TF-IDF features for column 'quality' and display top 10 words, then train a Linear Regression model to predict 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['quality'])\nprint(vect.get_feature_names_out())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then create a new feature 'flight_ratio' as the ratio of 'flight' to 'arrival_delay', then train a Linear Regression model to predict 'arrival_delay', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['flight_ratio'] = df['flight'] / df['arrival_delay']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then create a new feature 'temperature_ratio' as the ratio of 'temperature' to 'consumption', then plot a histogram of 'consumption', then train a Linear Regression model to predict 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['temperature_ratio'] = df['temperature'] / df['consumption']\ndf['consumption'].hist()\nplt.xlabel('consumption')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'humidity', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['humidity'], prefix=['humidity'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then normalize the 'sensor_value' column using min-max scaling, then one-hot encode the categorical column 'status', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['sensor_value_scaled'] = (df['sensor_value'] - df['sensor_value'].min()) / (df['sensor_value'].max() - df['sensor_value'].min())\ndf = pd.get_dummies(df, columns=['status'], prefix=['status'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sensor_value'])\ny = df['sensor_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then compute TF-IDF features for column 'TotalCharges' and display top 10 words, then handle missing values in 'tenure' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['TotalCharges'])\nprint(vect.get_feature_names_out())\ndf['tenure'].fillna(df['tenure'].median(), inplace=True)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then normalize the 'wind_speed' column using min-max scaling, then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['wind_speed_scaled'] = (df['wind_speed'] - df['wind_speed'].min()) / (df['wind_speed'].max() - df['wind_speed'].min())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features, then normalize the 'isFraud' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['isFraud_scaled'] = (df['isFraud'] - df['isFraud'].min()) / (df['isFraud'].max() - df['isFraud'].min())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then normalize the 'so2' column using min-max scaling, then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['so2_scaled'] = (df['so2'] - df['so2'].min()) / (df['so2'].max() - df['so2'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['pm2_5'].hist()\nplt.xlabel('pm2_5')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'patient_id' by imputing with median, then split the data into training and testing sets with an 80-20 split, then create a new feature 'heart_rate_ratio' as the ratio of 'heart_rate' to 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['patient_id'].fillna(df['patient_id'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['ecg_reading'])\ny = df['ecg_reading']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['heart_rate_ratio'] = df['heart_rate'] / df['ecg_reading']"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then normalize the 'consumption' column using min-max scaling, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['consumption_scaled'] = (df['consumption'] - df['consumption'].min()) / (df['consumption'].max() - df['consumption'].min())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['consumption'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nprint(df.describe())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'temperature', then plot a histogram of 'pressure', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf = pd.get_dummies(df, columns=['temperature'], prefix=['temperature'])\ndf['pressure'].hist()\nplt.xlabel('pressure')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then create a new feature 'quantity_ratio' as the ratio of 'quantity' to 'total_amount', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ncorr = df.corr()\nprint(corr)\ndf['quantity_ratio'] = df['quantity'] / df['total_amount']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'open', then normalize the 'day_of_week' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf = pd.get_dummies(df, columns=['open'], prefix=['open'])\ndf['day_of_week_scaled'] = (df['day_of_week'] - df['day_of_week'].min()) / (df['day_of_week'].max() - df['day_of_week'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'sensor_value' using the IQR method, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['sensor_value'].quantile(0.25)\nQ3 = df['sensor_value'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sensor_value'] < (Q1 - 1.5*IQR)) | (df['sensor_value'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'genre' and display top 10 words, then plot a histogram of 'review', then clean text data in column 'sentiment' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['genre'])\nprint(vect.get_feature_names_out())\ndf['review'].hist()\nplt.xlabel('review')\nplt.ylabel('Frequency')\nplt.show()\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sentiment_clean'] = df['sentiment'].apply(clean)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then detect outliers in 'date' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'deaths' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['country'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['deaths_scaled'] = (df['deaths'] - df['deaths'].min()) / (df['deaths'].max() - df['deaths'].min())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then handle missing values in 'temperature' by imputing with median, then perform time-series forecasting using ARIMA to predict the next 12 periods, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['temperature'].fillna(df['temperature'].median(), inplace=True)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['consumption'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then create a new feature 'date_ratio' as the ratio of 'date' to 'temperature', then display summary statistics of all numeric columns using df.describe(), then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['date_ratio'] = df['date'] / df['temperature']\nprint(df.describe())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display feature importances from the Random Forest model, then plot a histogram of 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['user_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['text'].hist()\nplt.xlabel('text')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then train a Linear Regression model to predict 'consumption', then train a Random Forest Classifier to predict 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'distance' and display top 10 words, then train a Linear Regression model to predict 'arrival_delay', then one-hot encode the categorical column 'flight'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['distance'])\nprint(vect.get_feature_names_out())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['flight'], prefix=['flight'])"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then normalize the 'so2' column using min-max scaling, then handle missing values in 'pm10' by imputing with median, then compute TF-IDF features for column 'pm2_5' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['so2_scaled'] = (df['so2'] - df['so2'].min()) / (df['so2'].max() - df['so2'].min())\ndf['pm10'].fillna(df['pm10'].median(), inplace=True)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['pm2_5'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then one-hot encode the categorical column 'LotArea', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ncorr = df.corr()\nprint(corr)\ndf = pd.get_dummies(df, columns=['LotArea'], prefix=['LotArea'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then plot a histogram of 'quality', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ncorr = df.corr()\nprint(corr)\ndf['quality'].hist()\nplt.xlabel('quality')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'sentiment' and display top 10 words, then clean text data in column 'sentiment' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sentiment'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sentiment_clean'] = df['sentiment'].apply(clean)\nprint(df.describe())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then normalize the 'day_of_week' column using min-max scaling, then plot a histogram of 'day_of_week', then detect outliers in 'sales' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['day_of_week_scaled'] = (df['day_of_week'] - df['day_of_week'].min()) / (df['day_of_week'].max() - df['day_of_week'].min())\ndf['day_of_week'].hist()\nplt.xlabel('day_of_week')\nplt.ylabel('Frequency')\nplt.show()\nQ1 = df['sales'].quantile(0.25)\nQ3 = df['sales'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sales'] < (Q1 - 1.5*IQR)) | (df['sales'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'time' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['time'].quantile(0.25)\nQ3 = df['time'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['time'] < (Q1 - 1.5*IQR)) | (df['time'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then detect outliers in 'distance' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['distance'].quantile(0.25)\nQ3 = df['distance'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['distance'] < (Q1 - 1.5*IQR)) | (df['distance'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['carrier'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Linear Regression model to predict 'consumption', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nprint(df.describe())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then handle missing values in 'shares' by imputing with median, then train a Linear Regression model to predict 'text', then clean text data in column 'shares' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['shares'].fillna(df['shares'].median(), inplace=True)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['shares_clean'] = df['shares'].apply(clean)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sensor_value', then clean text data in column 'location' by removing punctuation and stopwords, then detect outliers in 'device_id' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['location_clean'] = df['location'].apply(clean)\nQ1 = df['device_id'].quantile(0.25)\nQ3 = df['device_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['device_id'] < (Q1 - 1.5*IQR)) | (df['device_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then detect outliers in 'oldbalanceOrg' using the IQR method, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['oldbalanceOrg'].quantile(0.25)\nQ3 = df['oldbalanceOrg'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['oldbalanceOrg'] < (Q1 - 1.5*IQR)) | (df['oldbalanceOrg'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then create a new feature 'ecg_reading_ratio' as the ratio of 'ecg_reading' to 'ecg_reading', then train a Linear Regression model to predict 'ecg_reading', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['ecg_reading_ratio'] = df['ecg_reading'] / df['ecg_reading']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then create a new feature 'Open_ratio' as the ratio of 'Open' to 'Close', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Close'])\ny = df['Close']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['Open_ratio'] = df['Open'] / df['Close']\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'y', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nprint(df.describe())\ndf['y'].hist()\nplt.xlabel('y')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['education'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then normalize the 'job' column using min-max scaling, then train a Linear Regression model to predict 'y', then create a new feature 'education_ratio' as the ratio of 'education' to 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['job_scaled'] = (df['job'] - df['job'].min()) / (df['job'].max() - df['job'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['education_ratio'] = df['education'] / df['y']"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then perform K-Means clustering with k=3 on numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['temp'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then create a new feature 'LotArea_ratio' as the ratio of 'LotArea' to 'SalePrice', then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['LotArea_ratio'] = df['LotArea'] / df['SalePrice']\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sensor_value', then compute TF-IDF features for column 'sensor_value' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sensor_value'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then detect outliers in 'deaths' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['deaths'].quantile(0.25)\nQ3 = df['deaths'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['deaths'] < (Q1 - 1.5*IQR)) | (df['deaths'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['recovered'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then handle missing values in 'pressure' by imputing with median, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['pressure'].fillna(df['pressure'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Survived', then compute TF-IDF features for column 'Age' and display top 10 words, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf = pd.get_dummies(df, columns=['Survived'], prefix=['Survived'])\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Age'])\nprint(vect.get_feature_names_out())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'y', then compute TF-IDF features for column 'job' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['job'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Close', then compute TF-IDF features for column 'Low' and display top 10 words, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Low'])\nprint(vect.get_feature_names_out())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['High'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'ecg_reading', then display feature importances from the Random Forest model, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nprint(df.describe())"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then train a Random Forest Classifier to predict 'total_amount', then compute TF-IDF features for column 'quantity' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['quantity'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'pm10', then normalize the 'o3' column using min-max scaling, then train a Random Forest Classifier to predict 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf = pd.get_dummies(df, columns=['pm10'], prefix=['pm10'])\ndf['o3_scaled'] = (df['o3'] - df['o3'].min()) / (df['o3'].max() - df['o3'].min())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sentiment', then split the data into training and testing sets with an 80-20 split, then normalize the 'length' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['length_scaled'] = (df['length'] - df['length'].min()) / (df['length'].max() - df['length'].min())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'ecg_reading' using the IQR method, then clean text data in column 'time' by removing punctuation and stopwords, then create a new feature 'ecg_reading_ratio' as the ratio of 'ecg_reading' to 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nQ1 = df['ecg_reading'].quantile(0.25)\nQ3 = df['ecg_reading'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['ecg_reading'] < (Q1 - 1.5*IQR)) | (df['ecg_reading'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['time_clean'] = df['time'].apply(clean)\ndf['ecg_reading_ratio'] = df['ecg_reading'] / df['ecg_reading']"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then create a new feature 'Date_ratio' as the ratio of 'Date' to 'Revenue', then normalize the 'Date' column using min-max scaling, then detect outliers in 'Revenue' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Date_ratio'] = df['Date'] / df['Revenue']\ndf['Date_scaled'] = (df['Date'] - df['Date'].min()) / (df['Date'].max() - df['Date'].min())\nQ1 = df['Revenue'].quantile(0.25)\nQ3 = df['Revenue'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Revenue'] < (Q1 - 1.5*IQR)) | (df['Revenue'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features, then plot a histogram of 'YearBuilt'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)\ndf['YearBuilt'].hist()\nplt.xlabel('YearBuilt')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then clean text data in column 'status' by removing punctuation and stopwords, then train a Random Forest Classifier to predict 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['status_clean'] = df['status'].apply(clean)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'ecg_reading', then compute TF-IDF features for column 'quality' and display top 10 words, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['quality'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['ecg_reading'])\ny = df['ecg_reading']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then clean text data in column 'pressure' by removing punctuation and stopwords, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['pressure_clean'] = df['pressure'].apply(clean)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['temperature'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'UnitsSold' by removing punctuation and stopwords, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'Date' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['UnitsSold_clean'] = df['UnitsSold'].apply(clean)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['Date_scaled'] = (df['Date'] - df['Date'].min()) / (df['Date'].max() - df['Date'].min())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'rain_1h', then display feature importances from the Random Forest model, then detect outliers in 'temp' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf = pd.get_dummies(df, columns=['rain_1h'], prefix=['rain_1h'])\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['temp'].quantile(0.25)\nQ3 = df['temp'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['temp'] < (Q1 - 1.5*IQR)) | (df['temp'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then normalize the 'amount' column using min-max scaling, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['amount_scaled'] = (df['amount'] - df['amount'].min()) / (df['amount'].max() - df['amount'].min())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'status' using the IQR method, then train a Linear Regression model to predict 'sensor_value', then one-hot encode the categorical column 'timestamp'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nQ1 = df['status'].quantile(0.25)\nQ3 = df['status'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['status'] < (Q1 - 1.5*IQR)) | (df['status'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['timestamp'], prefix=['timestamp'])"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then handle missing values in 'Sex' by imputing with median, then train a Random Forest Classifier to predict 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['Sex'].fillna(df['Sex'].median(), inplace=True)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'LotArea', then plot a histogram of 'LotArea', then train a Linear Regression model to predict 'SalePrice'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf = pd.get_dummies(df, columns=['LotArea'], prefix=['LotArea'])\ndf['LotArea'].hist()\nplt.xlabel('LotArea')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'pm10'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nprint(df.describe())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['pm10'].hist()\nplt.xlabel('pm10')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'transaction_id' and display top 10 words, then display feature importances from the Random Forest model, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['transaction_id'])\nprint(vect.get_feature_names_out())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'newbalanceOrig' and display top 10 words, then detect outliers in 'time' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['newbalanceOrig'])\nprint(vect.get_feature_names_out())\nQ1 = df['time'].quantile(0.25)\nQ3 = df['time'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['time'] < (Q1 - 1.5*IQR)) | (df['time'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then clean text data in column 'time' by removing punctuation and stopwords, then split the data into training and testing sets with an 80-20 split, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['time_clean'] = df['time'].apply(clean)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then clean text data in column 'marital' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['marital_clean'] = df['marital'].apply(clean)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then create a new feature 'transaction_id_ratio' as the ratio of 'transaction_id' to 'total_amount', then display feature importances from the Random Forest model, then clean text data in column 'quantity' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['transaction_id_ratio'] = df['transaction_id'] / df['total_amount']\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['quantity_clean'] = df['quantity'].apply(clean)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then normalize the 'LotArea' column using min-max scaling, then train a Linear Regression model to predict 'SalePrice', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['LotArea_scaled'] = (df['LotArea'] - df['LotArea'].min()) / (df['LotArea'].max() - df['LotArea'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'store', then display feature importances from the Random Forest model, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf = pd.get_dummies(df, columns=['store'], prefix=['store'])\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then plot a histogram of 'MonthlyCharges', then display feature importances from the Random Forest model, then train a Random Forest Classifier to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['MonthlyCharges'].hist()\nplt.xlabel('MonthlyCharges')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then clean text data in column 'carrier' by removing punctuation and stopwords, then detect outliers in 'departure_delay' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['carrier_clean'] = df['carrier'].apply(clean)\nQ1 = df['departure_delay'].quantile(0.25)\nQ3 = df['departure_delay'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['departure_delay'] < (Q1 - 1.5*IQR)) | (df['departure_delay'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'TotalCharges' and display top 10 words, then clean text data in column 'MonthlyCharges' by removing punctuation and stopwords, then train a Linear Regression model to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['TotalCharges'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['MonthlyCharges_clean'] = df['MonthlyCharges'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then clean text data in column 'shares' by removing punctuation and stopwords, then train a Linear Regression model to predict 'text', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['shares_clean'] = df['shares'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['likes'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then clean text data in column 'temperature' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['temperature_clean'] = df['temperature'].apply(clean)\nprint(df.describe())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'sales', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'sales', then create a new feature 'store_ratio' as the ratio of 'store' to 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['store_ratio'] = df['store'] / df['sales']"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then plot a histogram of 'rating', then create a new feature 'review_ratio' as the ratio of 'review' to 'sentiment', then detect outliers in 'genre' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['rating'].hist()\nplt.xlabel('rating')\nplt.ylabel('Frequency')\nplt.show()\ndf['review_ratio'] = df['review'] / df['sentiment']\nQ1 = df['genre'].quantile(0.25)\nQ3 = df['genre'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['genre'] < (Q1 - 1.5*IQR)) | (df['genre'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then detect outliers in 'marital' using the IQR method, then train a Linear Regression model to predict 'y', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nQ1 = df['marital'].quantile(0.25)\nQ3 = df['marital'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['marital'] < (Q1 - 1.5*IQR)) | (df['marital'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then clean text data in column 'y' by removing punctuation and stopwords, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ncorr = df.corr()\nprint(corr)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['y_clean'] = df['y'].apply(clean)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sales', then plot a histogram of 'store', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['store'].hist()\nplt.xlabel('store')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features, then handle missing values in 'species' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['species'].fillna(df['species'].median(), inplace=True)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then clean text data in column 'sepal_width' by removing punctuation and stopwords, then detect outliers in 'species' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sepal_width_clean'] = df['sepal_width'].apply(clean)\nQ1 = df['species'].quantile(0.25)\nQ3 = df['species'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['species'] < (Q1 - 1.5*IQR)) | (df['species'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'traffic_volume', then normalize the 'temp' column using min-max scaling, then train a Linear Regression model to predict 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['temp_scaled'] = (df['temp'] - df['temp'].min()) / (df['temp'].max() - df['temp'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'quality' using the IQR method, then compute TF-IDF features for column 'heart_rate' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['patient_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['quality'].quantile(0.25)\nQ3 = df['quality'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['quality'] < (Q1 - 1.5*IQR)) | (df['quality'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['heart_rate'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'time' by removing punctuation and stopwords, then detect outliers in 'patient_id' using the IQR method, then create a new feature 'quality_ratio' as the ratio of 'quality' to 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['time_clean'] = df['time'].apply(clean)\nQ1 = df['patient_id'].quantile(0.25)\nQ3 = df['patient_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['patient_id'] < (Q1 - 1.5*IQR)) | (df['patient_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['quality_ratio'] = df['quality'] / df['ecg_reading']"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score, then handle missing values in 'total_amount' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['total_amount'].fillna(df['total_amount'].median(), inplace=True)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then plot a histogram of 'sepal_width', then clean text data in column 'petal_length' by removing punctuation and stopwords, then normalize the 'species' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['sepal_width'].hist()\nplt.xlabel('sepal_width')\nplt.ylabel('Frequency')\nplt.show()\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['petal_length_clean'] = df['petal_length'].apply(clean)\ndf['species_scaled'] = (df['species'] - df['species'].min()) / (df['species'].max() - df['species'].min())"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sensor_value', then evaluate the model performance using RMSE and R\u00b2 score, then plot a histogram of 'timestamp'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['timestamp'].hist()\nplt.xlabel('timestamp')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then normalize the 'y' column using min-max scaling, then compute TF-IDF features for column 'y' and display top 10 words, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['y_scaled'] = (df['y'] - df['y'].min()) / (df['y'].max() - df['y'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['y'])\nprint(vect.get_feature_names_out())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['age'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'UnitsSold', then train a Random Forest Classifier to predict 'Revenue', then normalize the 'Revenue' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['UnitsSold'].hist()\nplt.xlabel('UnitsSold')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['Revenue_scaled'] = (df['Revenue'] - df['Revenue'].min()) / (df['Revenue'].max() - df['Revenue'].min())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then plot a histogram of 'time', then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['time'].hist()\nplt.xlabel('time')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Fare', then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Fare'].hist()\nplt.xlabel('Fare')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Age', then train a Random Forest Classifier to predict 'Survived', then plot a histogram of 'Sex'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf = pd.get_dummies(df, columns=['Age'], prefix=['Age'])\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['Sex'].hist()\nplt.xlabel('Sex')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then detect outliers in 'amount' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nQ1 = df['amount'].quantile(0.25)\nQ3 = df['amount'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['amount'] < (Q1 - 1.5*IQR)) | (df['amount'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['newbalanceOrig'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then create a new feature 'date_ratio' as the ratio of 'date' to 'consumption', then split the data into training and testing sets with an 80-20 split, then handle missing values in 'pressure' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['date_ratio'] = df['date'] / df['consumption']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['pressure'].fillna(df['pressure'].median(), inplace=True)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'petal_length' using the IQR method, then normalize the 'species' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['petal_length'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['petal_length'].quantile(0.25)\nQ3 = df['petal_length'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['petal_length'] < (Q1 - 1.5*IQR)) | (df['petal_length'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['species_scaled'] = (df['species'] - df['species'].min()) / (df['species'].max() - df['species'].min())"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then normalize the 'quantity' column using min-max scaling, then display feature importances from the Random Forest model, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['quantity_scaled'] = (df['quantity'] - df['quantity'].min()) / (df['quantity'].max() - df['quantity'].min())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'time', then calculate the correlation matrix for numeric features, then plot a histogram of 'patient_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf = pd.get_dummies(df, columns=['time'], prefix=['time'])\ncorr = df.corr()\nprint(corr)\ndf['patient_id'].hist()\nplt.xlabel('patient_id')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then handle missing values in 'likes' by imputing with median, then evaluate the model performance using RMSE and R\u00b2 score, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['likes'].fillna(df['likes'].median(), inplace=True)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['text'])\ny = df['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then detect outliers in 'Low' using the IQR method, then train a Linear Regression model to predict 'Close', then compute TF-IDF features for column 'Low' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nQ1 = df['Low'].quantile(0.25)\nQ3 = df['Low'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Low'] < (Q1 - 1.5*IQR)) | (df['Low'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Low'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then normalize the 'temperature' column using min-max scaling, then plot a histogram of 'date', then one-hot encode the categorical column 'date'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['temperature_scaled'] = (df['temperature'] - df['temperature'].min()) / (df['temperature'].max() - df['temperature'].min())\ndf['date'].hist()\nplt.xlabel('date')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['date'], prefix=['date'])"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'isFraud', then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['amount'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then handle missing values in 'snow_1h' by imputing with median, then train a Random Forest Classifier to predict 'traffic_volume', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['snow_1h'].fillna(df['snow_1h'].median(), inplace=True)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'arrival_delay', then handle missing values in 'carrier' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nprint(df.describe())\ndf['arrival_delay'].hist()\nplt.xlabel('arrival_delay')\nplt.ylabel('Frequency')\nplt.show()\ndf['carrier'].fillna(df['carrier'].median(), inplace=True)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then detect outliers in 'Neighborhood' using the IQR method, then normalize the 'LotArea' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['Neighborhood'].quantile(0.25)\nQ3 = df['Neighborhood'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Neighborhood'] < (Q1 - 1.5*IQR)) | (df['Neighborhood'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['LotArea_scaled'] = (df['LotArea'] - df['LotArea'].min()) / (df['LotArea'].max() - df['LotArea'].min())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Linear Regression model to predict 'isFraud', then handle missing values in 'time' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nprint(df.describe())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['time'].fillna(df['time'].median(), inplace=True)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then perform time-series forecasting using ARIMA to predict the next 12 periods, then handle missing values in 'snow_1h' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['rain_1h'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['snow_1h'].fillna(df['snow_1h'].median(), inplace=True)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'pm2_5', then create a new feature 'pm2_5_ratio' as the ratio of 'pm2_5' to 'pm2_5', then train a Random Forest Classifier to predict 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf = pd.get_dummies(df, columns=['pm2_5'], prefix=['pm2_5'])\ndf['pm2_5_ratio'] = df['pm2_5'] / df['pm2_5']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'date', then normalize the 'country' column using min-max scaling, then compute TF-IDF features for column 'country' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf = pd.get_dummies(df, columns=['date'], prefix=['date'])\ndf['country_scaled'] = (df['country'] - df['country'].min()) / (df['country'].max() - df['country'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['country'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'user_id' using the IQR method, then one-hot encode the categorical column 'post_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['likes'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['user_id'].quantile(0.25)\nQ3 = df['user_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['user_id'] < (Q1 - 1.5*IQR)) | (df['user_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['post_id'], prefix=['post_id'])"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Linear Regression model to predict 'sentiment', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nprint(df.describe())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'patient_id' and display top 10 words, then detect outliers in 'heart_rate' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['patient_id'])\nprint(vect.get_feature_names_out())\nQ1 = df['heart_rate'].quantile(0.25)\nQ3 = df['heart_rate'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['heart_rate'] < (Q1 - 1.5*IQR)) | (df['heart_rate'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'job', then normalize the 'education' column using min-max scaling, then handle missing values in 'marital' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf = pd.get_dummies(df, columns=['job'], prefix=['job'])\ndf['education_scaled'] = (df['education'] - df['education'].min()) / (df['education'].max() - df['education'].min())\ndf['marital'].fillna(df['marital'].median(), inplace=True)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then create a new feature 'sensor_value_ratio' as the ratio of 'sensor_value' to 'sensor_value', then detect outliers in 'status' using the IQR method, then plot a histogram of 'device_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['sensor_value_ratio'] = df['sensor_value'] / df['sensor_value']\nQ1 = df['status'].quantile(0.25)\nQ3 = df['status'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['status'] < (Q1 - 1.5*IQR)) | (df['status'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['device_id'].hist()\nplt.xlabel('device_id')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features, then compute TF-IDF features for column 'status' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['timestamp'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['status'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then handle missing values in 'date_time' by imputing with median, then clean text data in column 'snow_1h' by removing punctuation and stopwords, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['date_time'].fillna(df['date_time'].median(), inplace=True)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['snow_1h_clean'] = df['snow_1h'].apply(clean)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then plot a histogram of 'rain_1h', then create a new feature 'temp_ratio' as the ratio of 'temp' to 'traffic_volume', then one-hot encode the categorical column 'rain_1h'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['rain_1h'].hist()\nplt.xlabel('rain_1h')\nplt.ylabel('Frequency')\nplt.show()\ndf['temp_ratio'] = df['temp'] / df['traffic_volume']\ndf = pd.get_dummies(df, columns=['rain_1h'], prefix=['rain_1h'])"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then plot a histogram of 'deaths', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['deaths'].hist()\nplt.xlabel('deaths')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'length', then perform K-Means clustering with k=3 on numeric features, then create a new feature 'rating_ratio' as the ratio of 'rating' to 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf = pd.get_dummies(df, columns=['length'], prefix=['length'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['rating_ratio'] = df['rating'] / df['sentiment']"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'pm2_5', then normalize the 'date' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['date_scaled'] = (df['date'] - df['date'].min()) / (df['date'].max() - df['date'].min())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then normalize the 'MonthlyCharges' column using min-max scaling, then train a Random Forest Classifier to predict 'Churn', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['MonthlyCharges_scaled'] = (df['MonthlyCharges'] - df['MonthlyCharges'].min()) / (df['MonthlyCharges'].max() - df['MonthlyCharges'].min())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then clean text data in column 'customer_id' by removing punctuation and stopwords, then one-hot encode the categorical column 'transaction_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['customer_id_clean'] = df['customer_id'].apply(clean)\ndf = pd.get_dummies(df, columns=['transaction_id'], prefix=['transaction_id'])"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then clean text data in column 'SalePrice' by removing punctuation and stopwords, then compute TF-IDF features for column 'Neighborhood' and display top 10 words, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['SalePrice_clean'] = df['SalePrice'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Neighborhood'])\nprint(vect.get_feature_names_out())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['OverallQual'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'humidity' and display top 10 words, then clean text data in column 'pressure' by removing punctuation and stopwords, then create a new feature 'consumption_ratio' as the ratio of 'consumption' to 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['humidity'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['pressure_clean'] = df['pressure'].apply(clean)\ndf['consumption_ratio'] = df['consumption'] / df['consumption']"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then create a new feature 'day_of_week_ratio' as the ratio of 'day_of_week' to 'sales', then plot a histogram of 'sales', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['day_of_week_ratio'] = df['day_of_week'] / df['sales']\ndf['sales'].hist()\nplt.xlabel('sales')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'consumption', then clean text data in column 'consumption' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['consumption_clean'] = df['consumption'].apply(clean)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then normalize the 'total_amount' column using min-max scaling, then clean text data in column 'transaction_id' by removing punctuation and stopwords, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['total_amount_scaled'] = (df['total_amount'] - df['total_amount'].min()) / (df['total_amount'].max() - df['total_amount'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['transaction_id_clean'] = df['transaction_id'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features, then detect outliers in 'ecg_reading' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)\nQ1 = df['ecg_reading'].quantile(0.25)\nQ3 = df['ecg_reading'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['ecg_reading'] < (Q1 - 1.5*IQR)) | (df['ecg_reading'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'temperature', then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['precipitation'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'job' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['y'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['job'].quantile(0.25)\nQ3 = df['job'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['job'] < (Q1 - 1.5*IQR)) | (df['job'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'traffic_volume' and display top 10 words, then detect outliers in 'temp' using the IQR method, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['traffic_volume'])\nprint(vect.get_feature_names_out())\nQ1 = df['temp'].quantile(0.25)\nQ3 = df['temp'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['temp'] < (Q1 - 1.5*IQR)) | (df['temp'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then clean text data in column 'OverallQual' by removing punctuation and stopwords, then handle missing values in 'SalePrice' by imputing with median, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['OverallQual_clean'] = df['OverallQual'].apply(clean)\ndf['SalePrice'].fillna(df['SalePrice'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then create a new feature 'departure_delay_ratio' as the ratio of 'departure_delay' to 'arrival_delay', then display summary statistics of all numeric columns using df.describe(), then train a Random Forest Classifier to predict 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['departure_delay_ratio'] = df['departure_delay'] / df['arrival_delay']\nprint(df.describe())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'traffic_volume', then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nprint(df.describe())\ndf['traffic_volume'].hist()\nplt.xlabel('traffic_volume')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'text', then compute TF-IDF features for column 'user_id' and display top 10 words, then clean text data in column 'user_id' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['user_id'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['user_id_clean'] = df['user_id'].apply(clean)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'arrival_delay', then clean text data in column 'distance' by removing punctuation and stopwords, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['distance_clean'] = df['distance'].apply(clean)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then create a new feature 'flight_ratio' as the ratio of 'flight' to 'arrival_delay', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nprint(df.describe())\ndf['flight_ratio'] = df['flight'] / df['arrival_delay']\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then normalize the 'user_id' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['user_id_scaled'] = (df['user_id'] - df['user_id'].min()) / (df['user_id'].max() - df['user_id'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Churn', then perform time-series forecasting using ARIMA to predict the next 12 periods, then plot a histogram of 'TotalCharges'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['TotalCharges'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['TotalCharges'].hist()\nplt.xlabel('TotalCharges')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'traffic_volume' column using min-max scaling, then plot a histogram of 'rain_1h'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['traffic_volume_scaled'] = (df['traffic_volume'] - df['traffic_volume'].min()) / (df['traffic_volume'].max() - df['traffic_volume'].min())\ndf['rain_1h'].hist()\nplt.xlabel('rain_1h')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then detect outliers in 'marital' using the IQR method, then one-hot encode the categorical column 'y', then handle missing values in 'marital' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nQ1 = df['marital'].quantile(0.25)\nQ3 = df['marital'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['marital'] < (Q1 - 1.5*IQR)) | (df['marital'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['y'], prefix=['y'])\ndf['marital'].fillna(df['marital'].median(), inplace=True)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then detect outliers in 'sepal_length' using the IQR method, then one-hot encode the categorical column 'petal_width', then handle missing values in 'sepal_length' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nQ1 = df['sepal_length'].quantile(0.25)\nQ3 = df['sepal_length'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sepal_length'] < (Q1 - 1.5*IQR)) | (df['sepal_length'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['petal_width'], prefix=['petal_width'])\ndf['sepal_length'].fillna(df['sepal_length'].median(), inplace=True)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Random Forest Classifier to predict 'Churn', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Close', then create a new feature 'Close_ratio' as the ratio of 'Close' to 'Close', then handle missing values in 'Low' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['Close_ratio'] = df['Close'] / df['Close']\ndf['Low'].fillna(df['Low'].median(), inplace=True)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'date', then normalize the 'precipitation' column using min-max scaling, then detect outliers in 'temperature' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf = pd.get_dummies(df, columns=['date'], prefix=['date'])\ndf['precipitation_scaled'] = (df['precipitation'] - df['precipitation'].min()) / (df['precipitation'].max() - df['precipitation'].min())\nQ1 = df['temperature'].quantile(0.25)\nQ3 = df['temperature'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['temperature'] < (Q1 - 1.5*IQR)) | (df['temperature'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'arrival_delay', then train a Linear Regression model to predict 'arrival_delay', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then clean text data in column 'flight' by removing punctuation and stopwords, then plot a histogram of 'flight', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['flight_clean'] = df['flight'].apply(clean)\ndf['flight'].hist()\nplt.xlabel('flight')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Revenue'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then clean text data in column 'Volume' by removing punctuation and stopwords, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ncorr = df.corr()\nprint(corr)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Volume_clean'] = df['Volume'].apply(clean)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then clean text data in column 'customer_id' by removing punctuation and stopwords, then compute TF-IDF features for column 'total_amount' and display top 10 words, then handle missing values in 'product_id' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['customer_id_clean'] = df['customer_id'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['total_amount'])\nprint(vect.get_feature_names_out())\ndf['product_id'].fillna(df['product_id'].median(), inplace=True)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sentiment', then detect outliers in 'review' using the IQR method, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['review'].quantile(0.25)\nQ3 = df['review'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['review'] < (Q1 - 1.5*IQR)) | (df['review'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then clean text data in column 'marital' by removing punctuation and stopwords, then normalize the 'y' column using min-max scaling, then one-hot encode the categorical column 'education'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['marital_clean'] = df['marital'].apply(clean)\ndf['y_scaled'] = (df['y'] - df['y'].min()) / (df['y'].max() - df['y'].min())\ndf = pd.get_dummies(df, columns=['education'], prefix=['education'])"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then normalize the 'flight' column using min-max scaling, then compute TF-IDF features for column 'departure_delay' and display top 10 words, then create a new feature 'carrier_ratio' as the ratio of 'carrier' to 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['flight_scaled'] = (df['flight'] - df['flight'].min()) / (df['flight'].max() - df['flight'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['departure_delay'])\nprint(vect.get_feature_names_out())\ndf['carrier_ratio'] = df['carrier'] / df['arrival_delay']"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then normalize the 'flight' column using min-max scaling, then train a Random Forest Classifier to predict 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['flight_scaled'] = (df['flight'] - df['flight'].min()) / (df['flight'].max() - df['flight'].min())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then normalize the 'date' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score, then create a new feature 'pm10_ratio' as the ratio of 'pm10' to 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['date_scaled'] = (df['date'] - df['date'].min()) / (df['date'].max() - df['date'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['pm10_ratio'] = df['pm10'] / df['pm2_5']"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then create a new feature 'MonthlyCharges_ratio' as the ratio of 'MonthlyCharges' to 'Churn', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ncorr = df.corr()\nprint(corr)\ndf['MonthlyCharges_ratio'] = df['MonthlyCharges'] / df['Churn']\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Churn'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then clean text data in column 'oldbalanceOrg' by removing punctuation and stopwords, then create a new feature 'isFraud_ratio' as the ratio of 'isFraud' to 'isFraud', then handle missing values in 'newbalanceOrig' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['oldbalanceOrg_clean'] = df['oldbalanceOrg'].apply(clean)\ndf['isFraud_ratio'] = df['isFraud'] / df['isFraud']\ndf['newbalanceOrig'].fillna(df['newbalanceOrig'].median(), inplace=True)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform K-Means clustering with k=3 on numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nprint(df.describe())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Revenue'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then handle missing values in 'OverallQual' by imputing with median, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['OverallQual'].fillna(df['OverallQual'].median(), inplace=True)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Neighborhood'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'user_id' using the IQR method, then create a new feature 'user_id_ratio' as the ratio of 'user_id' to 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['user_id'].quantile(0.25)\nQ3 = df['user_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['user_id'] < (Q1 - 1.5*IQR)) | (df['user_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['user_id_ratio'] = df['user_id'] / df['text']"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['departure_delay'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then one-hot encode the categorical column 'Region', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nprint(df.describe())\ndf = pd.get_dummies(df, columns=['Region'], prefix=['Region'])\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'so2' column using min-max scaling, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['so2'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['so2_scaled'] = (df['so2'] - df['so2'].min()) / (df['so2'].max() - df['so2'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then normalize the 'timestamp' column using min-max scaling, then perform time-series forecasting using ARIMA to predict the next 12 periods, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['timestamp_scaled'] = (df['timestamp'] - df['timestamp'].min()) / (df['timestamp'].max() - df['timestamp'].min())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sensor_value'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then plot a histogram of 'TotalCharges', then compute TF-IDF features for column 'ContractType' and display top 10 words, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['TotalCharges'].hist()\nplt.xlabel('TotalCharges')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['ContractType'])\nprint(vect.get_feature_names_out())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'traffic_volume', then clean text data in column 'traffic_volume' by removing punctuation and stopwords, then detect outliers in 'date_time' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['traffic_volume_clean'] = df['traffic_volume'].apply(clean)\nQ1 = df['date_time'].quantile(0.25)\nQ3 = df['date_time'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date_time'] < (Q1 - 1.5*IQR)) | (df['date_time'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then plot a histogram of 'store', then train a Random Forest Classifier to predict 'sales', then train a Linear Regression model to predict 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['store'].hist()\nplt.xlabel('store')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'recovered'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['recovered'], prefix=['recovered'])"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'tenure' and display top 10 words, then normalize the 'ContractType' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['tenure'])\nprint(vect.get_feature_names_out())\ndf['ContractType_scaled'] = (df['ContractType'] - df['ContractType'].min()) / (df['ContractType'].max() - df['ContractType'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Product', then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf = pd.get_dummies(df, columns=['Product'], prefix=['Product'])\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Product'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then handle missing values in 'Neighborhood' by imputing with median, then normalize the 'SalePrice' column using min-max scaling, then clean text data in column 'OverallQual' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['Neighborhood'].fillna(df['Neighborhood'].median(), inplace=True)\ndf['SalePrice_scaled'] = (df['SalePrice'] - df['SalePrice'].min()) / (df['SalePrice'].max() - df['SalePrice'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['OverallQual_clean'] = df['OverallQual'].apply(clean)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'status' and display top 10 words, then normalize the 'device_id' column using min-max scaling, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['status'])\nprint(vect.get_feature_names_out())\ndf['device_id_scaled'] = (df['device_id'] - df['device_id'].min()) / (df['device_id'].max() - df['device_id'].min())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features, then handle missing values in 'age' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['education'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['age'].fillna(df['age'].median(), inplace=True)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then plot a histogram of 'petal_width', then normalize the 'petal_length' column using min-max scaling, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['petal_width'].hist()\nplt.xlabel('petal_width')\nplt.ylabel('Frequency')\nplt.show()\ndf['petal_length_scaled'] = (df['petal_length'] - df['petal_length'].min()) / (df['petal_length'].max() - df['petal_length'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then compute TF-IDF features for column 'time' and display top 10 words, then one-hot encode the categorical column 'newbalanceOrig'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['time'])\nprint(vect.get_feature_names_out())\ndf = pd.get_dummies(df, columns=['newbalanceOrig'], prefix=['newbalanceOrig'])"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then detect outliers in 'shares' using the IQR method, then plot a histogram of 'likes', then compute TF-IDF features for column 'text' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nQ1 = df['shares'].quantile(0.25)\nQ3 = df['shares'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['shares'] < (Q1 - 1.5*IQR)) | (df['shares'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['likes'].hist()\nplt.xlabel('likes')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['text'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'precipitation' by imputing with median, then train a Linear Regression model to predict 'temperature', then train a Random Forest Classifier to predict 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['precipitation'].fillna(df['precipitation'].median(), inplace=True)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'traffic_volume', then normalize the 'date_time' column using min-max scaling, then compute TF-IDF features for column 'date_time' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['date_time_scaled'] = (df['date_time'] - df['date_time'].min()) / (df['date_time'].max() - df['date_time'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date_time'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then detect outliers in 'petal_width' using the IQR method, then handle missing values in 'sepal_width' by imputing with median, then train a Random Forest Classifier to predict 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nQ1 = df['petal_width'].quantile(0.25)\nQ3 = df['petal_width'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['petal_width'] < (Q1 - 1.5*IQR)) | (df['petal_width'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['sepal_width'].fillna(df['sepal_width'].median(), inplace=True)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then calculate the correlation matrix for numeric features, then detect outliers in 'tenure' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nprint(df.describe())\ncorr = df.corr()\nprint(corr)\nQ1 = df['tenure'].quantile(0.25)\nQ3 = df['tenure'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['tenure'] < (Q1 - 1.5*IQR)) | (df['tenure'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'open', then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf = pd.get_dummies(df, columns=['open'], prefix=['open'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then clean text data in column 'o3' by removing punctuation and stopwords, then compute TF-IDF features for column 'pm10' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ncorr = df.corr()\nprint(corr)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['o3_clean'] = df['o3'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['pm10'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Revenue', then plot a histogram of 'Date', then create a new feature 'Product_ratio' as the ratio of 'Product' to 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['Date'].hist()\nplt.xlabel('Date')\nplt.ylabel('Frequency')\nplt.show()\ndf['Product_ratio'] = df['Product'] / df['Revenue']"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'pm2_5' and display top 10 words, then clean text data in column 'date' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['pm2_5'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'arrival_delay_ratio' as the ratio of 'arrival_delay' to 'arrival_delay', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['flight'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['arrival_delay_ratio'] = df['arrival_delay'] / df['arrival_delay']\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Revenue', then clean text data in column 'Date' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Date_clean'] = df['Date'].apply(clean)\nprint(df.describe())"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Survived', then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe(), then create a new feature 'arrival_delay_ratio' as the ratio of 'arrival_delay' to 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())\ndf['arrival_delay_ratio'] = df['arrival_delay'] / df['arrival_delay']"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then plot a histogram of 'isFraud', then calculate the correlation matrix for numeric features, then normalize the 'oldbalanceOrg' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['isFraud'].hist()\nplt.xlabel('isFraud')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)\ndf['oldbalanceOrg_scaled'] = (df['oldbalanceOrg'] - df['oldbalanceOrg'].min()) / (df['oldbalanceOrg'].max() - df['oldbalanceOrg'].min())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then plot a histogram of 'sentiment', then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['sentiment'].hist()\nplt.xlabel('sentiment')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'Date' by removing punctuation and stopwords, then create a new feature 'Date_ratio' as the ratio of 'Date' to 'Revenue', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Date_clean'] = df['Date'].apply(clean)\ndf['Date_ratio'] = df['Date'] / df['Revenue']\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then compute TF-IDF features for column 'Date' and display top 10 words, then train a Linear Regression model to predict 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Date'])\nprint(vect.get_feature_names_out())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then handle missing values in 'ContractType' by imputing with median, then compute TF-IDF features for column 'ContractType' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['ContractType'].fillna(df['ContractType'].median(), inplace=True)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['ContractType'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Date', then one-hot encode the categorical column 'Revenue', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Date'].hist()\nplt.xlabel('Date')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['Revenue'], prefix=['Revenue'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Revenue'])\ny = df['Revenue']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then normalize the 'YearBuilt' column using min-max scaling, then plot a histogram of 'SalePrice', then clean text data in column 'Neighborhood' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['YearBuilt_scaled'] = (df['YearBuilt'] - df['YearBuilt'].min()) / (df['YearBuilt'].max() - df['YearBuilt'].min())\ndf['SalePrice'].hist()\nplt.xlabel('SalePrice')\nplt.ylabel('Frequency')\nplt.show()\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Neighborhood_clean'] = df['Neighborhood'].apply(clean)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then handle missing values in 'species' by imputing with median, then compute TF-IDF features for column 'sepal_length' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['species'].fillna(df['species'].median(), inplace=True)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sepal_length'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'isFraud', then handle missing values in 'oldbalanceOrg' by imputing with median, then detect outliers in 'time' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['oldbalanceOrg'].fillna(df['oldbalanceOrg'].median(), inplace=True)\nQ1 = df['time'].quantile(0.25)\nQ3 = df['time'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['time'] < (Q1 - 1.5*IQR)) | (df['time'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then handle missing values in 'user_id' by imputing with median, then one-hot encode the categorical column 'shares'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ncorr = df.corr()\nprint(corr)\ndf['user_id'].fillna(df['user_id'].median(), inplace=True)\ndf = pd.get_dummies(df, columns=['shares'], prefix=['shares'])"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'pm2_5', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['pm2_5'].hist()\nplt.xlabel('pm2_5')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then normalize the 'SalePrice' column using min-max scaling, then clean text data in column 'OverallQual' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nprint(df.describe())\ndf['SalePrice_scaled'] = (df['SalePrice'] - df['SalePrice'].min()) / (df['SalePrice'].max() - df['SalePrice'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['OverallQual_clean'] = df['OverallQual'].apply(clean)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Survived', then clean text data in column 'Fare' by removing punctuation and stopwords, then create a new feature 'Fare_ratio' as the ratio of 'Fare' to 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Fare_clean'] = df['Fare'].apply(clean)\ndf['Fare_ratio'] = df['Fare'] / df['Survived']"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'job' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['job_scaled'] = (df['job'] - df['job'].min()) / (df['job'].max() - df['job'].min())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then handle missing values in 'snow_1h' by imputing with median, then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['snow_1h'].fillna(df['snow_1h'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['temp'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then handle missing values in 'Low' by imputing with median, then one-hot encode the categorical column 'Open', then train a Random Forest Classifier to predict 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Low'].fillna(df['Low'].median(), inplace=True)\ndf = pd.get_dummies(df, columns=['Open'], prefix=['Open'])\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then evaluate the model performance using RMSE and R\u00b2 score, then handle missing values in 'tenure' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['tenure'].fillna(df['tenure'].median(), inplace=True)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then normalize the 'tenure' column using min-max scaling, then detect outliers in 'ContractType' using the IQR method, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['tenure_scaled'] = (df['tenure'] - df['tenure'].min()) / (df['tenure'].max() - df['tenure'].min())\nQ1 = df['ContractType'].quantile(0.25)\nQ3 = df['ContractType'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['ContractType'] < (Q1 - 1.5*IQR)) | (df['ContractType'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'pm2_5', then perform time-series forecasting using ARIMA to predict the next 12 periods, then handle missing values in 'pm10' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['pm2_5'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['pm10'].fillna(df['pm10'].median(), inplace=True)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then perform time-series forecasting using ARIMA to predict the next 12 periods, then compute TF-IDF features for column 'petal_width' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sepal_width'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['petal_width'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then one-hot encode the categorical column 'MonthlyCharges', then handle missing values in 'TotalCharges' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['ContractType'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf = pd.get_dummies(df, columns=['MonthlyCharges'], prefix=['MonthlyCharges'])\ndf['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then create a new feature 'pm2_5_ratio' as the ratio of 'pm2_5' to 'pm2_5', then display feature importances from the Random Forest model, then one-hot encode the categorical column 'o3'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['pm2_5_ratio'] = df['pm2_5'] / df['pm2_5']\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf = pd.get_dummies(df, columns=['o3'], prefix=['o3'])"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'wind_speed', then handle missing values in 'wind_speed' by imputing with median, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf = pd.get_dummies(df, columns=['wind_speed'], prefix=['wind_speed'])\ndf['wind_speed'].fillna(df['wind_speed'].median(), inplace=True)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['humidity'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then normalize the 'total_amount' column using min-max scaling, then detect outliers in 'transaction_id' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['total_amount_scaled'] = (df['total_amount'] - df['total_amount'].min()) / (df['total_amount'].max() - df['total_amount'].min())\nQ1 = df['transaction_id'].quantile(0.25)\nQ3 = df['transaction_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['transaction_id'] < (Q1 - 1.5*IQR)) | (df['transaction_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'Date' and display top 10 words, then clean text data in column 'Volume' by removing punctuation and stopwords, then one-hot encode the categorical column 'Open'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Date'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Volume_clean'] = df['Volume'].apply(clean)\ndf = pd.get_dummies(df, columns=['Open'], prefix=['Open'])"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then one-hot encode the categorical column 'deaths', then clean text data in column 'date' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf = pd.get_dummies(df, columns=['deaths'], prefix=['deaths'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then plot a histogram of 'rain_1h', then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['rain_1h'].hist()\nplt.xlabel('rain_1h')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date_time'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform time-series forecasting using ARIMA to predict the next 12 periods, then handle missing values in 'Date' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nprint(df.describe())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Product'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['Date'].fillna(df['Date'].median(), inplace=True)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'education' column using min-max scaling, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['education_scaled'] = (df['education'] - df['education'].min()) / (df['education'].max() - df['education'].min())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then clean text data in column 'rating' by removing punctuation and stopwords, then detect outliers in 'genre' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['rating_clean'] = df['rating'].apply(clean)\nQ1 = df['genre'].quantile(0.25)\nQ3 = df['genre'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['genre'] < (Q1 - 1.5*IQR)) | (df['genre'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'total_amount', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['total_amount'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then handle missing values in 'Churn' by imputing with median, then perform K-Means clustering with k=3 on numeric features, then normalize the 'MonthlyCharges' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['Churn'].fillna(df['Churn'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['MonthlyCharges_scaled'] = (df['MonthlyCharges'] - df['MonthlyCharges'].min()) / (df['MonthlyCharges'].max() - df['MonthlyCharges'].min())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then create a new feature 'departure_delay_ratio' as the ratio of 'departure_delay' to 'arrival_delay', then one-hot encode the categorical column 'carrier', then normalize the 'departure_delay' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['departure_delay_ratio'] = df['departure_delay'] / df['arrival_delay']\ndf = pd.get_dummies(df, columns=['carrier'], prefix=['carrier'])\ndf['departure_delay_scaled'] = (df['departure_delay'] - df['departure_delay'].min()) / (df['departure_delay'].max() - df['departure_delay'].min())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'ecg_reading', then perform time-series forecasting using ARIMA to predict the next 12 periods, then one-hot encode the categorical column 'heart_rate'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['patient_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf = pd.get_dummies(df, columns=['heart_rate'], prefix=['heart_rate'])"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then detect outliers in 'snow_1h' using the IQR method, then handle missing values in 'rain_1h' by imputing with median, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nQ1 = df['snow_1h'].quantile(0.25)\nQ3 = df['snow_1h'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['snow_1h'] < (Q1 - 1.5*IQR)) | (df['snow_1h'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['rain_1h'].fillna(df['rain_1h'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then detect outliers in 'date' using the IQR method, then create a new feature 'pm2_5_ratio' as the ratio of 'pm2_5' to 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ncorr = df.corr()\nprint(corr)\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['pm2_5_ratio'] = df['pm2_5'] / df['pm2_5']"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'sepal_width', then compute TF-IDF features for column 'petal_length' and display top 10 words, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf = pd.get_dummies(df, columns=['sepal_width'], prefix=['sepal_width'])\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['petal_length'])\nprint(vect.get_feature_names_out())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['job'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'traffic_volume', then perform time-series forecasting using ARIMA to predict the next 12 periods, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['rain_1h'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'temperature', then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf = pd.get_dummies(df, columns=['temperature'], prefix=['temperature'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then evaluate the model performance using RMSE and R\u00b2 score, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then normalize the 'Survived' column using min-max scaling, then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'Age'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Survived_scaled'] = (df['Survived'] - df['Survived'].min()) / (df['Survived'].max() - df['Survived'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['Age'].hist()\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then normalize the 'time' column using min-max scaling, then create a new feature 'patient_id_ratio' as the ratio of 'patient_id' to 'ecg_reading', then train a Linear Regression model to predict 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['time_scaled'] = (df['time'] - df['time'].min()) / (df['time'].max() - df['time'].min())\ndf['patient_id_ratio'] = df['patient_id'] / df['ecg_reading']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then normalize the 'flight' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score, then plot a histogram of 'carrier'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['flight_scaled'] = (df['flight'] - df['flight'].min()) / (df['flight'].max() - df['flight'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['carrier'].hist()\nplt.xlabel('carrier')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Random Forest Classifier to predict 'Survived', then compute TF-IDF features for column 'Fare' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nprint(df.describe())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Fare'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then handle missing values in 'post_id' by imputing with median, then compute TF-IDF features for column 'shares' and display top 10 words, then clean text data in column 'text' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['post_id'].fillna(df['post_id'].median(), inplace=True)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['shares'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['text_clean'] = df['text'].apply(clean)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then clean text data in column 'time' by removing punctuation and stopwords, then normalize the 'isFraud' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['time_clean'] = df['time'].apply(clean)\ndf['isFraud_scaled'] = (df['isFraud'] - df['isFraud'].min()) / (df['isFraud'].max() - df['isFraud'].min())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then create a new feature 'precipitation_ratio' as the ratio of 'precipitation' to 'temperature', then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['precipitation_ratio'] = df['precipitation'] / df['temperature']\nprint(df.describe())\ndf['temperature'].hist()\nplt.xlabel('temperature')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then detect outliers in 'text' using the IQR method, then plot a histogram of 'user_id', then clean text data in column 'text' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nQ1 = df['text'].quantile(0.25)\nQ3 = df['text'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['text'] < (Q1 - 1.5*IQR)) | (df['text'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['user_id'].hist()\nplt.xlabel('user_id')\nplt.ylabel('Frequency')\nplt.show()\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['text_clean'] = df['text'].apply(clean)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'ecg_reading' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['ecg_reading_scaled'] = (df['ecg_reading'] - df['ecg_reading'].min()) / (df['ecg_reading'].max() - df['ecg_reading'].min())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then clean text data in column 'deaths' by removing punctuation and stopwords, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['deaths_clean'] = df['deaths'].apply(clean)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'total_amount' and display top 10 words, then train a Random Forest Classifier to predict 'total_amount', then plot a histogram of 'quantity'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['total_amount'])\nprint(vect.get_feature_names_out())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['quantity'].hist()\nplt.xlabel('quantity')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then normalize the 'recovered' column using min-max scaling, then clean text data in column 'country' by removing punctuation and stopwords, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['recovered_scaled'] = (df['recovered'] - df['recovered'].min()) / (df['recovered'].max() - df['recovered'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['country_clean'] = df['country'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'arrival_delay' and display top 10 words, then plot a histogram of 'carrier', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['arrival_delay'])\nprint(vect.get_feature_names_out())\ndf['carrier'].hist()\nplt.xlabel('carrier')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['departure_delay'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'SalePrice', then train a Linear Regression model to predict 'SalePrice', then detect outliers in 'OverallQual' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf = pd.get_dummies(df, columns=['SalePrice'], prefix=['SalePrice'])\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nQ1 = df['OverallQual'].quantile(0.25)\nQ3 = df['OverallQual'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['OverallQual'] < (Q1 - 1.5*IQR)) | (df['OverallQual'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'timestamp' by imputing with median, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nprint(df.describe())\ndf['timestamp'].fillna(df['timestamp'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'consumption', then train a Random Forest Classifier to predict 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['humidity'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then plot a histogram of 'humidity', then one-hot encode the categorical column 'temperature', then normalize the 'date' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['humidity'].hist()\nplt.xlabel('humidity')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['temperature'], prefix=['temperature'])\ndf['date_scaled'] = (df['date'] - df['date'].min()) / (df['date'].max() - df['date'].min())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display feature importances from the Random Forest model, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['patient_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nprint(df.describe())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then handle missing values in 'length' by imputing with median, then normalize the 'length' column using min-max scaling, then train a Linear Regression model to predict 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['length'].fillna(df['length'].median(), inplace=True)\ndf['length_scaled'] = (df['length'] - df['length'].min()) / (df['length'].max() - df['length'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then one-hot encode the categorical column 'arrival_delay', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf = pd.get_dummies(df, columns=['arrival_delay'], prefix=['arrival_delay'])\nprint(df.describe())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then handle missing values in 'genre' by imputing with median, then create a new feature 'sentiment_ratio' as the ratio of 'sentiment' to 'sentiment', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['genre'].fillna(df['genre'].median(), inplace=True)\ndf['sentiment_ratio'] = df['sentiment'] / df['sentiment']\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'Date' by imputing with median, then one-hot encode the categorical column 'Product', then clean text data in column 'Product' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Date'].fillna(df['Date'].median(), inplace=True)\ndf = pd.get_dummies(df, columns=['Product'], prefix=['Product'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Product_clean'] = df['Product'].apply(clean)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then detect outliers in 'o3' using the IQR method, then train a Random Forest Classifier to predict 'pm2_5', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nQ1 = df['o3'].quantile(0.25)\nQ3 = df['o3'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['o3'] < (Q1 - 1.5*IQR)) | (df['o3'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'OverallQual' and display top 10 words, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['OverallQual'])\nprint(vect.get_feature_names_out())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['LotArea'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nprint(df.describe())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'date', then calculate the correlation matrix for numeric features, then one-hot encode the categorical column 'precipitation'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['date'].hist()\nplt.xlabel('date')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)\ndf = pd.get_dummies(df, columns=['precipitation'], prefix=['precipitation'])"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'length'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nprint(df.describe())\ndf['length'].hist()\nplt.xlabel('length')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then calculate the correlation matrix for numeric features, then plot a histogram of 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ncorr = df.corr()\nprint(corr)\ndf['traffic_volume'].hist()\nplt.xlabel('traffic_volume')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'length' by imputing with median, then clean text data in column 'rating' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nprint(df.describe())\ndf['length'].fillna(df['length'].median(), inplace=True)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['rating_clean'] = df['rating'].apply(clean)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then perform K-Means clustering with k=3 on numeric features, then create a new feature 'length_ratio' as the ratio of 'length' to 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['length_ratio'] = df['length'] / df['sentiment']"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then handle missing values in 'pm10' by imputing with median, then one-hot encode the categorical column 'so2', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['pm10'].fillna(df['pm10'].median(), inplace=True)\ndf = pd.get_dummies(df, columns=['so2'], prefix=['so2'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then handle missing values in 'education' by imputing with median, then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['education'].fillna(df['education'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['y'], prefix=['y'])"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'product_id_ratio' as the ratio of 'product_id' to 'total_amount', then detect outliers in 'transaction_id' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['transaction_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['product_id_ratio'] = df['product_id'] / df['total_amount']\nQ1 = df['transaction_id'].quantile(0.25)\nQ3 = df['transaction_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['transaction_id'] < (Q1 - 1.5*IQR)) | (df['transaction_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then clean text data in column 'sepal_width' by removing punctuation and stopwords, then train a Linear Regression model to predict 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sepal_width_clean'] = df['sepal_width'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model, then handle missing values in 'review' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['review'].fillna(df['review'].median(), inplace=True)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'recovered' and display top 10 words, then train a Linear Regression model to predict 'confirmed', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['recovered'])\nprint(vect.get_feature_names_out())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then detect outliers in 'education' using the IQR method, then train a Linear Regression model to predict 'y', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nQ1 = df['education'].quantile(0.25)\nQ3 = df['education'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['education'] < (Q1 - 1.5*IQR)) | (df['education'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'timestamp', then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf = pd.get_dummies(df, columns=['timestamp'], prefix=['timestamp'])\nprint(df.describe())\ndf['sensor_value'].hist()\nplt.xlabel('sensor_value')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sales', then one-hot encode the categorical column 'store', then compute TF-IDF features for column 'customers' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['store'], prefix=['store'])\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['customers'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'product_id' and display top 10 words, then detect outliers in 'total_amount' using the IQR method, then train a Linear Regression model to predict 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['product_id'])\nprint(vect.get_feature_names_out())\nQ1 = df['total_amount'].quantile(0.25)\nQ3 = df['total_amount'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['total_amount'] < (Q1 - 1.5*IQR)) | (df['total_amount'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then clean text data in column 'petal_width' by removing punctuation and stopwords, then normalize the 'sepal_length' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['petal_width_clean'] = df['petal_width'].apply(clean)\ndf['sepal_length_scaled'] = (df['sepal_length'] - df['sepal_length'].min()) / (df['sepal_length'].max() - df['sepal_length'].min())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'arrival_delay' column using min-max scaling, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['arrival_delay_scaled'] = (df['arrival_delay'] - df['arrival_delay'].min()) / (df['arrival_delay'].max() - df['arrival_delay'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'pressure' and display top 10 words, then detect outliers in 'pressure' using the IQR method, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['pressure'])\nprint(vect.get_feature_names_out())\nQ1 = df['pressure'].quantile(0.25)\nQ3 = df['pressure'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['pressure'] < (Q1 - 1.5*IQR)) | (df['pressure'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display feature importances from the Random Forest model, then detect outliers in 'y' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['y'].quantile(0.25)\nQ3 = df['y'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['y'] < (Q1 - 1.5*IQR)) | (df['y'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then detect outliers in 'Sex' using the IQR method, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['Sex'].quantile(0.25)\nQ3 = df['Sex'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Sex'] < (Q1 - 1.5*IQR)) | (df['Sex'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'patient_id' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['ecg_reading'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['patient_id_scaled'] = (df['patient_id'] - df['patient_id'].min()) / (df['patient_id'].max() - df['patient_id'].min())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then handle missing values in 'Revenue' by imputing with median, then normalize the 'Date' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['Revenue'].fillna(df['Revenue'].median(), inplace=True)\ndf['Date_scaled'] = (df['Date'] - df['Date'].min()) / (df['Date'].max() - df['Date'].min())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then detect outliers in 'date' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods, then one-hot encode the categorical column 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['recovered'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf = pd.get_dummies(df, columns=['confirmed'], prefix=['confirmed'])"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'consumption', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'traffic_volume', then normalize the 'rain_1h' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['rain_1h_scaled'] = (df['rain_1h'] - df['rain_1h'].min()) / (df['rain_1h'].max() - df['rain_1h'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then create a new feature 'flight_ratio' as the ratio of 'flight' to 'arrival_delay', then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'departure_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['flight_ratio'] = df['flight'] / df['arrival_delay']\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['departure_delay'], prefix=['departure_delay'])"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then handle missing values in 'date' by imputing with median, then train a Random Forest Classifier to predict 'consumption', then one-hot encode the categorical column 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['date'].fillna(df['date'].median(), inplace=True)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['temperature'], prefix=['temperature'])"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then detect outliers in 'total_amount' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score, then create a new feature 'quantity_ratio' as the ratio of 'quantity' to 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nQ1 = df['total_amount'].quantile(0.25)\nQ3 = df['total_amount'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['total_amount'] < (Q1 - 1.5*IQR)) | (df['total_amount'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['quantity_ratio'] = df['quantity'] / df['total_amount']"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then detect outliers in 'open' using the IQR method, then clean text data in column 'store' by removing punctuation and stopwords, then train a Random Forest Classifier to predict 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nQ1 = df['open'].quantile(0.25)\nQ3 = df['open'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['open'] < (Q1 - 1.5*IQR)) | (df['open'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['store_clean'] = df['store'].apply(clean)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then clean text data in column 'Volume' by removing punctuation and stopwords, then train a Linear Regression model to predict 'Close', then detect outliers in 'Close' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Volume_clean'] = df['Volume'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nQ1 = df['Close'].quantile(0.25)\nQ3 = df['Close'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Close'] < (Q1 - 1.5*IQR)) | (df['Close'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'ecg_reading' by removing punctuation and stopwords, then one-hot encode the categorical column 'heart_rate', then handle missing values in 'patient_id' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['ecg_reading_clean'] = df['ecg_reading'].apply(clean)\ndf = pd.get_dummies(df, columns=['heart_rate'], prefix=['heart_rate'])\ndf['patient_id'].fillna(df['patient_id'].median(), inplace=True)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'Close', then detect outliers in 'Low' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['High'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nQ1 = df['Low'].quantile(0.25)\nQ3 = df['Low'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Low'] < (Q1 - 1.5*IQR)) | (df['Low'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'sepal_width', then create a new feature 'petal_width_ratio' as the ratio of 'petal_width' to 'species', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf = pd.get_dummies(df, columns=['sepal_width'], prefix=['sepal_width'])\ndf['petal_width_ratio'] = df['petal_width'] / df['species']\nprint(df.describe())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then plot a histogram of 'confirmed', then create a new feature 'date_ratio' as the ratio of 'date' to 'confirmed', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['confirmed'].hist()\nplt.xlabel('confirmed')\nplt.ylabel('Frequency')\nplt.show()\ndf['date_ratio'] = df['date'] / df['confirmed']\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'tenure' and display top 10 words, then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'tenure' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['tenure'])\nprint(vect.get_feature_names_out())\nprint(df.describe())\nQ1 = df['tenure'].quantile(0.25)\nQ3 = df['tenure'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['tenure'] < (Q1 - 1.5*IQR)) | (df['tenure'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then perform K-Means clustering with k=3 on numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['quality'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Open', then handle missing values in 'Low' by imputing with median, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf = pd.get_dummies(df, columns=['Open'], prefix=['Open'])\ndf['Low'].fillna(df['Low'].median(), inplace=True)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then handle missing values in 'education' by imputing with median, then normalize the 'marital' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['education'].fillna(df['education'].median(), inplace=True)\ndf['marital_scaled'] = (df['marital'] - df['marital'].min()) / (df['marital'].max() - df['marital'].min())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'patient_id' and display top 10 words, then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['patient_id'])\nprint(vect.get_feature_names_out())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['time'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then train a Linear Regression model to predict 'SalePrice', then one-hot encode the categorical column 'OverallQual'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['OverallQual'], prefix=['OverallQual'])"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then plot a histogram of 'likes', then one-hot encode the categorical column 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['post_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['likes'].hist()\nplt.xlabel('likes')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['text'], prefix=['text'])"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then normalize the 'sepal_length' column using min-max scaling, then train a Linear Regression model to predict 'species', then compute TF-IDF features for column 'petal_width' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['sepal_length_scaled'] = (df['sepal_length'] - df['sepal_length'].min()) / (df['sepal_length'].max() - df['sepal_length'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['petal_width'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then normalize the 'MonthlyCharges' column using min-max scaling, then clean text data in column 'Churn' by removing punctuation and stopwords, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['MonthlyCharges_scaled'] = (df['MonthlyCharges'] - df['MonthlyCharges'].min()) / (df['MonthlyCharges'].max() - df['MonthlyCharges'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Churn_clean'] = df['Churn'].apply(clean)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then handle missing values in 'pressure' by imputing with median, then detect outliers in 'temperature' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ncorr = df.corr()\nprint(corr)\ndf['pressure'].fillna(df['pressure'].median(), inplace=True)\nQ1 = df['temperature'].quantile(0.25)\nQ3 = df['temperature'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['temperature'] < (Q1 - 1.5*IQR)) | (df['temperature'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'y', then train a Linear Regression model to predict 'y', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf = pd.get_dummies(df, columns=['y'], prefix=['y'])\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then normalize the 'heart_rate' column using min-max scaling, then perform K-Means clustering with k=3 on numeric features, then clean text data in column 'quality' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['heart_rate_scaled'] = (df['heart_rate'] - df['heart_rate'].min()) / (df['heart_rate'].max() - df['heart_rate'].min())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['quality_clean'] = df['quality'].apply(clean)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'Region' by removing punctuation and stopwords, then create a new feature 'UnitsSold_ratio' as the ratio of 'UnitsSold' to 'Revenue', then one-hot encode the categorical column 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Region_clean'] = df['Region'].apply(clean)\ndf['UnitsSold_ratio'] = df['UnitsSold'] / df['Revenue']\ndf = pd.get_dummies(df, columns=['Revenue'], prefix=['Revenue'])"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model, then detect outliers in 'Age' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['Age'].quantile(0.25)\nQ3 = df['Age'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Age'] < (Q1 - 1.5*IQR)) | (df['Age'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then create a new feature 'petal_width_ratio' as the ratio of 'petal_width' to 'species', then handle missing values in 'sepal_width' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['petal_width_ratio'] = df['petal_width'] / df['species']\ndf['sepal_width'].fillna(df['sepal_width'].median(), inplace=True)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then create a new feature 'Volume_ratio' as the ratio of 'Volume' to 'Close', then evaluate the model performance using RMSE and R\u00b2 score, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Volume_ratio'] = df['Volume'] / df['Close']\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'SalePrice', then handle missing values in 'Neighborhood' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['Neighborhood'].fillna(df['Neighborhood'].median(), inplace=True)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then compute TF-IDF features for column 'Region' and display top 10 words, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nprint(df.describe())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Region'])\nprint(vect.get_feature_names_out())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'Product' by imputing with median, then plot a histogram of 'UnitsSold', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Product'].fillna(df['Product'].median(), inplace=True)\ndf['UnitsSold'].hist()\nplt.xlabel('UnitsSold')\nplt.ylabel('Frequency')\nplt.show()\nprint(df.describe())"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'SalePrice', then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'Neighborhood'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['Neighborhood'], prefix=['Neighborhood'])"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then normalize the 'humidity' column using min-max scaling, then compute TF-IDF features for column 'temperature' and display top 10 words, then clean text data in column 'humidity' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['humidity_scaled'] = (df['humidity'] - df['humidity'].min()) / (df['humidity'].max() - df['humidity'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['temperature'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['humidity_clean'] = df['humidity'].apply(clean)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Survived', then handle missing values in 'Age' by imputing with median, then normalize the 'Sex' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['Age'].fillna(df['Age'].median(), inplace=True)\ndf['Sex_scaled'] = (df['Sex'] - df['Sex'].min()) / (df['Sex'].max() - df['Sex'].min())"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'total_amount', then detect outliers in 'customer_id' using the IQR method, then compute TF-IDF features for column 'quantity' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nQ1 = df['customer_id'].quantile(0.25)\nQ3 = df['customer_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['customer_id'] < (Q1 - 1.5*IQR)) | (df['customer_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['quantity'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then clean text data in column 'departure_delay' by removing punctuation and stopwords, then plot a histogram of 'departure_delay', then handle missing values in 'flight' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['departure_delay_clean'] = df['departure_delay'].apply(clean)\ndf['departure_delay'].hist()\nplt.xlabel('departure_delay')\nplt.ylabel('Frequency')\nplt.show()\ndf['flight'].fillna(df['flight'].median(), inplace=True)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then normalize the 'date' column using min-max scaling, then clean text data in column 'date' by removing punctuation and stopwords, then create a new feature 'humidity_ratio' as the ratio of 'humidity' to 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['date_scaled'] = (df['date'] - df['date'].min()) / (df['date'].max() - df['date'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)\ndf['humidity_ratio'] = df['humidity'] / df['temperature']"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'o3' and display top 10 words, then handle missing values in 'date' by imputing with median, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['o3'])\nprint(vect.get_feature_names_out())\ndf['date'].fillna(df['date'].median(), inplace=True)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then normalize the 'total_amount' column using min-max scaling, then detect outliers in 'total_amount' using the IQR method, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['total_amount_scaled'] = (df['total_amount'] - df['total_amount'].min()) / (df['total_amount'].max() - df['total_amount'].min())\nQ1 = df['total_amount'].quantile(0.25)\nQ3 = df['total_amount'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['total_amount'] < (Q1 - 1.5*IQR)) | (df['total_amount'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then normalize the 'date_time' column using min-max scaling, then perform time-series forecasting using ARIMA to predict the next 12 periods, then clean text data in column 'temp' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['date_time_scaled'] = (df['date_time'] - df['date_time'].min()) / (df['date_time'].max() - df['date_time'].min())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['rain_1h'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['temp_clean'] = df['temp'].apply(clean)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then handle missing values in 'total_amount' by imputing with median, then perform K-Means clustering with k=3 on numeric features, then normalize the 'transaction_id' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['total_amount'].fillna(df['total_amount'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['transaction_id_scaled'] = (df['transaction_id'] - df['transaction_id'].min()) / (df['transaction_id'].max() - df['transaction_id'].min())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'pm10' and display top 10 words, then clean text data in column 'pm10' by removing punctuation and stopwords, then train a Random Forest Classifier to predict 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['pm10'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['pm10_clean'] = df['pm10'].apply(clean)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'post_id', then create a new feature 'text_ratio' as the ratio of 'text' to 'text', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf = pd.get_dummies(df, columns=['post_id'], prefix=['post_id'])\ndf['text_ratio'] = df['text'] / df['text']\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['post_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then train a Linear Regression model to predict 'Revenue', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'store', then display feature importances from the Random Forest model, then train a Random Forest Classifier to predict 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf = pd.get_dummies(df, columns=['store'], prefix=['store'])\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'isFraud' and display top 10 words, then train a Linear Regression model to predict 'isFraud', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['isFraud'])\nprint(vect.get_feature_names_out())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then calculate the correlation matrix for numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['recovered'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ncorr = df.corr()\nprint(corr)\nprint(df.describe())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then detect outliers in 'rain_1h' using the IQR method, then calculate the correlation matrix for numeric features, then one-hot encode the categorical column 'temp'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nQ1 = df['rain_1h'].quantile(0.25)\nQ3 = df['rain_1h'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['rain_1h'] < (Q1 - 1.5*IQR)) | (df['rain_1h'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ncorr = df.corr()\nprint(corr)\ndf = pd.get_dummies(df, columns=['temp'], prefix=['temp'])"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then detect outliers in 'Churn' using the IQR method, then handle missing values in 'tenure' by imputing with median, then train a Linear Regression model to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nQ1 = df['Churn'].quantile(0.25)\nQ3 = df['Churn'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Churn'] < (Q1 - 1.5*IQR)) | (df['Churn'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['tenure'].fillna(df['tenure'].median(), inplace=True)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'ecg_reading', then calculate the correlation matrix for numeric features, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then plot a histogram of 'oldbalanceOrg', then calculate the correlation matrix for numeric features, then create a new feature 'newbalanceOrig_ratio' as the ratio of 'newbalanceOrig' to 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['oldbalanceOrg'].hist()\nplt.xlabel('oldbalanceOrg')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)\ndf['newbalanceOrig_ratio'] = df['newbalanceOrig'] / df['isFraud']"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then normalize the 'post_id' column using min-max scaling, then create a new feature 'post_id_ratio' as the ratio of 'post_id' to 'text', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['post_id_scaled'] = (df['post_id'] - df['post_id'].min()) / (df['post_id'].max() - df['post_id'].min())\ndf['post_id_ratio'] = df['post_id'] / df['text']\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then handle missing values in 'SalePrice' by imputing with median, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['SalePrice'].fillna(df['SalePrice'].median(), inplace=True)\nprint(df.describe())"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then perform K-Means clustering with k=3 on numeric features, then compute TF-IDF features for column 'transaction_id' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['transaction_id'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then create a new feature 'petal_width_ratio' as the ratio of 'petal_width' to 'species', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nprint(df.describe())\ndf['petal_width_ratio'] = df['petal_width'] / df['species']\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sepal_width'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then create a new feature 'carrier_ratio' as the ratio of 'carrier' to 'arrival_delay', then calculate the correlation matrix for numeric features, then one-hot encode the categorical column 'flight'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['carrier_ratio'] = df['carrier'] / df['arrival_delay']\ncorr = df.corr()\nprint(corr)\ndf = pd.get_dummies(df, columns=['flight'], prefix=['flight'])"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'rating', then clean text data in column 'rating' by removing punctuation and stopwords, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf = pd.get_dummies(df, columns=['rating'], prefix=['rating'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['rating_clean'] = df['rating'].apply(clean)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then plot a histogram of 'humidity', then normalize the 'humidity' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['humidity'].hist()\nplt.xlabel('humidity')\nplt.ylabel('Frequency')\nplt.show()\ndf['humidity_scaled'] = (df['humidity'] - df['humidity'].min()) / (df['humidity'].max() - df['humidity'].min())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'ContractType' and display top 10 words, then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['ContractType'])\nprint(vect.get_feature_names_out())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'confirmed', then calculate the correlation matrix for numeric features, then one-hot encode the categorical column 'recovered'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)\ndf = pd.get_dummies(df, columns=['recovered'], prefix=['recovered'])"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display summary statistics of all numeric columns using df.describe(), then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['pm10'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nprint(df.describe())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'traffic_volume', then clean text data in column 'rain_1h' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['rain_1h_clean'] = df['rain_1h'].apply(clean)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'ecg_reading' and display top 10 words, then normalize the 'time' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['ecg_reading'])\nprint(vect.get_feature_names_out())\ndf['time_scaled'] = (df['time'] - df['time'].min()) / (df['time'].max() - df['time'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then normalize the 'Low' column using min-max scaling, then compute TF-IDF features for column 'High' and display top 10 words, then plot a histogram of 'Open'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Low_scaled'] = (df['Low'] - df['Low'].min()) / (df['Low'].max() - df['Low'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['High'])\nprint(vect.get_feature_names_out())\ndf['Open'].hist()\nplt.xlabel('Open')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then train a Linear Regression model to predict 'SalePrice', then normalize the 'LotArea' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['LotArea_scaled'] = (df['LotArea'] - df['LotArea'].min()) / (df['LotArea'].max() - df['LotArea'].min())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then handle missing values in 'recovered' by imputing with median, then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['recovered'].fillna(df['recovered'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['confirmed'].hist()\nplt.xlabel('confirmed')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Churn', then one-hot encode the categorical column 'tenure', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['tenure'], prefix=['tenure'])\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Churn'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then handle missing values in 'sepal_length' by imputing with median, then plot a histogram of 'sepal_length', then train a Linear Regression model to predict 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['sepal_length'].fillna(df['sepal_length'].median(), inplace=True)\ndf['sepal_length'].hist()\nplt.xlabel('sepal_length')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then clean text data in column 'transaction_id' by removing punctuation and stopwords, then compute TF-IDF features for column 'product_id' and display top 10 words, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['transaction_id_clean'] = df['transaction_id'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['product_id'])\nprint(vect.get_feature_names_out())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then clean text data in column 'Date' by removing punctuation and stopwords, then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Date_clean'] = df['Date'].apply(clean)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Low'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Revenue', then display feature importances from the Random Forest model, then compute TF-IDF features for column 'Revenue' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Revenue'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'location', then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'timestamp_ratio' as the ratio of 'timestamp' to 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf = pd.get_dummies(df, columns=['location'], prefix=['location'])\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['status'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['timestamp_ratio'] = df['timestamp'] / df['sensor_value']"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then calculate the correlation matrix for numeric features, then plot a histogram of 'Age'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nprint(df.describe())\ncorr = df.corr()\nprint(corr)\ndf['Age'].hist()\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then plot a histogram of 'isFraud', then one-hot encode the categorical column 'isFraud', then clean text data in column 'newbalanceOrig' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['isFraud'].hist()\nplt.xlabel('isFraud')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['isFraud'], prefix=['isFraud'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['newbalanceOrig_clean'] = df['newbalanceOrig'].apply(clean)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'arrival_delay', then clean text data in column 'carrier' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['carrier_clean'] = df['carrier'].apply(clean)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then clean text data in column 'Volume' by removing punctuation and stopwords, then display feature importances from the Random Forest model, then detect outliers in 'Date' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Volume_clean'] = df['Volume'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['Date'].quantile(0.25)\nQ3 = df['Date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Date'] < (Q1 - 1.5*IQR)) | (df['Date'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'arrival_delay', then perform K-Means clustering with k=3 on numeric features, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'traffic_volume', then normalize the 'traffic_volume' column using min-max scaling, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['traffic_volume_scaled'] = (df['traffic_volume'] - df['traffic_volume'].min()) / (df['traffic_volume'].max() - df['traffic_volume'].min())\nprint(df.describe())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then train a Linear Regression model to predict 'Churn', then train a Random Forest Classifier to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then clean text data in column 'product_id' by removing punctuation and stopwords, then normalize the 'product_id' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nprint(df.describe())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['product_id_clean'] = df['product_id'].apply(clean)\ndf['product_id_scaled'] = (df['product_id'] - df['product_id'].min()) / (df['product_id'].max() - df['product_id'].min())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then detect outliers in 'oldbalanceOrg' using the IQR method, then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'time'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nQ1 = df['oldbalanceOrg'].quantile(0.25)\nQ3 = df['oldbalanceOrg'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['oldbalanceOrg'] < (Q1 - 1.5*IQR)) | (df['oldbalanceOrg'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['time'].hist()\nplt.xlabel('time')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then calculate the correlation matrix for numeric features, then plot a histogram of 'country'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ncorr = df.corr()\nprint(corr)\ndf['country'].hist()\nplt.xlabel('country')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'education' column using min-max scaling, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['y'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['education_scaled'] = (df['education'] - df['education'].min()) / (df['education'].max() - df['education'].min())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then clean text data in column 'Volume' by removing punctuation and stopwords, then compute TF-IDF features for column 'Close' and display top 10 words, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Volume_clean'] = df['Volume'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Close'])\nprint(vect.get_feature_names_out())\nprint(df.describe())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then handle missing values in 'petal_length' by imputing with median, then display feature importances from the Random Forest model, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['petal_length'].fillna(df['petal_length'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['total_amount'], prefix=['total_amount'])"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then detect outliers in 'tenure' using the IQR method, then handle missing values in 'ContractType' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nQ1 = df['tenure'].quantile(0.25)\nQ3 = df['tenure'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['tenure'] < (Q1 - 1.5*IQR)) | (df['tenure'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['ContractType'].fillna(df['ContractType'].median(), inplace=True)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Revenue', then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Revenue'].hist()\nplt.xlabel('Revenue')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Revenue'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Revenue'])\ny = df['Revenue']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'education', then detect outliers in 'education' using the IQR method, then train a Linear Regression model to predict 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf = pd.get_dummies(df, columns=['education'], prefix=['education'])\nQ1 = df['education'].quantile(0.25)\nQ3 = df['education'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['education'] < (Q1 - 1.5*IQR)) | (df['education'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features, then handle missing values in 'customers' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['customers'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['customers'].fillna(df['customers'].median(), inplace=True)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then normalize the 'Product' column using min-max scaling, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Product_scaled'] = (df['Product'] - df['Product'].min()) / (df['Product'].max() - df['Product'].min())\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then detect outliers in 'humidity' using the IQR method, then create a new feature 'date_ratio' as the ratio of 'date' to 'consumption', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nQ1 = df['humidity'].quantile(0.25)\nQ3 = df['humidity'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['humidity'] < (Q1 - 1.5*IQR)) | (df['humidity'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['date_ratio'] = df['date'] / df['consumption']\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then clean text data in column 'Low' by removing punctuation and stopwords, then plot a histogram of 'Low'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Close'])\ny = df['Close']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Low_clean'] = df['Low'].apply(clean)\ndf['Low'].hist()\nplt.xlabel('Low')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'likes' column using min-max scaling, then create a new feature 'post_id_ratio' as the ratio of 'post_id' to 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['post_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['likes_scaled'] = (df['likes'] - df['likes'].min()) / (df['likes'].max() - df['likes'].min())\ndf['post_id_ratio'] = df['post_id'] / df['text']"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then normalize the 'sepal_width' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['sepal_width_scaled'] = (df['sepal_width'] - df['sepal_width'].min()) / (df['sepal_width'].max() - df['sepal_width'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then plot a histogram of 'review', then train a Random Forest Classifier to predict 'sentiment', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['review'].hist()\nplt.xlabel('review')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Random Forest Classifier to predict 'Survived', then one-hot encode the categorical column 'Pclass'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Age'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['Pclass'], prefix=['Pclass'])"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then detect outliers in 'marital' using the IQR method, then perform K-Means clustering with k=3 on numeric features, then plot a histogram of 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nQ1 = df['marital'].quantile(0.25)\nQ3 = df['marital'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['marital'] < (Q1 - 1.5*IQR)) | (df['marital'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['y'].hist()\nplt.xlabel('y')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'Region' and display top 10 words, then display summary statistics of all numeric columns using df.describe(), then create a new feature 'Region_ratio' as the ratio of 'Region' to 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Region'])\nprint(vect.get_feature_names_out())\nprint(df.describe())\ndf['Region_ratio'] = df['Region'] / df['Revenue']"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then clean text data in column 'customer_id' by removing punctuation and stopwords, then plot a histogram of 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['total_amount'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['customer_id_clean'] = df['customer_id'].apply(clean)\ndf['total_amount'].hist()\nplt.xlabel('total_amount')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then clean text data in column 'status' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['timestamp'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['status_clean'] = df['status'].apply(clean)\nprint(df.describe())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'TotalCharges' and display top 10 words, then train a Linear Regression model to predict 'Churn', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['TotalCharges'])\nprint(vect.get_feature_names_out())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then normalize the 'tenure' column using min-max scaling, then calculate the correlation matrix for numeric features, then one-hot encode the categorical column 'ContractType'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['tenure_scaled'] = (df['tenure'] - df['tenure'].min()) / (df['tenure'].max() - df['tenure'].min())\ncorr = df.corr()\nprint(corr)\ndf = pd.get_dummies(df, columns=['ContractType'], prefix=['ContractType'])"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then create a new feature 'confirmed_ratio' as the ratio of 'confirmed' to 'confirmed', then train a Random Forest Classifier to predict 'confirmed', then clean text data in column 'country' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['confirmed_ratio'] = df['confirmed'] / df['confirmed']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['country_clean'] = df['country'].apply(clean)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then create a new feature 'text_ratio' as the ratio of 'text' to 'text', then clean text data in column 'user_id' by removing punctuation and stopwords, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['text_ratio'] = df['text'] / df['text']\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['user_id_clean'] = df['user_id'].apply(clean)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then normalize the 'SalePrice' column using min-max scaling, then create a new feature 'LotArea_ratio' as the ratio of 'LotArea' to 'SalePrice'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nprint(df.describe())\ndf['SalePrice_scaled'] = (df['SalePrice'] - df['SalePrice'].min()) / (df['SalePrice'].max() - df['SalePrice'].min())\ndf['LotArea_ratio'] = df['LotArea'] / df['SalePrice']"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then handle missing values in 'no2' by imputing with median, then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['no2'].fillna(df['no2'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['o3'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then clean text data in column 'sepal_width' by removing punctuation and stopwords, then display feature importances from the Random Forest model, then detect outliers in 'sepal_length' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sepal_width_clean'] = df['sepal_width'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['sepal_length'].quantile(0.25)\nQ3 = df['sepal_length'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sepal_length'] < (Q1 - 1.5*IQR)) | (df['sepal_length'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then create a new feature 'quality_ratio' as the ratio of 'quality' to 'ecg_reading', then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['quality_ratio'] = df['quality'] / df['ecg_reading']\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['heart_rate'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Churn', then compute TF-IDF features for column 'tenure' and display top 10 words, then clean text data in column 'Churn' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['tenure'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Churn_clean'] = df['Churn'].apply(clean)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'precipitation' using the IQR method, then compute TF-IDF features for column 'precipitation' and display top 10 words, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nQ1 = df['precipitation'].quantile(0.25)\nQ3 = df['precipitation'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['precipitation'] < (Q1 - 1.5*IQR)) | (df['precipitation'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['precipitation'])\nprint(vect.get_feature_names_out())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Revenue', then train a Linear Regression model to predict 'Revenue', then clean text data in column 'Region' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf = pd.get_dummies(df, columns=['Revenue'], prefix=['Revenue'])\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Region_clean'] = df['Region'].apply(clean)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'arrival_delay', then clean text data in column 'arrival_delay' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['arrival_delay_clean'] = df['arrival_delay'].apply(clean)\nprint(df.describe())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then detect outliers in 'petal_length' using the IQR method, then clean text data in column 'sepal_width' by removing punctuation and stopwords, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nQ1 = df['petal_length'].quantile(0.25)\nQ3 = df['petal_length'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['petal_length'] < (Q1 - 1.5*IQR)) | (df['petal_length'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sepal_width_clean'] = df['sepal_width'].apply(clean)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then clean text data in column 'carrier' by removing punctuation and stopwords, then train a Random Forest Classifier to predict 'arrival_delay', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['carrier_clean'] = df['carrier'].apply(clean)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nprint(df.describe())"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then calculate the correlation matrix for numeric features, then clean text data in column 'device_id' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sensor_value'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ncorr = df.corr()\nprint(corr)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['device_id_clean'] = df['device_id'].apply(clean)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then compute TF-IDF features for column 'petal_length' and display top 10 words, then create a new feature 'sepal_length_ratio' as the ratio of 'sepal_length' to 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nprint(df.describe())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['petal_length'])\nprint(vect.get_feature_names_out())\ndf['sepal_length_ratio'] = df['sepal_length'] / df['species']"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sensor_value', then plot a histogram of 'timestamp', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['timestamp'].hist()\nplt.xlabel('timestamp')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['timestamp'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then handle missing values in 'date' by imputing with median, then calculate the correlation matrix for numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['date'].fillna(df['date'].median(), inplace=True)\ncorr = df.corr()\nprint(corr)\nprint(df.describe())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then plot a histogram of 'o3', then evaluate the model performance using RMSE and R\u00b2 score, then compute TF-IDF features for column 'o3' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['o3'].hist()\nplt.xlabel('o3')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['o3'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then plot a histogram of 'carrier', then evaluate the model performance using RMSE and R\u00b2 score, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['carrier'].hist()\nplt.xlabel('carrier')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['departure_delay'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then clean text data in column 'pm10' by removing punctuation and stopwords, then detect outliers in 'no2' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['pm10_clean'] = df['pm10'].apply(clean)\nQ1 = df['no2'].quantile(0.25)\nQ3 = df['no2'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['no2'] < (Q1 - 1.5*IQR)) | (df['no2'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then create a new feature 'date_ratio' as the ratio of 'date' to 'consumption', then handle missing values in 'pressure' by imputing with median, then plot a histogram of 'pressure'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['date_ratio'] = df['date'] / df['consumption']\ndf['pressure'].fillna(df['pressure'].median(), inplace=True)\ndf['pressure'].hist()\nplt.xlabel('pressure')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'traffic_volume', then display feature importances from the Random Forest model, then create a new feature 'traffic_volume_ratio' as the ratio of 'traffic_volume' to 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['traffic_volume_ratio'] = df['traffic_volume'] / df['traffic_volume']"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Churn', then evaluate the model performance using RMSE and R\u00b2 score, then compute TF-IDF features for column 'MonthlyCharges' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf = pd.get_dummies(df, columns=['Churn'], prefix=['Churn'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['MonthlyCharges'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'total_amount', then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'total_amount' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['total_amount_scaled'] = (df['total_amount'] - df['total_amount'].min()) / (df['total_amount'].max() - df['total_amount'].min())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then create a new feature 'confirmed_ratio' as the ratio of 'confirmed' to 'confirmed', then handle missing values in 'deaths' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['confirmed_ratio'] = df['confirmed'] / df['confirmed']\ndf['deaths'].fillna(df['deaths'].median(), inplace=True)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sales', then detect outliers in 'sales' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['sales'].quantile(0.25)\nQ3 = df['sales'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sales'] < (Q1 - 1.5*IQR)) | (df['sales'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'tenure' and display top 10 words, then split the data into training and testing sets with an 80-20 split, then clean text data in column 'ContractType' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['tenure'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['ContractType_clean'] = df['ContractType'].apply(clean)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'location' and display top 10 words, then train a Random Forest Classifier to predict 'sensor_value', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['location'])\nprint(vect.get_feature_names_out())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nprint(df.describe())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then plot a histogram of 'temperature', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['temperature'].hist()\nplt.xlabel('temperature')\nplt.ylabel('Frequency')\nplt.show()\nprint(df.describe())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sentiment', then split the data into training and testing sets with an 80-20 split, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then handle missing values in 'Age' by imputing with median, then normalize the 'Sex' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['Age'].fillna(df['Age'].median(), inplace=True)\ndf['Sex_scaled'] = (df['Sex'] - df['Sex'].min()) / (df['Sex'].max() - df['Sex'].min())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then detect outliers in 'review' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods, then clean text data in column 'sentiment' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nQ1 = df['review'].quantile(0.25)\nQ3 = df['review'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['review'] < (Q1 - 1.5*IQR)) | (df['review'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sentiment'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sentiment_clean'] = df['sentiment'].apply(clean)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display feature importances from the Random Forest model, then normalize the 'distance' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['distance_scaled'] = (df['distance'] - df['distance'].min()) / (df['distance'].max() - df['distance'].min())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'arrival_delay', then normalize the 'distance' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['arrival_delay'], prefix=['arrival_delay'])\ndf['distance_scaled'] = (df['distance'] - df['distance'].min()) / (df['distance'].max() - df['distance'].min())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'Open' and display top 10 words, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Open'])\nprint(vect.get_feature_names_out())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then detect outliers in 'marital' using the IQR method, then clean text data in column 'y' by removing punctuation and stopwords, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nQ1 = df['marital'].quantile(0.25)\nQ3 = df['marital'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['marital'] < (Q1 - 1.5*IQR)) | (df['marital'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['y_clean'] = df['y'].apply(clean)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then plot a histogram of 'wind_speed', then handle missing values in 'wind_speed' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['wind_speed'].hist()\nplt.xlabel('wind_speed')\nplt.ylabel('Frequency')\nplt.show()\ndf['wind_speed'].fillna(df['wind_speed'].median(), inplace=True)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['customer_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then clean text data in column 'total_amount' by removing punctuation and stopwords, then display feature importances from the Random Forest model, then handle missing values in 'quantity' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['total_amount_clean'] = df['total_amount'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['quantity'].fillna(df['quantity'].median(), inplace=True)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'pm10' using the IQR method, then normalize the 'date' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['no2'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['pm10'].quantile(0.25)\nQ3 = df['pm10'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['pm10'] < (Q1 - 1.5*IQR)) | (df['pm10'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['date_scaled'] = (df['date'] - df['date'].min()) / (df['date'].max() - df['date'].min())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'pm2_5', then train a Random Forest Classifier to predict 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then handle missing values in 'date' by imputing with median, then plot a histogram of 'humidity'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['date'].fillna(df['date'].median(), inplace=True)\ndf['humidity'].hist()\nplt.xlabel('humidity')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'text', then evaluate the model performance using RMSE and R\u00b2 score, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then detect outliers in 'SalePrice' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nQ1 = df['SalePrice'].quantile(0.25)\nQ3 = df['SalePrice'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['SalePrice'] < (Q1 - 1.5*IQR)) | (df['SalePrice'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['SalePrice'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then detect outliers in 'amount' using the IQR method, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['amount'].quantile(0.25)\nQ3 = df['amount'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['amount'] < (Q1 - 1.5*IQR)) | (df['amount'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Revenue', then create a new feature 'UnitsSold_ratio' as the ratio of 'UnitsSold' to 'Revenue', then compute TF-IDF features for column 'Revenue' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['UnitsSold_ratio'] = df['UnitsSold'] / df['Revenue']\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Revenue'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'time', then detect outliers in 'patient_id' using the IQR method, then compute TF-IDF features for column 'ecg_reading' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf = pd.get_dummies(df, columns=['time'], prefix=['time'])\nQ1 = df['patient_id'].quantile(0.25)\nQ3 = df['patient_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['patient_id'] < (Q1 - 1.5*IQR)) | (df['patient_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['ecg_reading'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'country' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['country_scaled'] = (df['country'] - df['country'].min()) / (df['country'].max() - df['country'].min())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'shares' using the IQR method, then handle missing values in 'user_id' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['shares'].quantile(0.25)\nQ3 = df['shares'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['shares'] < (Q1 - 1.5*IQR)) | (df['shares'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['user_id'].fillna(df['user_id'].median(), inplace=True)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'species', then clean text data in column 'sepal_width' by removing punctuation and stopwords, then detect outliers in 'species' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sepal_width_clean'] = df['sepal_width'].apply(clean)\nQ1 = df['species'].quantile(0.25)\nQ3 = df['species'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['species'] < (Q1 - 1.5*IQR)) | (df['species'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'MonthlyCharges' and display top 10 words, then display summary statistics of all numeric columns using df.describe(), then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['MonthlyCharges'])\nprint(vect.get_feature_names_out())\nprint(df.describe())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then detect outliers in 'transaction_id' using the IQR method, then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nQ1 = df['transaction_id'].quantile(0.25)\nQ3 = df['transaction_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['transaction_id'] < (Q1 - 1.5*IQR)) | (df['transaction_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then normalize the 'deaths' column using min-max scaling, then plot a histogram of 'deaths'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['deaths_scaled'] = (df['deaths'] - df['deaths'].min()) / (df['deaths'].max() - df['deaths'].min())\ndf['deaths'].hist()\nplt.xlabel('deaths')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then create a new feature 'Age_ratio' as the ratio of 'Age' to 'Survived', then evaluate the model performance using RMSE and R\u00b2 score, then detect outliers in 'Fare' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Age_ratio'] = df['Age'] / df['Survived']\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nQ1 = df['Fare'].quantile(0.25)\nQ3 = df['Fare'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Fare'] < (Q1 - 1.5*IQR)) | (df['Fare'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then clean text data in column 'date_time' by removing punctuation and stopwords, then train a Linear Regression model to predict 'traffic_volume', then handle missing values in 'rain_1h' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_time_clean'] = df['date_time'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['rain_1h'].fillna(df['rain_1h'].median(), inplace=True)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'traffic_volume', then compute TF-IDF features for column 'date_time' and display top 10 words, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date_time'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then normalize the 'total_amount' column using min-max scaling, then create a new feature 'quantity_ratio' as the ratio of 'quantity' to 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['total_amount_scaled'] = (df['total_amount'] - df['total_amount'].min()) / (df['total_amount'].max() - df['total_amount'].min())\ndf['quantity_ratio'] = df['quantity'] / df['total_amount']"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then handle missing values in 'review' by imputing with median, then perform K-Means clustering with k=3 on numeric features, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['review'].fillna(df['review'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then create a new feature 'Revenue_ratio' as the ratio of 'Revenue' to 'Revenue', then compute TF-IDF features for column 'Product' and display top 10 words, then detect outliers in 'Date' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Revenue_ratio'] = df['Revenue'] / df['Revenue']\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Product'])\nprint(vect.get_feature_names_out())\nQ1 = df['Date'].quantile(0.25)\nQ3 = df['Date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Date'] < (Q1 - 1.5*IQR)) | (df['Date'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then clean text data in column 'country' by removing punctuation and stopwords, then normalize the 'deaths' column using min-max scaling, then one-hot encode the categorical column 'date'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['country_clean'] = df['country'].apply(clean)\ndf['deaths_scaled'] = (df['deaths'] - df['deaths'].min()) / (df['deaths'].max() - df['deaths'].min())\ndf = pd.get_dummies(df, columns=['date'], prefix=['date'])"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'isFraud', then perform K-Means clustering with k=3 on numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['oldbalanceOrg'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Survived', then clean text data in column 'Survived' by removing punctuation and stopwords, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Survived_clean'] = df['Survived'].apply(clean)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'y', then normalize the 'job' column using min-max scaling, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['job_scaled'] = (df['job'] - df['job'].min()) / (df['job'].max() - df['job'].min())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then display summary statistics of all numeric columns using df.describe(), then train a Random Forest Classifier to predict 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ncorr = df.corr()\nprint(corr)\nprint(df.describe())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then create a new feature 'tenure_ratio' as the ratio of 'tenure' to 'Churn', then clean text data in column 'Churn' by removing punctuation and stopwords, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['tenure_ratio'] = df['tenure'] / df['Churn']\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Churn_clean'] = df['Churn'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'day_of_week', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nprint(df.describe())\ndf['day_of_week'].hist()\nplt.xlabel('day_of_week')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'Region' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nprint(df.describe())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Region'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['Region'].quantile(0.25)\nQ3 = df['Region'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Region'] < (Q1 - 1.5*IQR)) | (df['Region'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then normalize the 'genre' column using min-max scaling, then plot a histogram of 'length', then create a new feature 'genre_ratio' as the ratio of 'genre' to 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['genre_scaled'] = (df['genre'] - df['genre'].min()) / (df['genre'].max() - df['genre'].min())\ndf['length'].hist()\nplt.xlabel('length')\nplt.ylabel('Frequency')\nplt.show()\ndf['genre_ratio'] = df['genre'] / df['sentiment']"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then create a new feature 'store_ratio' as the ratio of 'store' to 'sales', then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'sales' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['store_ratio'] = df['store'] / df['sales']\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['sales'].quantile(0.25)\nQ3 = df['sales'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sales'] < (Q1 - 1.5*IQR)) | (df['sales'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nprint(df.describe())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Revenue', then display feature importances from the Random Forest model, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nprint(df.describe())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then normalize the 'species' column using min-max scaling, then train a Linear Regression model to predict 'species', then plot a histogram of 'petal_length'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['species_scaled'] = (df['species'] - df['species'].min()) / (df['species'].max() - df['species'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['petal_length'].hist()\nplt.xlabel('petal_length')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'post_id' and display top 10 words, then calculate the correlation matrix for numeric features, then normalize the 'shares' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['post_id'])\nprint(vect.get_feature_names_out())\ncorr = df.corr()\nprint(corr)\ndf['shares_scaled'] = (df['shares'] - df['shares'].min()) / (df['shares'].max() - df['shares'].min())"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'sensor_value' by imputing with median, then one-hot encode the categorical column 'timestamp'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nprint(df.describe())\ndf['sensor_value'].fillna(df['sensor_value'].median(), inplace=True)\ndf = pd.get_dummies(df, columns=['timestamp'], prefix=['timestamp'])"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'ecg_reading' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score, then handle missing values in 'ecg_reading' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['ecg_reading'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['ecg_reading'].fillna(df['ecg_reading'].median(), inplace=True)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'post_id', then train a Linear Regression model to predict 'text', then normalize the 'user_id' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf = pd.get_dummies(df, columns=['post_id'], prefix=['post_id'])\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['user_id_scaled'] = (df['user_id'] - df['user_id'].min()) / (df['user_id'].max() - df['user_id'].min())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then create a new feature 'user_id_ratio' as the ratio of 'user_id' to 'text', then train a Linear Regression model to predict 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['text'])\ny = df['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['user_id_ratio'] = df['user_id'] / df['text']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then clean text data in column 'rating' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['rating_clean'] = df['rating'].apply(clean)\nprint(df.describe())\ndf['sentiment'].hist()\nplt.xlabel('sentiment')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then normalize the 'Pclass' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ncorr = df.corr()\nprint(corr)\ndf['Pclass_scaled'] = (df['Pclass'] - df['Pclass'].min()) / (df['Pclass'].max() - df['Pclass'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then normalize the 'quantity' column using min-max scaling, then create a new feature 'total_amount_ratio' as the ratio of 'total_amount' to 'total_amount', then handle missing values in 'product_id' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['quantity_scaled'] = (df['quantity'] - df['quantity'].min()) / (df['quantity'].max() - df['quantity'].min())\ndf['total_amount_ratio'] = df['total_amount'] / df['total_amount']\ndf['product_id'].fillna(df['product_id'].median(), inplace=True)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then clean text data in column 'humidity' by removing punctuation and stopwords, then create a new feature 'temperature_ratio' as the ratio of 'temperature' to 'consumption', then one-hot encode the categorical column 'humidity'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['humidity_clean'] = df['humidity'].apply(clean)\ndf['temperature_ratio'] = df['temperature'] / df['consumption']\ndf = pd.get_dummies(df, columns=['humidity'], prefix=['humidity'])"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then clean text data in column 'y' by removing punctuation and stopwords, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['y_clean'] = df['y'].apply(clean)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['job'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then compute TF-IDF features for column 'confirmed' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['confirmed'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then clean text data in column 'date' by removing punctuation and stopwords, then one-hot encode the categorical column 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)\ndf = pd.get_dummies(df, columns=['pm2_5'], prefix=['pm2_5'])"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Date', then create a new feature 'Low_ratio' as the ratio of 'Low' to 'Close', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf = pd.get_dummies(df, columns=['Date'], prefix=['Date'])\ndf['Low_ratio'] = df['Low'] / df['Close']\nprint(df.describe())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then compute TF-IDF features for column 'heart_rate' and display top 10 words, then one-hot encode the categorical column 'time'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['heart_rate'])\nprint(vect.get_feature_names_out())\ndf = pd.get_dummies(df, columns=['time'], prefix=['time'])"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then normalize the 'humidity' column using min-max scaling, then create a new feature 'temperature_ratio' as the ratio of 'temperature' to 'consumption', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['humidity_scaled'] = (df['humidity'] - df['humidity'].min()) / (df['humidity'].max() - df['humidity'].min())\ndf['temperature_ratio'] = df['temperature'] / df['consumption']\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split, then handle missing values in 'amount' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['newbalanceOrig'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['amount'].fillna(df['amount'].median(), inplace=True)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then clean text data in column 'day_of_week' by removing punctuation and stopwords, then one-hot encode the categorical column 'store', then compute TF-IDF features for column 'store' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['day_of_week_clean'] = df['day_of_week'].apply(clean)\ndf = pd.get_dummies(df, columns=['store'], prefix=['store'])\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['store'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then create a new feature 'newbalanceOrig_ratio' as the ratio of 'newbalanceOrig' to 'isFraud', then clean text data in column 'oldbalanceOrg' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ncorr = df.corr()\nprint(corr)\ndf['newbalanceOrig_ratio'] = df['newbalanceOrig'] / df['isFraud']\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['oldbalanceOrg_clean'] = df['oldbalanceOrg'].apply(clean)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['pressure'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nprint(df.describe())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then evaluate the model performance using RMSE and R\u00b2 score, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['rain_1h'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'Date' and display top 10 words, then normalize the 'Close' column using min-max scaling, then train a Linear Regression model to predict 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Date'])\nprint(vect.get_feature_names_out())\ndf['Close_scaled'] = (df['Close'] - df['Close'].min()) / (df['Close'].max() - df['Close'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then normalize the 'Churn' column using min-max scaling, then create a new feature 'ContractType_ratio' as the ratio of 'ContractType' to 'Churn', then one-hot encode the categorical column 'MonthlyCharges'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['Churn_scaled'] = (df['Churn'] - df['Churn'].min()) / (df['Churn'].max() - df['Churn'].min())\ndf['ContractType_ratio'] = df['ContractType'] / df['Churn']\ndf = pd.get_dummies(df, columns=['MonthlyCharges'], prefix=['MonthlyCharges'])"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then perform K-Means clustering with k=3 on numeric features, then plot a histogram of 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['total_amount'].hist()\nplt.xlabel('total_amount')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'age' and display top 10 words, then handle missing values in 'age' by imputing with median, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['age'])\nprint(vect.get_feature_names_out())\ndf['age'].fillna(df['age'].median(), inplace=True)\nprint(df.describe())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nprint(df.describe())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then train a Random Forest Classifier to predict 'text', then normalize the 'shares' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['shares_scaled'] = (df['shares'] - df['shares'].min()) / (df['shares'].max() - df['shares'].min())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'ecg_reading', then handle missing values in 'ecg_reading' by imputing with median, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['ecg_reading'].fillna(df['ecg_reading'].median(), inplace=True)\nprint(df.describe())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'traffic_volume', then split the data into training and testing sets with an 80-20 split, then normalize the 'traffic_volume' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['traffic_volume_scaled'] = (df['traffic_volume'] - df['traffic_volume'].min()) / (df['traffic_volume'].max() - df['traffic_volume'].min())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nprint(df.describe())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['shares'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model, then create a new feature 'isFraud_ratio' as the ratio of 'isFraud' to 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['isFraud_ratio'] = df['isFraud'] / df['isFraud']"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then detect outliers in 'Age' using the IQR method, then plot a histogram of 'Sex', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nQ1 = df['Age'].quantile(0.25)\nQ3 = df['Age'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Age'] < (Q1 - 1.5*IQR)) | (df['Age'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['Sex'].hist()\nplt.xlabel('Sex')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'sepal_width' and display top 10 words, then split the data into training and testing sets with an 80-20 split, then handle missing values in 'petal_width' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sepal_width'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['petal_width'].fillna(df['petal_width'].median(), inplace=True)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sentiment', then perform K-Means clustering with k=3 on numeric features, then compute TF-IDF features for column 'review' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['review'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'arrival_delay' and display top 10 words, then calculate the correlation matrix for numeric features, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['arrival_delay'])\nprint(vect.get_feature_names_out())\ncorr = df.corr()\nprint(corr)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split, then handle missing values in 'time' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['oldbalanceOrg'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['time'].fillna(df['time'].median(), inplace=True)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then create a new feature 'text_ratio' as the ratio of 'text' to 'text', then handle missing values in 'text' by imputing with median, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['text_ratio'] = df['text'] / df['text']\ndf['text'].fillna(df['text'].median(), inplace=True)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then plot a histogram of 'quantity', then create a new feature 'product_id_ratio' as the ratio of 'product_id' to 'total_amount', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['quantity'].hist()\nplt.xlabel('quantity')\nplt.ylabel('Frequency')\nplt.show()\ndf['product_id_ratio'] = df['product_id'] / df['total_amount']\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['customer_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe(), then train a Linear Regression model to predict 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'o3', then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf = pd.get_dummies(df, columns=['o3'], prefix=['o3'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'humidity' and display top 10 words, then create a new feature 'date_ratio' as the ratio of 'date' to 'temperature', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['humidity'])\nprint(vect.get_feature_names_out())\ndf['date_ratio'] = df['date'] / df['temperature']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'location', then normalize the 'device_id' column using min-max scaling, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['location'].hist()\nplt.xlabel('location')\nplt.ylabel('Frequency')\nplt.show()\ndf['device_id_scaled'] = (df['device_id'] - df['device_id'].min()) / (df['device_id'].max() - df['device_id'].min())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['timestamp'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then compute TF-IDF features for column 'post_id' and display top 10 words, then normalize the 'text' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['likes'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['post_id'])\nprint(vect.get_feature_names_out())\ndf['text_scaled'] = (df['text'] - df['text'].min()) / (df['text'].max() - df['text'].min())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then handle missing values in 'arrival_delay' by imputing with median, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['arrival_delay'].fillna(df['arrival_delay'].median(), inplace=True)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'pm2_5' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score, then clean text data in column 'o3' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['pm2_5'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['o3_clean'] = df['o3'].apply(clean)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'heart_rate' by imputing with median, then one-hot encode the categorical column 'time', then create a new feature 'patient_id_ratio' as the ratio of 'patient_id' to 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['heart_rate'].fillna(df['heart_rate'].median(), inplace=True)\ndf = pd.get_dummies(df, columns=['time'], prefix=['time'])\ndf['patient_id_ratio'] = df['patient_id'] / df['ecg_reading']"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then plot a histogram of 'length', then handle missing values in 'sentiment' by imputing with median, then train a Linear Regression model to predict 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['length'].hist()\nplt.xlabel('length')\nplt.ylabel('Frequency')\nplt.show()\ndf['sentiment'].fillna(df['sentiment'].median(), inplace=True)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'petal_width' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['petal_width'].quantile(0.25)\nQ3 = df['petal_width'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['petal_width'] < (Q1 - 1.5*IQR)) | (df['petal_width'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'date' by removing punctuation and stopwords, then display feature importances from the Random Forest model, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nprint(df.describe())"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'LotArea' and display top 10 words, then train a Linear Regression model to predict 'SalePrice', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['LotArea'])\nprint(vect.get_feature_names_out())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'date', then handle missing values in 'humidity' by imputing with median, then normalize the 'humidity' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf = pd.get_dummies(df, columns=['date'], prefix=['date'])\ndf['humidity'].fillna(df['humidity'].median(), inplace=True)\ndf['humidity_scaled'] = (df['humidity'] - df['humidity'].min()) / (df['humidity'].max() - df['humidity'].min())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'Region' using the IQR method, then handle missing values in 'Region' by imputing with median, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nQ1 = df['Region'].quantile(0.25)\nQ3 = df['Region'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Region'] < (Q1 - 1.5*IQR)) | (df['Region'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['Region'].fillna(df['Region'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then handle missing values in 'shares' by imputing with median, then plot a histogram of 'user_id', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['shares'].fillna(df['shares'].median(), inplace=True)\ndf['user_id'].hist()\nplt.xlabel('user_id')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then handle missing values in 'ContractType' by imputing with median, then train a Linear Regression model to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ncorr = df.corr()\nprint(corr)\ndf['ContractType'].fillna(df['ContractType'].median(), inplace=True)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then plot a histogram of 'sensor_value', then train a Linear Regression model to predict 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['sensor_value'].hist()\nplt.xlabel('sensor_value')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then detect outliers in 'store' using the IQR method, then create a new feature 'day_of_week_ratio' as the ratio of 'day_of_week' to 'sales', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nQ1 = df['store'].quantile(0.25)\nQ3 = df['store'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['store'] < (Q1 - 1.5*IQR)) | (df['store'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['day_of_week_ratio'] = df['day_of_week'] / df['sales']\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then create a new feature 'total_amount_ratio' as the ratio of 'total_amount' to 'total_amount', then one-hot encode the categorical column 'total_amount', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['total_amount_ratio'] = df['total_amount'] / df['total_amount']\ndf = pd.get_dummies(df, columns=['total_amount'], prefix=['total_amount'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sales'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Revenue', then perform time-series forecasting using ARIMA to predict the next 12 periods, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then detect outliers in 'consumption' using the IQR method, then normalize the 'humidity' column using min-max scaling, then create a new feature 'date_ratio' as the ratio of 'date' to 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nQ1 = df['consumption'].quantile(0.25)\nQ3 = df['consumption'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['consumption'] < (Q1 - 1.5*IQR)) | (df['consumption'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['humidity_scaled'] = (df['humidity'] - df['humidity'].min()) / (df['humidity'].max() - df['humidity'].min())\ndf['date_ratio'] = df['date'] / df['consumption']"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Revenue', then handle missing values in 'Date' by imputing with median, then detect outliers in 'Date' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['Date'].fillna(df['Date'].median(), inplace=True)\nQ1 = df['Date'].quantile(0.25)\nQ3 = df['Date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Date'] < (Q1 - 1.5*IQR)) | (df['Date'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'text', then one-hot encode the categorical column 'post_id', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['post_id'], prefix=['post_id'])\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['likes'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then handle missing values in 'isFraud' by imputing with median, then detect outliers in 'isFraud' using the IQR method, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['isFraud'].fillna(df['isFraud'].median(), inplace=True)\nQ1 = df['isFraud'].quantile(0.25)\nQ3 = df['isFraud'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['isFraud'] < (Q1 - 1.5*IQR)) | (df['isFraud'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'consumption', then detect outliers in 'humidity' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nQ1 = df['humidity'].quantile(0.25)\nQ3 = df['humidity'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['humidity'] < (Q1 - 1.5*IQR)) | (df['humidity'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then compute TF-IDF features for column 'time' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['time'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'isFraud', then plot a histogram of 'isFraud', then clean text data in column 'newbalanceOrig' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['isFraud'].hist()\nplt.xlabel('isFraud')\nplt.ylabel('Frequency')\nplt.show()\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['newbalanceOrig_clean'] = df['newbalanceOrig'].apply(clean)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then clean text data in column 'no2' by removing punctuation and stopwords, then train a Linear Regression model to predict 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['no2_clean'] = df['no2'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Random Forest Classifier to predict 'total_amount', then detect outliers in 'transaction_id' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['transaction_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['transaction_id'].quantile(0.25)\nQ3 = df['transaction_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['transaction_id'] < (Q1 - 1.5*IQR)) | (df['transaction_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then plot a histogram of 'temperature', then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['temperature'].hist()\nplt.xlabel('temperature')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'age' and display top 10 words, then handle missing values in 'age' by imputing with median, then create a new feature 'y_ratio' as the ratio of 'y' to 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['age'])\nprint(vect.get_feature_names_out())\ndf['age'].fillna(df['age'].median(), inplace=True)\ndf['y_ratio'] = df['y'] / df['y']"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'location' using the IQR method, then plot a histogram of 'status', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nQ1 = df['location'].quantile(0.25)\nQ3 = df['location'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['location'] < (Q1 - 1.5*IQR)) | (df['location'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['status'].hist()\nplt.xlabel('status')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'heart_rate', then display feature importances from the Random Forest model, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf = pd.get_dummies(df, columns=['heart_rate'], prefix=['heart_rate'])\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nprint(df.describe())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then handle missing values in 'text' by imputing with median, then one-hot encode the categorical column 'user_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['text'])\ny = df['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['text'].fillna(df['text'].median(), inplace=True)\ndf = pd.get_dummies(df, columns=['user_id'], prefix=['user_id'])"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then create a new feature 'humidity_ratio' as the ratio of 'humidity' to 'consumption', then clean text data in column 'date' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['humidity_ratio'] = df['humidity'] / df['consumption']\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then clean text data in column 'YearBuilt' by removing punctuation and stopwords, then train a Random Forest Classifier to predict 'SalePrice', then one-hot encode the categorical column 'OverallQual'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['YearBuilt_clean'] = df['YearBuilt'].apply(clean)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['OverallQual'], prefix=['OverallQual'])"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'time' by imputing with median, then create a new feature 'heart_rate_ratio' as the ratio of 'heart_rate' to 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nprint(df.describe())\ndf['time'].fillna(df['time'].median(), inplace=True)\ndf['heart_rate_ratio'] = df['heart_rate'] / df['ecg_reading']"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then one-hot encode the categorical column 'education', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nprint(df.describe())\ndf = pd.get_dummies(df, columns=['education'], prefix=['education'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'heart_rate' column using min-max scaling, then train a Linear Regression model to predict 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['patient_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['heart_rate_scaled'] = (df['heart_rate'] - df['heart_rate'].min()) / (df['heart_rate'].max() - df['heart_rate'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display summary statistics of all numeric columns using df.describe(), then normalize the 'time' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['ecg_reading'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nprint(df.describe())\ndf['time_scaled'] = (df['time'] - df['time'].min()) / (df['time'].max() - df['time'].min())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Random Forest Classifier to predict 'isFraud', then clean text data in column 'oldbalanceOrg' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['time'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['oldbalanceOrg_clean'] = df['oldbalanceOrg'].apply(clean)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'country' and display top 10 words, then detect outliers in 'deaths' using the IQR method, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['country'])\nprint(vect.get_feature_names_out())\nQ1 = df['deaths'].quantile(0.25)\nQ3 = df['deaths'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['deaths'] < (Q1 - 1.5*IQR)) | (df['deaths'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then one-hot encode the categorical column 'sepal_width', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nprint(df.describe())\ndf = pd.get_dummies(df, columns=['sepal_width'], prefix=['sepal_width'])\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nprint(df.describe())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['customers'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'sales', then normalize the 'sales' column using min-max scaling, then handle missing values in 'customers' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf = pd.get_dummies(df, columns=['sales'], prefix=['sales'])\ndf['sales_scaled'] = (df['sales'] - df['sales'].min()) / (df['sales'].max() - df['sales'].min())\ndf['customers'].fillna(df['customers'].median(), inplace=True)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then handle missing values in 'y' by imputing with median, then normalize the 'job' column using min-max scaling, then detect outliers in 'job' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['y'].fillna(df['y'].median(), inplace=True)\ndf['job_scaled'] = (df['job'] - df['job'].min()) / (df['job'].max() - df['job'].min())\nQ1 = df['job'].quantile(0.25)\nQ3 = df['job'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['job'] < (Q1 - 1.5*IQR)) | (df['job'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Churn', then create a new feature 'tenure_ratio' as the ratio of 'tenure' to 'Churn', then clean text data in column 'tenure' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['tenure_ratio'] = df['tenure'] / df['Churn']\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['tenure_clean'] = df['tenure'].apply(clean)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then evaluate the model performance using RMSE and R\u00b2 score, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['amount'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then plot a histogram of 'temp', then one-hot encode the categorical column 'temp'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date_time'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['temp'].hist()\nplt.xlabel('temp')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['temp'], prefix=['temp'])"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sensor_value', then plot a histogram of 'timestamp', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['timestamp'].hist()\nplt.xlabel('timestamp')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'y', then handle missing values in 'education' by imputing with median, then create a new feature 'marital_ratio' as the ratio of 'marital' to 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['education'].fillna(df['education'].median(), inplace=True)\ndf['marital_ratio'] = df['marital'] / df['y']"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then handle missing values in 'species' by imputing with median, then display summary statistics of all numeric columns using df.describe(), then train a Random Forest Classifier to predict 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['species'].fillna(df['species'].median(), inplace=True)\nprint(df.describe())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sensor_value', then create a new feature 'timestamp_ratio' as the ratio of 'timestamp' to 'sensor_value', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['timestamp_ratio'] = df['timestamp'] / df['sensor_value']\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then create a new feature 'Churn_ratio' as the ratio of 'Churn' to 'Churn', then detect outliers in 'tenure' using the IQR method, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['Churn_ratio'] = df['Churn'] / df['Churn']\nQ1 = df['tenure'].quantile(0.25)\nQ3 = df['tenure'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['tenure'] < (Q1 - 1.5*IQR)) | (df['tenure'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Open', then plot a histogram of 'Close', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf = pd.get_dummies(df, columns=['Open'], prefix=['Open'])\ndf['Close'].hist()\nplt.xlabel('Close')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'heart_rate' by removing punctuation and stopwords, then handle missing values in 'heart_rate' by imputing with median, then detect outliers in 'quality' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['heart_rate_clean'] = df['heart_rate'].apply(clean)\ndf['heart_rate'].fillna(df['heart_rate'].median(), inplace=True)\nQ1 = df['quality'].quantile(0.25)\nQ3 = df['quality'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['quality'] < (Q1 - 1.5*IQR)) | (df['quality'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sentiment', then one-hot encode the categorical column 'sentiment', then compute TF-IDF features for column 'rating' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['sentiment'], prefix=['sentiment'])\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['rating'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then clean text data in column 'rating' by removing punctuation and stopwords, then perform K-Means clustering with k=3 on numeric features, then train a Linear Regression model to predict 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['rating_clean'] = df['rating'].apply(clean)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then normalize the 'LotArea' column using min-max scaling, then create a new feature 'Neighborhood_ratio' as the ratio of 'Neighborhood' to 'SalePrice', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['LotArea_scaled'] = (df['LotArea'] - df['LotArea'].min()) / (df['LotArea'].max() - df['LotArea'].min())\ndf['Neighborhood_ratio'] = df['Neighborhood'] / df['SalePrice']\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'total_amount', then handle missing values in 'total_amount' by imputing with median, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['total_amount'].fillna(df['total_amount'].median(), inplace=True)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Survived', then one-hot encode the categorical column 'Pclass', then compute TF-IDF features for column 'Survived' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['Pclass'], prefix=['Pclass'])\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Survived'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then train a Random Forest Classifier to predict 'sales', then plot a histogram of 'open'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['open'].hist()\nplt.xlabel('open')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then plot a histogram of 'text', then display summary statistics of all numeric columns using df.describe(), then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['text'].hist()\nplt.xlabel('text')\nplt.ylabel('Frequency')\nplt.show()\nprint(df.describe())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['text'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then handle missing values in 'arrival_delay' by imputing with median, then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['arrival_delay'].fillna(df['arrival_delay'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'High', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['High'], prefix=['High'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Close'])\ny = df['Close']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then normalize the 'job' column using min-max scaling, then clean text data in column 'y' by removing punctuation and stopwords, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['job_scaled'] = (df['job'] - df['job'].min()) / (df['job'].max() - df['job'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['y_clean'] = df['y'].apply(clean)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'Neighborhood' column using min-max scaling, then clean text data in column 'LotArea' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['Neighborhood_scaled'] = (df['Neighborhood'] - df['Neighborhood'].min()) / (df['Neighborhood'].max() - df['Neighborhood'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['LotArea_clean'] = df['LotArea'].apply(clean)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split, then compute TF-IDF features for column 'Date' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['UnitsSold'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Revenue'])\ny = df['Revenue']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Date'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'time'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())\ndf['time'].hist()\nplt.xlabel('time')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'Survived' using the IQR method, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nprint(df.describe())\nQ1 = df['Survived'].quantile(0.25)\nQ3 = df['Survived'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Survived'] < (Q1 - 1.5*IQR)) | (df['Survived'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then clean text data in column 'temperature' by removing punctuation and stopwords, then normalize the 'consumption' column using min-max scaling, then one-hot encode the categorical column 'date'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['temperature_clean'] = df['temperature'].apply(clean)\ndf['consumption_scaled'] = (df['consumption'] - df['consumption'].min()) / (df['consumption'].max() - df['consumption'].min())\ndf = pd.get_dummies(df, columns=['date'], prefix=['date'])"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then handle missing values in 'flight' by imputing with median, then display summary statistics of all numeric columns using df.describe(), then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['flight'].fillna(df['flight'].median(), inplace=True)\nprint(df.describe())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['flight'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then split the data into training and testing sets with an 80-20 split, then detect outliers in 'Sex' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nprint(df.describe())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['Sex'].quantile(0.25)\nQ3 = df['Sex'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Sex'] < (Q1 - 1.5*IQR)) | (df['Sex'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then handle missing values in 'sensor_value' by imputing with median, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['sensor_value'].fillna(df['sensor_value'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sensor_value'])\ny = df['sensor_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Revenue', then handle missing values in 'Revenue' by imputing with median, then normalize the 'Region' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['Revenue'].fillna(df['Revenue'].median(), inplace=True)\ndf['Region_scaled'] = (df['Region'] - df['Region'].min()) / (df['Region'].max() - df['Region'].min())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'traffic_volume', then normalize the 'temp' column using min-max scaling, then detect outliers in 'temp' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['temp_scaled'] = (df['temp'] - df['temp'].min()) / (df['temp'].max() - df['temp'].min())\nQ1 = df['temp'].quantile(0.25)\nQ3 = df['temp'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['temp'] < (Q1 - 1.5*IQR)) | (df['temp'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'humidity' and display top 10 words, then display feature importances from the Random Forest model, then clean text data in column 'temperature' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['humidity'])\nprint(vect.get_feature_names_out())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['temperature_clean'] = df['temperature'].apply(clean)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then clean text data in column 'Survived' by removing punctuation and stopwords, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Survived_clean'] = df['Survived'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Revenue', then one-hot encode the categorical column 'Product', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['Product'], prefix=['Product'])\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'confirmed', then one-hot encode the categorical column 'deaths', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['deaths'], prefix=['deaths'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features, then plot a histogram of 'flight'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['flight'].hist()\nplt.xlabel('flight')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then create a new feature 'quality_ratio' as the ratio of 'quality' to 'ecg_reading', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nprint(df.describe())\ndf['quality_ratio'] = df['quality'] / df['ecg_reading']\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['ecg_reading'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then detect outliers in 'Churn' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['Churn'].quantile(0.25)\nQ3 = df['Churn'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Churn'] < (Q1 - 1.5*IQR)) | (df['Churn'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['ContractType'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then create a new feature 'sensor_value_ratio' as the ratio of 'sensor_value' to 'sensor_value', then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['sensor_value_ratio'] = df['sensor_value'] / df['sensor_value']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sensor_value'])\ny = df['sensor_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'quality', then handle missing values in 'heart_rate' by imputing with median, then train a Random Forest Classifier to predict 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['quality'].hist()\nplt.xlabel('quality')\nplt.ylabel('Frequency')\nplt.show()\ndf['heart_rate'].fillna(df['heart_rate'].median(), inplace=True)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'time', then calculate the correlation matrix for numeric features, then handle missing values in 'ecg_reading' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf = pd.get_dummies(df, columns=['time'], prefix=['time'])\ncorr = df.corr()\nprint(corr)\ndf['ecg_reading'].fillna(df['ecg_reading'].median(), inplace=True)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'education', then evaluate the model performance using RMSE and R\u00b2 score, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf = pd.get_dummies(df, columns=['education'], prefix=['education'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Pclass', then detect outliers in 'Survived' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Pclass'].hist()\nplt.xlabel('Pclass')\nplt.ylabel('Frequency')\nplt.show()\nQ1 = df['Survived'].quantile(0.25)\nQ3 = df['Survived'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Survived'] < (Q1 - 1.5*IQR)) | (df['Survived'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'arrival_delay', then evaluate the model performance using RMSE and R\u00b2 score, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then create a new feature 'Survived_ratio' as the ratio of 'Survived' to 'Survived', then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Survived_ratio'] = df['Survived'] / df['Survived']\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'o3' and display top 10 words, then train a Random Forest Classifier to predict 'pm2_5', then detect outliers in 'date' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['o3'])\nprint(vect.get_feature_names_out())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then create a new feature 'date_ratio' as the ratio of 'date' to 'confirmed', then train a Linear Regression model to predict 'confirmed', then plot a histogram of 'date'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['date_ratio'] = df['date'] / df['confirmed']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['date'].hist()\nplt.xlabel('date')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then normalize the 'customer_id' column using min-max scaling, then handle missing values in 'product_id' by imputing with median, then plot a histogram of 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['customer_id_scaled'] = (df['customer_id'] - df['customer_id'].min()) / (df['customer_id'].max() - df['customer_id'].min())\ndf['product_id'].fillna(df['product_id'].median(), inplace=True)\ndf['total_amount'].hist()\nplt.xlabel('total_amount')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'Churn', then plot a histogram of 'tenure'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['tenure'].hist()\nplt.xlabel('tenure')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nprint(df.describe())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'heart_rate', then handle missing values in 'quality' by imputing with median, then normalize the 'time' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf = pd.get_dummies(df, columns=['heart_rate'], prefix=['heart_rate'])\ndf['quality'].fillna(df['quality'].median(), inplace=True)\ndf['time_scaled'] = (df['time'] - df['time'].min()) / (df['time'].max() - df['time'].min())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then plot a histogram of 'job', then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['job'].hist()\nplt.xlabel('job')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then normalize the 'Pclass' column using min-max scaling, then train a Random Forest Classifier to predict 'Survived', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Pclass_scaled'] = (df['Pclass'] - df['Pclass'].min()) / (df['Pclass'].max() - df['Pclass'].min())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split, then handle missing values in 'tenure' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['tenure'].fillna(df['tenure'].median(), inplace=True)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Revenue', then display feature importances from the Random Forest model, then normalize the 'Product' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['Product_scaled'] = (df['Product'] - df['Product'].min()) / (df['Product'].max() - df['Product'].min())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'precipitation' by removing punctuation and stopwords, then one-hot encode the categorical column 'date', then plot a histogram of 'humidity'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['precipitation_clean'] = df['precipitation'].apply(clean)\ndf = pd.get_dummies(df, columns=['date'], prefix=['date'])\ndf['humidity'].hist()\nplt.xlabel('humidity')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'recovered', then clean text data in column 'confirmed' by removing punctuation and stopwords, then normalize the 'deaths' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf = pd.get_dummies(df, columns=['recovered'], prefix=['recovered'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['confirmed_clean'] = df['confirmed'].apply(clean)\ndf['deaths_scaled'] = (df['deaths'] - df['deaths'].min()) / (df['deaths'].max() - df['deaths'].min())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'temperature', then detect outliers in 'temperature' using the IQR method, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['temperature'].hist()\nplt.xlabel('temperature')\nplt.ylabel('Frequency')\nplt.show()\nQ1 = df['temperature'].quantile(0.25)\nQ3 = df['temperature'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['temperature'] < (Q1 - 1.5*IQR)) | (df['temperature'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features, then handle missing values in 'rating' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)\ndf['rating'].fillna(df['rating'].median(), inplace=True)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then clean text data in column 'departure_delay' by removing punctuation and stopwords, then handle missing values in 'carrier' by imputing with median, then plot a histogram of 'departure_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['departure_delay_clean'] = df['departure_delay'].apply(clean)\ndf['carrier'].fillna(df['carrier'].median(), inplace=True)\ndf['departure_delay'].hist()\nplt.xlabel('departure_delay')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Random Forest Classifier to predict 'text', then normalize the 'post_id' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['post_id_scaled'] = (df['post_id'] - df['post_id'].min()) / (df['post_id'].max() - df['post_id'].min())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then plot a histogram of 'pm2_5', then one-hot encode the categorical column 'pm10', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['pm2_5'].hist()\nplt.xlabel('pm2_5')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['pm10'], prefix=['pm10'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['MonthlyCharges'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then normalize the 'date' column using min-max scaling, then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'humidity' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['date_scaled'] = (df['date'] - df['date'].min()) / (df['date'].max() - df['date'].min())\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['humidity'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then detect outliers in 'confirmed' using the IQR method, then create a new feature 'date_ratio' as the ratio of 'date' to 'confirmed', then train a Random Forest Classifier to predict 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nQ1 = df['confirmed'].quantile(0.25)\nQ3 = df['confirmed'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['confirmed'] < (Q1 - 1.5*IQR)) | (df['confirmed'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['date_ratio'] = df['date'] / df['confirmed']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then normalize the 'quantity' column using min-max scaling, then create a new feature 'quantity_ratio' as the ratio of 'quantity' to 'total_amount', then detect outliers in 'quantity' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['quantity_scaled'] = (df['quantity'] - df['quantity'].min()) / (df['quantity'].max() - df['quantity'].min())\ndf['quantity_ratio'] = df['quantity'] / df['total_amount']\nQ1 = df['quantity'].quantile(0.25)\nQ3 = df['quantity'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['quantity'] < (Q1 - 1.5*IQR)) | (df['quantity'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe(), then train a Linear Regression model to predict 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'OverallQual', then clean text data in column 'YearBuilt' by removing punctuation and stopwords, then compute TF-IDF features for column 'SalePrice' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf = pd.get_dummies(df, columns=['OverallQual'], prefix=['OverallQual'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['YearBuilt_clean'] = df['YearBuilt'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['SalePrice'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then clean text data in column 'snow_1h' by removing punctuation and stopwords, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nprint(df.describe())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['snow_1h_clean'] = df['snow_1h'].apply(clean)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date_time'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'High' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Low'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['High_scaled'] = (df['High'] - df['High'].min()) / (df['High'].max() - df['High'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then display summary statistics of all numeric columns using df.describe(), then one-hot encode the categorical column 'heart_rate'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nprint(df.describe())\ndf = pd.get_dummies(df, columns=['heart_rate'], prefix=['heart_rate'])"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['ecg_reading'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then normalize the 'length' column using min-max scaling, then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'review_ratio' as the ratio of 'review' to 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['length_scaled'] = (df['length'] - df['length'].min()) / (df['length'].max() - df['length'].min())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['length'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['review_ratio'] = df['review'] / df['sentiment']"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform time-series forecasting using ARIMA to predict the next 12 periods, then plot a histogram of 'Product'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nprint(df.describe())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['UnitsSold'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['Product'].hist()\nplt.xlabel('Product')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then clean text data in column 'Open' by removing punctuation and stopwords, then train a Linear Regression model to predict 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Close'])\ny = df['Close']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Open_clean'] = df['Open'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['pm10'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'traffic_volume', then detect outliers in 'traffic_volume' using the IQR method, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['traffic_volume'].quantile(0.25)\nQ3 = df['traffic_volume'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['traffic_volume'] < (Q1 - 1.5*IQR)) | (df['traffic_volume'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then one-hot encode the categorical column 'Churn', then train a Random Forest Classifier to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Churn'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf = pd.get_dummies(df, columns=['Churn'], prefix=['Churn'])\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then handle missing values in 'recovered' by imputing with median, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['recovered'].fillna(df['recovered'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then create a new feature 'post_id_ratio' as the ratio of 'post_id' to 'text', then calculate the correlation matrix for numeric features, then plot a histogram of 'post_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['post_id_ratio'] = df['post_id'] / df['text']\ncorr = df.corr()\nprint(corr)\ndf['post_id'].hist()\nplt.xlabel('post_id')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe(), then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then create a new feature 'quantity_ratio' as the ratio of 'quantity' to 'total_amount', then split the data into training and testing sets with an 80-20 split, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['quantity_ratio'] = df['quantity'] / df['total_amount']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then handle missing values in 'Neighborhood' by imputing with median, then detect outliers in 'LotArea' using the IQR method, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['Neighborhood'].fillna(df['Neighborhood'].median(), inplace=True)\nQ1 = df['LotArea'].quantile(0.25)\nQ3 = df['LotArea'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['LotArea'] < (Q1 - 1.5*IQR)) | (df['LotArea'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then detect outliers in 'rating' using the IQR method, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ncorr = df.corr()\nprint(corr)\nQ1 = df['rating'].quantile(0.25)\nQ3 = df['rating'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['rating'] < (Q1 - 1.5*IQR)) | (df['rating'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'deaths' and display top 10 words, then train a Random Forest Classifier to predict 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['deaths'])\nprint(vect.get_feature_names_out())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then normalize the 'sentiment' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['sentiment_scaled'] = (df['sentiment'] - df['sentiment'].min()) / (df['sentiment'].max() - df['sentiment'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nprint(df.describe())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then normalize the 'day_of_week' column using min-max scaling, then train a Linear Regression model to predict 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nprint(df.describe())\ndf['day_of_week_scaled'] = (df['day_of_week'] - df['day_of_week'].min()) / (df['day_of_week'].max() - df['day_of_week'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then detect outliers in 'temp' using the IQR method, then one-hot encode the categorical column 'traffic_volume', then handle missing values in 'temp' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nQ1 = df['temp'].quantile(0.25)\nQ3 = df['temp'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['temp'] < (Q1 - 1.5*IQR)) | (df['temp'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['traffic_volume'], prefix=['traffic_volume'])\ndf['temp'].fillna(df['temp'].median(), inplace=True)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then one-hot encode the categorical column 'sensor_value', then compute TF-IDF features for column 'device_id' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ncorr = df.corr()\nprint(corr)\ndf = pd.get_dummies(df, columns=['sensor_value'], prefix=['sensor_value'])\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['device_id'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'no2' and display top 10 words, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['no2'])\nprint(vect.get_feature_names_out())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['pm10'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then handle missing values in 'amount' by imputing with median, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['amount'].fillna(df['amount'].median(), inplace=True)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then handle missing values in 'user_id' by imputing with median, then perform K-Means clustering with k=3 on numeric features, then create a new feature 'likes_ratio' as the ratio of 'likes' to 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['user_id'].fillna(df['user_id'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['likes_ratio'] = df['likes'] / df['text']"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then detect outliers in 'Churn' using the IQR method, then train a Linear Regression model to predict 'Churn', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nQ1 = df['Churn'].quantile(0.25)\nQ3 = df['Churn'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Churn'] < (Q1 - 1.5*IQR)) | (df['Churn'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then normalize the 'sepal_width' column using min-max scaling, then train a Random Forest Classifier to predict 'species', then create a new feature 'sepal_width_ratio' as the ratio of 'sepal_width' to 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['sepal_width_scaled'] = (df['sepal_width'] - df['sepal_width'].min()) / (df['sepal_width'].max() - df['sepal_width'].min())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['sepal_width_ratio'] = df['sepal_width'] / df['species']"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'time', then train a Random Forest Classifier to predict 'ecg_reading', then plot a histogram of 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf = pd.get_dummies(df, columns=['time'], prefix=['time'])\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['ecg_reading'].hist()\nplt.xlabel('ecg_reading')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'consumption', then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'species', then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'TotalCharges' and display top 10 words, then detect outliers in 'tenure' using the IQR method, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['TotalCharges'])\nprint(vect.get_feature_names_out())\nQ1 = df['tenure'].quantile(0.25)\nQ3 = df['tenure'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['tenure'] < (Q1 - 1.5*IQR)) | (df['tenure'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nprint(df.describe())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sales', then perform K-Means clustering with k=3 on numeric features, then handle missing values in 'open' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['open'].fillna(df['open'].median(), inplace=True)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'so2', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['so2'], prefix=['so2'])\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['pm10'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'UnitsSold' and display top 10 words, then handle missing values in 'Product' by imputing with median, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['UnitsSold'])\nprint(vect.get_feature_names_out())\ndf['Product'].fillna(df['Product'].median(), inplace=True)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Product'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Revenue', then compute TF-IDF features for column 'Product' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Product'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'time' by imputing with median, then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'patient_id' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['time'].fillna(df['time'].median(), inplace=True)\nprint(df.describe())\nQ1 = df['patient_id'].quantile(0.25)\nQ3 = df['patient_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['patient_id'] < (Q1 - 1.5*IQR)) | (df['patient_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then normalize the 'device_id' column using min-max scaling, then calculate the correlation matrix for numeric features, then handle missing values in 'timestamp' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['device_id_scaled'] = (df['device_id'] - df['device_id'].min()) / (df['device_id'].max() - df['device_id'].min())\ncorr = df.corr()\nprint(corr)\ndf['timestamp'].fillna(df['timestamp'].median(), inplace=True)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features, then one-hot encode the categorical column 'MonthlyCharges'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)\ndf = pd.get_dummies(df, columns=['MonthlyCharges'], prefix=['MonthlyCharges'])"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'sepal_width', then train a Random Forest Classifier to predict 'species', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf = pd.get_dummies(df, columns=['sepal_width'], prefix=['sepal_width'])\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['species'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'temperature' and display top 10 words, then detect outliers in 'consumption' using the IQR method, then one-hot encode the categorical column 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['temperature'])\nprint(vect.get_feature_names_out())\nQ1 = df['consumption'].quantile(0.25)\nQ3 = df['consumption'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['consumption'] < (Q1 - 1.5*IQR)) | (df['consumption'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['temperature'], prefix=['temperature'])"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Revenue'])\ny = df['Revenue']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then create a new feature 'SalePrice_ratio' as the ratio of 'SalePrice' to 'SalePrice', then clean text data in column 'SalePrice' by removing punctuation and stopwords, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['SalePrice_ratio'] = df['SalePrice'] / df['SalePrice']\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['SalePrice_clean'] = df['SalePrice'].apply(clean)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['YearBuilt'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then detect outliers in 'temperature' using the IQR method, then train a Random Forest Classifier to predict 'consumption', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nQ1 = df['temperature'].quantile(0.25)\nQ3 = df['temperature'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['temperature'] < (Q1 - 1.5*IQR)) | (df['temperature'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then clean text data in column 'heart_rate' by removing punctuation and stopwords, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['heart_rate_clean'] = df['heart_rate'].apply(clean)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['time'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then normalize the 'recovered' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['recovered_scaled'] = (df['recovered'] - df['recovered'].min()) / (df['recovered'].max() - df['recovered'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then clean text data in column 'location' by removing punctuation and stopwords, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nprint(df.describe())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['location_clean'] = df['location'].apply(clean)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model, then create a new feature 'species_ratio' as the ratio of 'species' to 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['species_ratio'] = df['species'] / df['species']"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then normalize the 'Close' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['Close_scaled'] = (df['Close'] - df['Close'].min()) / (df['Close'].max() - df['Close'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'timestamp' by imputing with median, then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['timestamp'].fillna(df['timestamp'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sensor_value'])\ny = df['sensor_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['sensor_value'].hist()\nplt.xlabel('sensor_value')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then handle missing values in 'distance' by imputing with median, then train a Linear Regression model to predict 'arrival_delay', then detect outliers in 'flight' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['distance'].fillna(df['distance'].median(), inplace=True)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nQ1 = df['flight'].quantile(0.25)\nQ3 = df['flight'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['flight'] < (Q1 - 1.5*IQR)) | (df['flight'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'temperature' and display top 10 words, then create a new feature 'humidity_ratio' as the ratio of 'humidity' to 'temperature', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['temperature'])\nprint(vect.get_feature_names_out())\ndf['humidity_ratio'] = df['humidity'] / df['temperature']\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then detect outliers in 'length' using the IQR method, then compute TF-IDF features for column 'sentiment' and display top 10 words, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nQ1 = df['length'].quantile(0.25)\nQ3 = df['length'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['length'] < (Q1 - 1.5*IQR)) | (df['length'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sentiment'])\nprint(vect.get_feature_names_out())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sentiment', then clean text data in column 'rating' by removing punctuation and stopwords, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['rating_clean'] = df['rating'].apply(clean)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then normalize the 'wind_speed' column using min-max scaling, then train a Random Forest Classifier to predict 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ncorr = df.corr()\nprint(corr)\ndf['wind_speed_scaled'] = (df['wind_speed'] - df['wind_speed'].min()) / (df['wind_speed'].max() - df['wind_speed'].min())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then handle missing values in 'age' by imputing with median, then plot a histogram of 'job', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['age'].fillna(df['age'].median(), inplace=True)\ndf['job'].hist()\nplt.xlabel('job')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['job'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'species', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'patient_id' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe(), then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['patient_id_clean'] = df['patient_id'].apply(clean)\nprint(df.describe())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['time'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then one-hot encode the categorical column 'temp', then create a new feature 'date_time_ratio' as the ratio of 'date_time' to 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nprint(df.describe())\ndf = pd.get_dummies(df, columns=['temp'], prefix=['temp'])\ndf['date_time_ratio'] = df['date_time'] / df['traffic_volume']"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display feature importances from the Random Forest model, then handle missing values in 'temperature' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['temperature'].fillna(df['temperature'].median(), inplace=True)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then normalize the 'temp' column using min-max scaling, then train a Linear Regression model to predict 'traffic_volume', then clean text data in column 'temp' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['temp_scaled'] = (df['temp'] - df['temp'].min()) / (df['temp'].max() - df['temp'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['temp_clean'] = df['temp'].apply(clean)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Close', then train a Random Forest Classifier to predict 'Close', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Close'].hist()\nplt.xlabel('Close')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Close'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then handle missing values in 'customer_id' by imputing with median, then display summary statistics of all numeric columns using df.describe(), then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['customer_id'].fillna(df['customer_id'].median(), inplace=True)\nprint(df.describe())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'product_id' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score, then plot a histogram of 'customer_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['product_id'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['customer_id'].hist()\nplt.xlabel('customer_id')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then handle missing values in 'temperature' by imputing with median, then plot a histogram of 'humidity', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['temperature'].fillna(df['temperature'].median(), inplace=True)\ndf['humidity'].hist()\nplt.xlabel('humidity')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'humidity', then display feature importances from the Random Forest model, then clean text data in column 'temperature' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf = pd.get_dummies(df, columns=['humidity'], prefix=['humidity'])\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['temperature_clean'] = df['temperature'].apply(clean)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then handle missing values in 'deaths' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['deaths'].fillna(df['deaths'].median(), inplace=True)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then handle missing values in 'oldbalanceOrg' by imputing with median, then train a Linear Regression model to predict 'isFraud', then clean text data in column 'oldbalanceOrg' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['oldbalanceOrg'].fillna(df['oldbalanceOrg'].median(), inplace=True)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['oldbalanceOrg_clean'] = df['oldbalanceOrg'].apply(clean)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Close', then one-hot encode the categorical column 'High', then create a new feature 'Date_ratio' as the ratio of 'Date' to 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['High'], prefix=['High'])\ndf['Date_ratio'] = df['Date'] / df['Close']"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then clean text data in column 'humidity' by removing punctuation and stopwords, then normalize the 'consumption' column using min-max scaling, then create a new feature 'temperature_ratio' as the ratio of 'temperature' to 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['humidity_clean'] = df['humidity'].apply(clean)\ndf['consumption_scaled'] = (df['consumption'] - df['consumption'].min()) / (df['consumption'].max() - df['consumption'].min())\ndf['temperature_ratio'] = df['temperature'] / df['consumption']"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['no2'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then one-hot encode the categorical column 'recovered', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ncorr = df.corr()\nprint(corr)\ndf = pd.get_dummies(df, columns=['recovered'], prefix=['recovered'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'ContractType', then create a new feature 'tenure_ratio' as the ratio of 'tenure' to 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['ContractType'], prefix=['ContractType'])\ndf['tenure_ratio'] = df['tenure'] / df['Churn']"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Churn', then handle missing values in 'TotalCharges' by imputing with median, then normalize the 'TotalCharges' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)\ndf['TotalCharges_scaled'] = (df['TotalCharges'] - df['TotalCharges'].min()) / (df['TotalCharges'].max() - df['TotalCharges'].min())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then detect outliers in 'sepal_width' using the IQR method, then plot a histogram of 'species', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nQ1 = df['sepal_width'].quantile(0.25)\nQ3 = df['sepal_width'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sepal_width'] < (Q1 - 1.5*IQR)) | (df['sepal_width'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['species'].hist()\nplt.xlabel('species')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then detect outliers in 'Fare' using the IQR method, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nQ1 = df['Fare'].quantile(0.25)\nQ3 = df['Fare'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Fare'] < (Q1 - 1.5*IQR)) | (df['Fare'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'LotArea', then handle missing values in 'OverallQual' by imputing with median, then train a Linear Regression model to predict 'SalePrice'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf = pd.get_dummies(df, columns=['LotArea'], prefix=['LotArea'])\ndf['OverallQual'].fillna(df['OverallQual'].median(), inplace=True)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then clean text data in column 'date_time' by removing punctuation and stopwords, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_time_clean'] = df['date_time'].apply(clean)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'arrival_delay', then clean text data in column 'carrier' by removing punctuation and stopwords, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['carrier_clean'] = df['carrier'].apply(clean)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['flight'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then normalize the 'Age' column using min-max scaling, then train a Linear Regression model to predict 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['Age_scaled'] = (df['Age'] - df['Age'].min()) / (df['Age'].max() - df['Age'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'petal_length' and display top 10 words, then display feature importances from the Random Forest model, then train a Random Forest Classifier to predict 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['petal_length'])\nprint(vect.get_feature_names_out())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'deaths', then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'date' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf = pd.get_dummies(df, columns=['deaths'], prefix=['deaths'])\nprint(df.describe())\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then clean text data in column 'TotalCharges' by removing punctuation and stopwords, then plot a histogram of 'Churn', then detect outliers in 'TotalCharges' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['TotalCharges_clean'] = df['TotalCharges'].apply(clean)\ndf['Churn'].hist()\nplt.xlabel('Churn')\nplt.ylabel('Frequency')\nplt.show()\nQ1 = df['TotalCharges'].quantile(0.25)\nQ3 = df['TotalCharges'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['TotalCharges'] < (Q1 - 1.5*IQR)) | (df['TotalCharges'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then create a new feature 'genre_ratio' as the ratio of 'genre' to 'sentiment', then normalize the 'sentiment' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['genre_ratio'] = df['genre'] / df['sentiment']\ndf['sentiment_scaled'] = (df['sentiment'] - df['sentiment'].min()) / (df['sentiment'].max() - df['sentiment'].min())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'ecg_reading', then clean text data in column 'patient_id' by removing punctuation and stopwords, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['patient_id_clean'] = df['patient_id'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'isFraud' and display top 10 words, then normalize the 'amount' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['isFraud'])\nprint(vect.get_feature_names_out())\ndf['amount_scaled'] = (df['amount'] - df['amount'].min()) / (df['amount'].max() - df['amount'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)\nprint(df.describe())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'y', then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Churn', then handle missing values in 'TotalCharges' by imputing with median, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)\nprint(df.describe())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then clean text data in column 'species' by removing punctuation and stopwords, then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['species_clean'] = df['species'].apply(clean)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then create a new feature 'total_amount_ratio' as the ratio of 'total_amount' to 'total_amount', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['total_amount_ratio'] = df['total_amount'] / df['total_amount']\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then create a new feature 'sepal_width_ratio' as the ratio of 'sepal_width' to 'species', then plot a histogram of 'petal_length', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['sepal_width_ratio'] = df['sepal_width'] / df['species']\ndf['petal_length'].hist()\nplt.xlabel('petal_length')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then normalize the 'user_id' column using min-max scaling, then compute TF-IDF features for column 'shares' and display top 10 words, then handle missing values in 'text' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['user_id_scaled'] = (df['user_id'] - df['user_id'].min()) / (df['user_id'].max() - df['user_id'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['shares'])\nprint(vect.get_feature_names_out())\ndf['text'].fillna(df['text'].median(), inplace=True)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'UnitsSold' by removing punctuation and stopwords, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Random Forest Classifier to predict 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['UnitsSold_clean'] = df['UnitsSold'].apply(clean)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Revenue'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then handle missing values in 'sepal_length' by imputing with median, then compute TF-IDF features for column 'sepal_length' and display top 10 words, then detect outliers in 'petal_length' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['sepal_length'].fillna(df['sepal_length'].median(), inplace=True)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sepal_length'])\nprint(vect.get_feature_names_out())\nQ1 = df['petal_length'].quantile(0.25)\nQ3 = df['petal_length'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['petal_length'] < (Q1 - 1.5*IQR)) | (df['petal_length'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['isFraud'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then plot a histogram of 'newbalanceOrig', then handle missing values in 'time' by imputing with median, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['newbalanceOrig'].hist()\nplt.xlabel('newbalanceOrig')\nplt.ylabel('Frequency')\nplt.show()\ndf['time'].fillna(df['time'].median(), inplace=True)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then create a new feature 'date_ratio' as the ratio of 'date' to 'temperature', then train a Random Forest Classifier to predict 'temperature', then clean text data in column 'date' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['date_ratio'] = df['date'] / df['temperature']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then plot a histogram of 'y', then normalize the 'y' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ncorr = df.corr()\nprint(corr)\ndf['y'].hist()\nplt.xlabel('y')\nplt.ylabel('Frequency')\nplt.show()\ndf['y_scaled'] = (df['y'] - df['y'].min()) / (df['y'].max() - df['y'].min())"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Survived', then plot a histogram of 'Fare', then compute TF-IDF features for column 'Sex' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['Fare'].hist()\nplt.xlabel('Fare')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Sex'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'Churn' using the IQR method, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['Churn'].quantile(0.25)\nQ3 = df['Churn'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Churn'] < (Q1 - 1.5*IQR)) | (df['Churn'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then detect outliers in 'sepal_length' using the IQR method, then handle missing values in 'sepal_width' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['sepal_length'].quantile(0.25)\nQ3 = df['sepal_length'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sepal_length'] < (Q1 - 1.5*IQR)) | (df['sepal_length'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['sepal_width'].fillna(df['sepal_width'].median(), inplace=True)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then split the data into training and testing sets with an 80-20 split, then create a new feature 'date_ratio' as the ratio of 'date' to 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nprint(df.describe())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['date_ratio'] = df['date'] / df['consumption']"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'TotalCharges', then clean text data in column 'TotalCharges' by removing punctuation and stopwords, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf = pd.get_dummies(df, columns=['TotalCharges'], prefix=['TotalCharges'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['TotalCharges_clean'] = df['TotalCharges'].apply(clean)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'transaction_id', then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf = pd.get_dummies(df, columns=['transaction_id'], prefix=['transaction_id'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then split the data into training and testing sets with an 80-20 split, then compute TF-IDF features for column 'job' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['job'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then detect outliers in 'text' using the IQR method, then handle missing values in 'shares' by imputing with median, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nQ1 = df['text'].quantile(0.25)\nQ3 = df['text'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['text'] < (Q1 - 1.5*IQR)) | (df['text'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['shares'].fillna(df['shares'].median(), inplace=True)\nprint(df.describe())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Volume', then split the data into training and testing sets with an 80-20 split, then detect outliers in 'Volume' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf = pd.get_dummies(df, columns=['Volume'], prefix=['Volume'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Close'])\ny = df['Close']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['Volume'].quantile(0.25)\nQ3 = df['Volume'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Volume'] < (Q1 - 1.5*IQR)) | (df['Volume'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then clean text data in column 'tenure' by removing punctuation and stopwords, then train a Random Forest Classifier to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['tenure_clean'] = df['tenure'].apply(clean)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['wind_speed'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then clean text data in column 'newbalanceOrig' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['newbalanceOrig_clean'] = df['newbalanceOrig'].apply(clean)\nprint(df.describe())"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then plot a histogram of 'transaction_id', then display feature importances from the Random Forest model, then clean text data in column 'transaction_id' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['transaction_id'].hist()\nplt.xlabel('transaction_id')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['transaction_id_clean'] = df['transaction_id'].apply(clean)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'open' using the IQR method, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['store'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['open'].quantile(0.25)\nQ3 = df['open'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['open'] < (Q1 - 1.5*IQR)) | (df['open'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then detect outliers in 'deaths' using the IQR method, then train a Linear Regression model to predict 'confirmed', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nQ1 = df['deaths'].quantile(0.25)\nQ3 = df['deaths'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['deaths'] < (Q1 - 1.5*IQR)) | (df['deaths'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'ecg_reading', then detect outliers in 'time' using the IQR method, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nQ1 = df['time'].quantile(0.25)\nQ3 = df['time'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['time'] < (Q1 - 1.5*IQR)) | (df['time'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then normalize the 'date' column using min-max scaling, then clean text data in column 'date' by removing punctuation and stopwords, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['date_scaled'] = (df['date'] - df['date'].min()) / (df['date'].max() - df['date'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then create a new feature 'y_ratio' as the ratio of 'y' to 'y', then perform time-series forecasting using ARIMA to predict the next 12 periods, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['y_ratio'] = df['y'] / df['y']\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['marital'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nprint(df.describe())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sentiment', then detect outliers in 'sentiment' using the IQR method, then one-hot encode the categorical column 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nQ1 = df['sentiment'].quantile(0.25)\nQ3 = df['sentiment'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sentiment'] < (Q1 - 1.5*IQR)) | (df['sentiment'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['sentiment'], prefix=['sentiment'])"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'status' by imputing with median, then plot a histogram of 'status', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['status'].fillna(df['status'].median(), inplace=True)\ndf['status'].hist()\nplt.xlabel('status')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Churn', then detect outliers in 'ContractType' using the IQR method, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nQ1 = df['ContractType'].quantile(0.25)\nQ3 = df['ContractType'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['ContractType'] < (Q1 - 1.5*IQR)) | (df['ContractType'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nprint(df.describe())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'Revenue' using the IQR method, then clean text data in column 'Region' by removing punctuation and stopwords, then plot a histogram of 'Product'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nQ1 = df['Revenue'].quantile(0.25)\nQ3 = df['Revenue'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Revenue'] < (Q1 - 1.5*IQR)) | (df['Revenue'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Region_clean'] = df['Region'].apply(clean)\ndf['Product'].hist()\nplt.xlabel('Product')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods, then clean text data in column 'recovered' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['recovered_clean'] = df['recovered'].apply(clean)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display feature importances from the Random Forest model, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['text'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['text'])\ny = df['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'Fare' by imputing with median, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nprint(df.describe())\ndf['Fare'].fillna(df['Fare'].median(), inplace=True)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then plot a histogram of 'date', then one-hot encode the categorical column 'consumption', then create a new feature 'pressure_ratio' as the ratio of 'pressure' to 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['date'].hist()\nplt.xlabel('date')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['consumption'], prefix=['consumption'])\ndf['pressure_ratio'] = df['pressure'] / df['consumption']"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then plot a histogram of 'snow_1h', then compute TF-IDF features for column 'temp' and display top 10 words, then train a Random Forest Classifier to predict 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['snow_1h'].hist()\nplt.xlabel('snow_1h')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['temp'])\nprint(vect.get_feature_names_out())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sensor_value', then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sensor_value'])\ny = df['sensor_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display feature importances from the Random Forest model, then plot a histogram of 'so2'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['pm10'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['so2'].hist()\nplt.xlabel('so2')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['pm2_5'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'total_amount', then detect outliers in 'quantity' using the IQR method, then plot a histogram of 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['quantity'].quantile(0.25)\nQ3 = df['quantity'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['quantity'] < (Q1 - 1.5*IQR)) | (df['quantity'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['total_amount'].hist()\nplt.xlabel('total_amount')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then normalize the 'Open' column using min-max scaling, then clean text data in column 'Low' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Open_scaled'] = (df['Open'] - df['Open'].min()) / (df['Open'].max() - df['Open'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Low_clean'] = df['Low'].apply(clean)\nprint(df.describe())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'petal_width', then plot a histogram of 'sepal_width', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf = pd.get_dummies(df, columns=['petal_width'], prefix=['petal_width'])\ndf['sepal_width'].hist()\nplt.xlabel('sepal_width')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sepal_width'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then one-hot encode the categorical column 'confirmed', then create a new feature 'confirmed_ratio' as the ratio of 'confirmed' to 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf = pd.get_dummies(df, columns=['confirmed'], prefix=['confirmed'])\ndf['confirmed_ratio'] = df['confirmed'] / df['confirmed']"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then handle missing values in 'temperature' by imputing with median, then plot a histogram of 'humidity', then compute TF-IDF features for column 'humidity' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['temperature'].fillna(df['temperature'].median(), inplace=True)\ndf['humidity'].hist()\nplt.xlabel('humidity')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['humidity'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'y', then normalize the 'job' column using min-max scaling, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf = pd.get_dummies(df, columns=['y'], prefix=['y'])\ndf['job_scaled'] = (df['job'] - df['job'].min()) / (df['job'].max() - df['job'].min())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'pm2_5', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then plot a histogram of 'text', then perform time-series forecasting using ARIMA to predict the next 12 periods, then one-hot encode the categorical column 'user_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['text'].hist()\nplt.xlabel('text')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['text'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf = pd.get_dummies(df, columns=['user_id'], prefix=['user_id'])"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'Close', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Close'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'species', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sensor_value', then one-hot encode the categorical column 'timestamp', then plot a histogram of 'status'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['timestamp'], prefix=['timestamp'])\ndf['status'].hist()\nplt.xlabel('status')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'OverallQual' and display top 10 words, then display summary statistics of all numeric columns using df.describe(), then normalize the 'SalePrice' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['OverallQual'])\nprint(vect.get_feature_names_out())\nprint(df.describe())\ndf['SalePrice_scaled'] = (df['SalePrice'] - df['SalePrice'].min()) / (df['SalePrice'].max() - df['SalePrice'].min())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then handle missing values in 'no2' by imputing with median, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['no2'].fillna(df['no2'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Date', then train a Linear Regression model to predict 'Revenue', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf = pd.get_dummies(df, columns=['Date'], prefix=['Date'])\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then detect outliers in 'rating' using the IQR method, then train a Random Forest Classifier to predict 'sentiment', then one-hot encode the categorical column 'rating'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nQ1 = df['rating'].quantile(0.25)\nQ3 = df['rating'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['rating'] < (Q1 - 1.5*IQR)) | (df['rating'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['rating'], prefix=['rating'])"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'traffic_volume', then create a new feature 'date_time_ratio' as the ratio of 'date_time' to 'traffic_volume', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['date_time_ratio'] = df['date_time'] / df['traffic_volume']\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['traffic_volume'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then create a new feature 'sensor_value_ratio' as the ratio of 'sensor_value' to 'sensor_value', then train a Linear Regression model to predict 'sensor_value', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['sensor_value_ratio'] = df['sensor_value'] / df['sensor_value']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sensor_value'])\ny = df['sensor_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then create a new feature 'Low_ratio' as the ratio of 'Low' to 'Close', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ncorr = df.corr()\nprint(corr)\ndf['Low_ratio'] = df['Low'] / df['Close']\nprint(df.describe())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'Churn', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Churn'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then handle missing values in 'product_id' by imputing with median, then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'customer_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['product_id'].fillna(df['product_id'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['customer_id'].hist()\nplt.xlabel('customer_id')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then normalize the 'sepal_length' column using min-max scaling, then clean text data in column 'petal_width' by removing punctuation and stopwords, then detect outliers in 'petal_width' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['sepal_length_scaled'] = (df['sepal_length'] - df['sepal_length'].min()) / (df['sepal_length'].max() - df['sepal_length'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['petal_width_clean'] = df['petal_width'].apply(clean)\nQ1 = df['petal_width'].quantile(0.25)\nQ3 = df['petal_width'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['petal_width'] < (Q1 - 1.5*IQR)) | (df['petal_width'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sales'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'arrival_delay', then train a Linear Regression model to predict 'arrival_delay', then detect outliers in 'carrier' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nQ1 = df['carrier'].quantile(0.25)\nQ3 = df['carrier'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['carrier'] < (Q1 - 1.5*IQR)) | (df['carrier'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'temperature', then train a Linear Regression model to predict 'temperature', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['temperature'].hist()\nplt.xlabel('temperature')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then normalize the 'timestamp' column using min-max scaling, then create a new feature 'sensor_value_ratio' as the ratio of 'sensor_value' to 'sensor_value', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['timestamp_scaled'] = (df['timestamp'] - df['timestamp'].min()) / (df['timestamp'].max() - df['timestamp'].min())\ndf['sensor_value_ratio'] = df['sensor_value'] / df['sensor_value']\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Revenue', then display feature importances from the Random Forest model, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split, then one-hot encode the categorical column 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf = pd.get_dummies(df, columns=['sentiment'], prefix=['sentiment'])"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Neighborhood', then display summary statistics of all numeric columns using df.describe(), then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['Neighborhood'].hist()\nplt.xlabel('Neighborhood')\nplt.ylabel('Frequency')\nplt.show()\nprint(df.describe())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then detect outliers in 'review' using the IQR method, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nQ1 = df['review'].quantile(0.25)\nQ3 = df['review'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['review'] < (Q1 - 1.5*IQR)) | (df['review'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nprint(df.describe())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'time', then train a Linear Regression model to predict 'isFraud', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf = pd.get_dummies(df, columns=['time'], prefix=['time'])\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['time'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'distance', then handle missing values in 'departure_delay' by imputing with median, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf = pd.get_dummies(df, columns=['distance'], prefix=['distance'])\ndf['departure_delay'].fillna(df['departure_delay'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'snow_1h'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['snow_1h'], prefix=['snow_1h'])"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then perform time-series forecasting using ARIMA to predict the next 12 periods, then plot a histogram of 'newbalanceOrig'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['isFraud'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['newbalanceOrig'].hist()\nplt.xlabel('newbalanceOrig')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then detect outliers in 'flight' using the IQR method, then split the data into training and testing sets with an 80-20 split, then create a new feature 'distance_ratio' as the ratio of 'distance' to 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nQ1 = df['flight'].quantile(0.25)\nQ3 = df['flight'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['flight'] < (Q1 - 1.5*IQR)) | (df['flight'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['distance_ratio'] = df['distance'] / df['arrival_delay']"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe(), then create a new feature 'so2_ratio' as the ratio of 'so2' to 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())\ndf['so2_ratio'] = df['so2'] / df['pm2_5']"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'total_amount', then one-hot encode the categorical column 'total_amount', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['total_amount'], prefix=['total_amount'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'date' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then evaluate the model performance using RMSE and R\u00b2 score, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['humidity'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'SalePrice', then create a new feature 'Neighborhood_ratio' as the ratio of 'Neighborhood' to 'SalePrice', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['Neighborhood_ratio'] = df['Neighborhood'] / df['SalePrice']\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then clean text data in column 'shares' by removing punctuation and stopwords, then compute TF-IDF features for column 'post_id' and display top 10 words, then train a Linear Regression model to predict 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['shares_clean'] = df['shares'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['post_id'])\nprint(vect.get_feature_names_out())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then normalize the 'Pclass' column using min-max scaling, then train a Linear Regression model to predict 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nprint(df.describe())\ndf['Pclass_scaled'] = (df['Pclass'] - df['Pclass'].min()) / (df['Pclass'].max() - df['Pclass'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'recovered' and display top 10 words, then normalize the 'country' column using min-max scaling, then train a Random Forest Classifier to predict 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['recovered'])\nprint(vect.get_feature_names_out())\ndf['country_scaled'] = (df['country'] - df['country'].min()) / (df['country'].max() - df['country'].min())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then handle missing values in 'product_id' by imputing with median, then perform time-series forecasting using ARIMA to predict the next 12 periods, then one-hot encode the categorical column 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['product_id'].fillna(df['product_id'].median(), inplace=True)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['total_amount'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf = pd.get_dummies(df, columns=['total_amount'], prefix=['total_amount'])"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then create a new feature 'Fare_ratio' as the ratio of 'Fare' to 'Survived', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['Fare_ratio'] = df['Fare'] / df['Survived']\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Close', then one-hot encode the categorical column 'Date', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['Date'], prefix=['Date'])\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'Sex' and display top 10 words, then calculate the correlation matrix for numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Sex'])\nprint(vect.get_feature_names_out())\ncorr = df.corr()\nprint(corr)\nprint(df.describe())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then handle missing values in 'likes' by imputing with median, then plot a histogram of 'post_id', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['likes'].fillna(df['likes'].median(), inplace=True)\ndf['post_id'].hist()\nplt.xlabel('post_id')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then create a new feature 'quality_ratio' as the ratio of 'quality' to 'ecg_reading', then one-hot encode the categorical column 'patient_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nprint(df.describe())\ndf['quality_ratio'] = df['quality'] / df['ecg_reading']\ndf = pd.get_dummies(df, columns=['patient_id'], prefix=['patient_id'])"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then create a new feature 'temperature_ratio' as the ratio of 'temperature' to 'temperature', then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['temperature_ratio'] = df['temperature'] / df['temperature']\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then handle missing values in 'pressure' by imputing with median, then one-hot encode the categorical column 'temperature', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['pressure'].fillna(df['pressure'].median(), inplace=True)\ndf = pd.get_dummies(df, columns=['temperature'], prefix=['temperature'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then normalize the 'Open' column using min-max scaling, then display summary statistics of all numeric columns using df.describe(), then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Open_scaled'] = (df['Open'] - df['Open'].min()) / (df['Open'].max() - df['Open'].min())\nprint(df.describe())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Random Forest Classifier to predict 'y', then plot a histogram of 'education'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nprint(df.describe())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['education'].hist()\nplt.xlabel('education')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then handle missing values in 'likes' by imputing with median, then compute TF-IDF features for column 'likes' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['likes'].fillna(df['likes'].median(), inplace=True)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['likes'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then detect outliers in 'transaction_id' using the IQR method, then train a Random Forest Classifier to predict 'total_amount', then train a Linear Regression model to predict 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nQ1 = df['transaction_id'].quantile(0.25)\nQ3 = df['transaction_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['transaction_id'] < (Q1 - 1.5*IQR)) | (df['transaction_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then clean text data in column 'arrival_delay' by removing punctuation and stopwords, then compute TF-IDF features for column 'distance' and display top 10 words, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['arrival_delay_clean'] = df['arrival_delay'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['distance'])\nprint(vect.get_feature_names_out())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then clean text data in column 'OverallQual' by removing punctuation and stopwords, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['OverallQual_clean'] = df['OverallQual'].apply(clean)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then create a new feature 'review_ratio' as the ratio of 'review' to 'sentiment', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nprint(df.describe())\ndf['review_ratio'] = df['review'] / df['sentiment']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'ecg_reading', then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'temperature', then plot a histogram of 'precipitation', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['precipitation'].hist()\nplt.xlabel('precipitation')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then handle missing values in 'age' by imputing with median, then plot a histogram of 'job', then normalize the 'age' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['age'].fillna(df['age'].median(), inplace=True)\ndf['job'].hist()\nplt.xlabel('job')\nplt.ylabel('Frequency')\nplt.show()\ndf['age_scaled'] = (df['age'] - df['age'].min()) / (df['age'].max() - df['age'].min())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then plot a histogram of 'sepal_length', then clean text data in column 'sepal_length' by removing punctuation and stopwords, then train a Random Forest Classifier to predict 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['sepal_length'].hist()\nplt.xlabel('sepal_length')\nplt.ylabel('Frequency')\nplt.show()\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sepal_length_clean'] = df['sepal_length'].apply(clean)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'text', then normalize the 'shares' column using min-max scaling, then clean text data in column 'likes' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['shares_scaled'] = (df['shares'] - df['shares'].min()) / (df['shares'].max() - df['shares'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['likes_clean'] = df['likes'].apply(clean)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then create a new feature 'TotalCharges_ratio' as the ratio of 'TotalCharges' to 'Churn', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ncorr = df.corr()\nprint(corr)\ndf['TotalCharges_ratio'] = df['TotalCharges'] / df['Churn']\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['MonthlyCharges'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Churn', then create a new feature 'MonthlyCharges_ratio' as the ratio of 'MonthlyCharges' to 'Churn', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['MonthlyCharges_ratio'] = df['MonthlyCharges'] / df['Churn']\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then handle missing values in 'distance' by imputing with median, then normalize the 'flight' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['distance'].fillna(df['distance'].median(), inplace=True)\ndf['flight_scaled'] = (df['flight'] - df['flight'].min()) / (df['flight'].max() - df['flight'].min())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then normalize the 'Product' column using min-max scaling, then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'Region' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Product_scaled'] = (df['Product'] - df['Product'].min()) / (df['Product'].max() - df['Product'].min())\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Region'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split, then one-hot encode the categorical column 'pm10'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf = pd.get_dummies(df, columns=['pm10'], prefix=['pm10'])"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'newbalanceOrig' column using min-max scaling, then one-hot encode the categorical column 'amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['newbalanceOrig_scaled'] = (df['newbalanceOrig'] - df['newbalanceOrig'].min()) / (df['newbalanceOrig'].max() - df['newbalanceOrig'].min())\ndf = pd.get_dummies(df, columns=['amount'], prefix=['amount'])"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then normalize the 'Volume' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'Low'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Volume_scaled'] = (df['Volume'] - df['Volume'].min()) / (df['Volume'].max() - df['Volume'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['Low'], prefix=['Low'])"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then detect outliers in 'consumption' using the IQR method, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['consumption'].quantile(0.25)\nQ3 = df['consumption'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['consumption'] < (Q1 - 1.5*IQR)) | (df['consumption'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then create a new feature 'Sex_ratio' as the ratio of 'Sex' to 'Survived', then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'Survived' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Sex_ratio'] = df['Sex'] / df['Survived']\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Sex'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['Survived_scaled'] = (df['Survived'] - df['Survived'].min()) / (df['Survived'].max() - df['Survived'].min())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then create a new feature 'date_ratio' as the ratio of 'date' to 'temperature', then clean text data in column 'precipitation' by removing punctuation and stopwords, then one-hot encode the categorical column 'wind_speed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['date_ratio'] = df['date'] / df['temperature']\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['precipitation_clean'] = df['precipitation'].apply(clean)\ndf = pd.get_dummies(df, columns=['wind_speed'], prefix=['wind_speed'])"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'pm2_5', then display feature importances from the Random Forest model, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then normalize the 'YearBuilt' column using min-max scaling, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['YearBuilt_scaled'] = (df['YearBuilt'] - df['YearBuilt'].min()) / (df['YearBuilt'].max() - df['YearBuilt'].min())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['LotArea'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then clean text data in column 'review' by removing punctuation and stopwords, then normalize the 'genre' column using min-max scaling, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['review_clean'] = df['review'].apply(clean)\ndf['genre_scaled'] = (df['genre'] - df['genre'].min()) / (df['genre'].max() - df['genre'].min())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'oldbalanceOrg' and display top 10 words, then split the data into training and testing sets with an 80-20 split, then one-hot encode the categorical column 'oldbalanceOrg'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['oldbalanceOrg'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf = pd.get_dummies(df, columns=['oldbalanceOrg'], prefix=['oldbalanceOrg'])"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'location' using the IQR method, then handle missing values in 'status' by imputing with median, then create a new feature 'status_ratio' as the ratio of 'status' to 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nQ1 = df['location'].quantile(0.25)\nQ3 = df['location'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['location'] < (Q1 - 1.5*IQR)) | (df['location'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['status'].fillna(df['status'].median(), inplace=True)\ndf['status_ratio'] = df['status'] / df['sensor_value']"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then detect outliers in 'length' using the IQR method, then normalize the 'genre' column using min-max scaling, then clean text data in column 'sentiment' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nQ1 = df['length'].quantile(0.25)\nQ3 = df['length'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['length'] < (Q1 - 1.5*IQR)) | (df['length'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['genre_scaled'] = (df['genre'] - df['genre'].min()) / (df['genre'].max() - df['genre'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sentiment_clean'] = df['sentiment'].apply(clean)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'OverallQual', then create a new feature 'YearBuilt_ratio' as the ratio of 'YearBuilt' to 'SalePrice', then clean text data in column 'OverallQual' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf = pd.get_dummies(df, columns=['OverallQual'], prefix=['OverallQual'])\ndf['YearBuilt_ratio'] = df['YearBuilt'] / df['SalePrice']\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['OverallQual_clean'] = df['OverallQual'].apply(clean)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Open'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then handle missing values in 'sentiment' by imputing with median, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['sentiment'].fillna(df['sentiment'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'device_id' by imputing with median, then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['device_id'].fillna(df['device_id'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features, then normalize the 'day_of_week' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)\ndf['day_of_week_scaled'] = (df['day_of_week'] - df['day_of_week'].min()) / (df['day_of_week'].max() - df['day_of_week'].min())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'ecg_reading', then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Low', then create a new feature 'High_ratio' as the ratio of 'High' to 'Close', then one-hot encode the categorical column 'Volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Low'].hist()\nplt.xlabel('Low')\nplt.ylabel('Frequency')\nplt.show()\ndf['High_ratio'] = df['High'] / df['Close']\ndf = pd.get_dummies(df, columns=['Volume'], prefix=['Volume'])"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'deaths', then evaluate the model performance using RMSE and R\u00b2 score, then detect outliers in 'deaths' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf = pd.get_dummies(df, columns=['deaths'], prefix=['deaths'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nQ1 = df['deaths'].quantile(0.25)\nQ3 = df['deaths'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['deaths'] < (Q1 - 1.5*IQR)) | (df['deaths'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then handle missing values in 'review' by imputing with median, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['review'].fillna(df['review'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'likes', then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf = pd.get_dummies(df, columns=['likes'], prefix=['likes'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then handle missing values in 'distance' by imputing with median, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['distance'].fillna(df['distance'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'SalePrice', then train a Linear Regression model to predict 'SalePrice', then handle missing values in 'SalePrice' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['SalePrice'].fillna(df['SalePrice'].median(), inplace=True)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then handle missing values in 'TotalCharges' by imputing with median, then compute TF-IDF features for column 'ContractType' and display top 10 words, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['ContractType'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then display summary statistics of all numeric columns using df.describe(), then train a Linear Regression model to predict 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nprint(df.describe())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'pm2_5', then evaluate the model performance using RMSE and R\u00b2 score, then detect outliers in 'so2' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nQ1 = df['so2'].quantile(0.25)\nQ3 = df['so2'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['so2'] < (Q1 - 1.5*IQR)) | (df['so2'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then normalize the 'timestamp' column using min-max scaling, then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['timestamp_scaled'] = (df['timestamp'] - df['timestamp'].min()) / (df['timestamp'].max() - df['timestamp'].min())\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'quality' by removing punctuation and stopwords, then split the data into training and testing sets with an 80-20 split, then detect outliers in 'patient_id' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['quality_clean'] = df['quality'].apply(clean)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['ecg_reading'])\ny = df['ecg_reading']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['patient_id'].quantile(0.25)\nQ3 = df['patient_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['patient_id'] < (Q1 - 1.5*IQR)) | (df['patient_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'ContractType', then handle missing values in 'TotalCharges' by imputing with median, then create a new feature 'Churn_ratio' as the ratio of 'Churn' to 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf = pd.get_dummies(df, columns=['ContractType'], prefix=['ContractType'])\ndf['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)\ndf['Churn_ratio'] = df['Churn'] / df['Churn']"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then normalize the 'LotArea' column using min-max scaling, then train a Linear Regression model to predict 'SalePrice', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['LotArea_scaled'] = (df['LotArea'] - df['LotArea'].min()) / (df['LotArea'].max() - df['LotArea'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then detect outliers in 'transaction_id' using the IQR method, then split the data into training and testing sets with an 80-20 split, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nQ1 = df['transaction_id'].quantile(0.25)\nQ3 = df['transaction_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['transaction_id'] < (Q1 - 1.5*IQR)) | (df['transaction_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then normalize the 'date' column using min-max scaling, then train a Linear Regression model to predict 'temperature', then compute TF-IDF features for column 'humidity' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['date_scaled'] = (df['date'] - df['date'].min()) / (df['date'].max() - df['date'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['humidity'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then create a new feature 'deaths_ratio' as the ratio of 'deaths' to 'confirmed', then display summary statistics of all numeric columns using df.describe(), then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['deaths_ratio'] = df['deaths'] / df['confirmed']\nprint(df.describe())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then detect outliers in 'sepal_length' using the IQR method, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['sepal_length'].quantile(0.25)\nQ3 = df['sepal_length'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sepal_length'] < (Q1 - 1.5*IQR)) | (df['sepal_length'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nprint(df.describe())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Random Forest Classifier to predict 'consumption', then train a Linear Regression model to predict 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['humidity'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then clean text data in column 'user_id' by removing punctuation and stopwords, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['user_id_clean'] = df['user_id'].apply(clean)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then plot a histogram of 'newbalanceOrig', then calculate the correlation matrix for numeric features, then train a Random Forest Classifier to predict 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['newbalanceOrig'].hist()\nplt.xlabel('newbalanceOrig')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'total_amount', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'pressure' using the IQR method, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['pressure'].quantile(0.25)\nQ3 = df['pressure'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['pressure'] < (Q1 - 1.5*IQR)) | (df['pressure'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'flight', then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf = pd.get_dummies(df, columns=['flight'], prefix=['flight'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'pressure' column using min-max scaling, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['pressure_scaled'] = (df['pressure'] - df['pressure'].min()) / (df['pressure'].max() - df['pressure'].min())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['temperature'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'confirmed', then train a Random Forest Classifier to predict 'confirmed', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then clean text data in column 'education' by removing punctuation and stopwords, then evaluate the model performance using RMSE and R\u00b2 score, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['education_clean'] = df['education'].apply(clean)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['marital'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'arrival_delay', then clean text data in column 'carrier' by removing punctuation and stopwords, then detect outliers in 'carrier' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['carrier_clean'] = df['carrier'].apply(clean)\nQ1 = df['carrier'].quantile(0.25)\nQ3 = df['carrier'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['carrier'] < (Q1 - 1.5*IQR)) | (df['carrier'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then split the data into training and testing sets with an 80-20 split, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nprint(df.describe())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Close'])\ny = df['Close']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then normalize the 'sales' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['sales_scaled'] = (df['sales'] - df['sales'].min()) / (df['sales'].max() - df['sales'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then detect outliers in 'departure_delay' using the IQR method, then create a new feature 'arrival_delay_ratio' as the ratio of 'arrival_delay' to 'arrival_delay', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nQ1 = df['departure_delay'].quantile(0.25)\nQ3 = df['departure_delay'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['departure_delay'] < (Q1 - 1.5*IQR)) | (df['departure_delay'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['arrival_delay_ratio'] = df['arrival_delay'] / df['arrival_delay']\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['departure_delay'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Close', then one-hot encode the categorical column 'Open', then compute TF-IDF features for column 'Low' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['Open'], prefix=['Open'])\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Low'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then clean text data in column 'day_of_week' by removing punctuation and stopwords, then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['day_of_week_clean'] = df['day_of_week'].apply(clean)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then clean text data in column 'review' by removing punctuation and stopwords, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['review_clean'] = df['review'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods, then one-hot encode the categorical column 'newbalanceOrig'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['isFraud'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf = pd.get_dummies(df, columns=['newbalanceOrig'], prefix=['newbalanceOrig'])"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then perform K-Means clustering with k=3 on numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Churn'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'species', then train a Linear Regression model to predict 'species', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf = pd.get_dummies(df, columns=['species'], prefix=['species'])\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then compute TF-IDF features for column 'Sex' and display top 10 words, then handle missing values in 'Age' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Sex'])\nprint(vect.get_feature_names_out())\ndf['Age'].fillna(df['Age'].median(), inplace=True)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sensor_value', then train a Random Forest Classifier to predict 'sensor_value', then one-hot encode the categorical column 'status'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['status'], prefix=['status'])"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then plot a histogram of 'open', then one-hot encode the categorical column 'store', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['open'].hist()\nplt.xlabel('open')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['store'], prefix=['store'])\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'quantity' column using min-max scaling, then detect outliers in 'customer_id' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['quantity_scaled'] = (df['quantity'] - df['quantity'].min()) / (df['quantity'].max() - df['quantity'].min())\nQ1 = df['customer_id'].quantile(0.25)\nQ3 = df['customer_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['customer_id'] < (Q1 - 1.5*IQR)) | (df['customer_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'Pclass'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['Pclass'], prefix=['Pclass'])"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then calculate the correlation matrix for numeric features, then normalize the 'amount' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nprint(df.describe())\ncorr = df.corr()\nprint(corr)\ndf['amount_scaled'] = (df['amount'] - df['amount'].min()) / (df['amount'].max() - df['amount'].min())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then handle missing values in 'customers' by imputing with median, then create a new feature 'open_ratio' as the ratio of 'open' to 'sales', then normalize the 'open' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['customers'].fillna(df['customers'].median(), inplace=True)\ndf['open_ratio'] = df['open'] / df['sales']\ndf['open_scaled'] = (df['open'] - df['open'].min()) / (df['open'].max() - df['open'].min())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then plot a histogram of 'High', then create a new feature 'High_ratio' as the ratio of 'High' to 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['High'].hist()\nplt.xlabel('High')\nplt.ylabel('Frequency')\nplt.show()\ndf['High_ratio'] = df['High'] / df['Close']"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then clean text data in column 'TotalCharges' by removing punctuation and stopwords, then split the data into training and testing sets with an 80-20 split, then handle missing values in 'Churn' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['TotalCharges_clean'] = df['TotalCharges'].apply(clean)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['Churn'].fillna(df['Churn'].median(), inplace=True)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Churn', then detect outliers in 'MonthlyCharges' using the IQR method, then plot a histogram of 'tenure'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nQ1 = df['MonthlyCharges'].quantile(0.25)\nQ3 = df['MonthlyCharges'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['MonthlyCharges'] < (Q1 - 1.5*IQR)) | (df['MonthlyCharges'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['tenure'].hist()\nplt.xlabel('tenure')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then compute TF-IDF features for column 'device_id' and display top 10 words, then detect outliers in 'sensor_value' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sensor_value'])\ny = df['sensor_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['device_id'])\nprint(vect.get_feature_names_out())\nQ1 = df['sensor_value'].quantile(0.25)\nQ3 = df['sensor_value'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sensor_value'] < (Q1 - 1.5*IQR)) | (df['sensor_value'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then clean text data in column 'sepal_width' by removing punctuation and stopwords, then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sepal_width_clean'] = df['sepal_width'].apply(clean)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform time-series forecasting using ARIMA to predict the next 12 periods, then compute TF-IDF features for column 'transaction_id' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nprint(df.describe())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['customer_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['transaction_id'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'confirmed', then compute TF-IDF features for column 'recovered' and display top 10 words, then train a Random Forest Classifier to predict 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['recovered'])\nprint(vect.get_feature_names_out())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'total_amount', then split the data into training and testing sets with an 80-20 split, then normalize the 'customer_id' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['customer_id_scaled'] = (df['customer_id'] - df['customer_id'].min()) / (df['customer_id'].max() - df['customer_id'].min())"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'SalePrice', then compute TF-IDF features for column 'LotArea' and display top 10 words, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['LotArea'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'pm2_5', then normalize the 'pm2_5' column using min-max scaling, then handle missing values in 'so2' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['pm2_5_scaled'] = (df['pm2_5'] - df['pm2_5'].min()) / (df['pm2_5'].max() - df['pm2_5'].min())\ndf['so2'].fillna(df['so2'].median(), inplace=True)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then detect outliers in 'text' using the IQR method, then compute TF-IDF features for column 'shares' and display top 10 words, then plot a histogram of 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nQ1 = df['text'].quantile(0.25)\nQ3 = df['text'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['text'] < (Q1 - 1.5*IQR)) | (df['text'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['shares'])\nprint(vect.get_feature_names_out())\ndf['text'].hist()\nplt.xlabel('text')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then normalize the 'High' column using min-max scaling, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['High_scaled'] = (df['High'] - df['High'].min()) / (df['High'].max() - df['High'].min())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Volume'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'temperature', then detect outliers in 'humidity' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nQ1 = df['humidity'].quantile(0.25)\nQ3 = df['humidity'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['humidity'] < (Q1 - 1.5*IQR)) | (df['humidity'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then clean text data in column 'shares' by removing punctuation and stopwords, then evaluate the model performance using RMSE and R\u00b2 score, then create a new feature 'likes_ratio' as the ratio of 'likes' to 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['shares_clean'] = df['shares'].apply(clean)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['likes_ratio'] = df['likes'] / df['text']"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then one-hot encode the categorical column 'country', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf = pd.get_dummies(df, columns=['country'], prefix=['country'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Random Forest Classifier to predict 'ecg_reading', then create a new feature 'ecg_reading_ratio' as the ratio of 'ecg_reading' to 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nprint(df.describe())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['ecg_reading_ratio'] = df['ecg_reading'] / df['ecg_reading']"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'temperature' by imputing with median, then calculate the correlation matrix for numeric features, then detect outliers in 'date' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['temperature'].fillna(df['temperature'].median(), inplace=True)\ncorr = df.corr()\nprint(corr)\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'y', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then clean text data in column 'wind_speed' by removing punctuation and stopwords, then compute TF-IDF features for column 'date' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['wind_speed_clean'] = df['wind_speed'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'confirmed', then detect outliers in 'recovered' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf = pd.get_dummies(df, columns=['confirmed'], prefix=['confirmed'])\nQ1 = df['recovered'].quantile(0.25)\nQ3 = df['recovered'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['recovered'] < (Q1 - 1.5*IQR)) | (df['recovered'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then normalize the 'Open' column using min-max scaling, then train a Random Forest Classifier to predict 'Close', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Open_scaled'] = (df['Open'] - df['Open'].min()) / (df['Open'].max() - df['Open'].min())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Close'])\ny = df['Close']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then one-hot encode the categorical column 'wind_speed', then plot a histogram of 'date'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nprint(df.describe())\ndf = pd.get_dummies(df, columns=['wind_speed'], prefix=['wind_speed'])\ndf['date'].hist()\nplt.xlabel('date')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'total_amount', then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then clean text data in column 'rain_1h' by removing punctuation and stopwords, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ncorr = df.corr()\nprint(corr)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['rain_1h_clean'] = df['rain_1h'].apply(clean)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Survived', then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'Survived' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Survived'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'temperature' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nprint(df.describe())\nQ1 = df['temperature'].quantile(0.25)\nQ3 = df['temperature'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['temperature'] < (Q1 - 1.5*IQR)) | (df['temperature'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Survived', then split the data into training and testing sets with an 80-20 split, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Open', then create a new feature 'Date_ratio' as the ratio of 'Date' to 'Close', then detect outliers in 'Open' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf = pd.get_dummies(df, columns=['Open'], prefix=['Open'])\ndf['Date_ratio'] = df['Date'] / df['Close']\nQ1 = df['Open'].quantile(0.25)\nQ3 = df['Open'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Open'] < (Q1 - 1.5*IQR)) | (df['Open'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Revenue', then detect outliers in 'Region' using the IQR method, then clean text data in column 'Region' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['Region'].quantile(0.25)\nQ3 = df['Region'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Region'] < (Q1 - 1.5*IQR)) | (df['Region'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Region_clean'] = df['Region'].apply(clean)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then create a new feature 'temperature_ratio' as the ratio of 'temperature' to 'temperature', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['temperature_ratio'] = df['temperature'] / df['temperature']\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then clean text data in column 'LotArea' by removing punctuation and stopwords, then create a new feature 'SalePrice_ratio' as the ratio of 'SalePrice' to 'SalePrice', then compute TF-IDF features for column 'OverallQual' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['LotArea_clean'] = df['LotArea'].apply(clean)\ndf['SalePrice_ratio'] = df['SalePrice'] / df['SalePrice']\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['OverallQual'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then handle missing values in 'isFraud' by imputing with median, then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'time' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['isFraud'].fillna(df['isFraud'].median(), inplace=True)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['time'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then train a Random Forest Classifier to predict 'sales', then create a new feature 'customers_ratio' as the ratio of 'customers' to 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['customers_ratio'] = df['customers'] / df['sales']"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then handle missing values in 'date_time' by imputing with median, then clean text data in column 'rain_1h' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['date_time'].fillna(df['date_time'].median(), inplace=True)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['rain_1h_clean'] = df['rain_1h'].apply(clean)\nprint(df.describe())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features, then create a new feature 'heart_rate_ratio' as the ratio of 'heart_rate' to 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)\ndf['heart_rate_ratio'] = df['heart_rate'] / df['ecg_reading']"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'temperature', then plot a histogram of 'precipitation', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['precipitation'].hist()\nplt.xlabel('precipitation')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'Fare' and display top 10 words, then plot a histogram of 'Sex', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Fare'])\nprint(vect.get_feature_names_out())\ndf['Sex'].hist()\nplt.xlabel('Sex')\nplt.ylabel('Frequency')\nplt.show()\nprint(df.describe())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'sentiment' column using min-max scaling, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['genre'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['sentiment_scaled'] = (df['sentiment'] - df['sentiment'].min()) / (df['sentiment'].max() - df['sentiment'].min())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'SalePrice' and display top 10 words, then create a new feature 'YearBuilt_ratio' as the ratio of 'YearBuilt' to 'SalePrice', then one-hot encode the categorical column 'LotArea'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['SalePrice'])\nprint(vect.get_feature_names_out())\ndf['YearBuilt_ratio'] = df['YearBuilt'] / df['SalePrice']\ndf = pd.get_dummies(df, columns=['LotArea'], prefix=['LotArea'])"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then compute TF-IDF features for column 'post_id' and display top 10 words, then detect outliers in 'shares' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nprint(df.describe())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['post_id'])\nprint(vect.get_feature_names_out())\nQ1 = df['shares'].quantile(0.25)\nQ3 = df['shares'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['shares'] < (Q1 - 1.5*IQR)) | (df['shares'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then handle missing values in 'sentiment' by imputing with median, then display summary statistics of all numeric columns using df.describe(), then normalize the 'review' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['sentiment'].fillna(df['sentiment'].median(), inplace=True)\nprint(df.describe())\ndf['review_scaled'] = (df['review'] - df['review'].min()) / (df['review'].max() - df['review'].min())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then clean text data in column 'pm10' by removing punctuation and stopwords, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['so2'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['pm10_clean'] = df['pm10'].apply(clean)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Survived', then display feature importances from the Random Forest model, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'date_time', then detect outliers in 'rain_1h' using the IQR method, then handle missing values in 'snow_1h' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf = pd.get_dummies(df, columns=['date_time'], prefix=['date_time'])\nQ1 = df['rain_1h'].quantile(0.25)\nQ3 = df['rain_1h'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['rain_1h'] < (Q1 - 1.5*IQR)) | (df['rain_1h'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['snow_1h'].fillna(df['snow_1h'].median(), inplace=True)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'sensor_value', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sensor_value'])\ny = df['sensor_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then one-hot encode the categorical column 'date', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf = pd.get_dummies(df, columns=['date'], prefix=['date'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Age', then split the data into training and testing sets with an 80-20 split, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Age'].hist()\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then clean text data in column 'Close' by removing punctuation and stopwords, then handle missing values in 'Date' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Close_clean'] = df['Close'].apply(clean)\ndf['Date'].fillna(df['Date'].median(), inplace=True)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'traffic_volume', then detect outliers in 'temp' using the IQR method, then clean text data in column 'snow_1h' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['temp'].quantile(0.25)\nQ3 = df['temp'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['temp'] < (Q1 - 1.5*IQR)) | (df['temp'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['snow_1h_clean'] = df['snow_1h'].apply(clean)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'species', then train a Linear Regression model to predict 'species', then plot a histogram of 'sepal_length'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['sepal_length'].hist()\nplt.xlabel('sepal_length')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then handle missing values in 'o3' by imputing with median, then detect outliers in 'pm2_5' using the IQR method, then train a Linear Regression model to predict 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['o3'].fillna(df['o3'].median(), inplace=True)\nQ1 = df['pm2_5'].quantile(0.25)\nQ3 = df['pm2_5'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['pm2_5'] < (Q1 - 1.5*IQR)) | (df['pm2_5'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then normalize the 'country' column using min-max scaling, then create a new feature 'date_ratio' as the ratio of 'date' to 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ncorr = df.corr()\nprint(corr)\ndf['country_scaled'] = (df['country'] - df['country'].min()) / (df['country'].max() - df['country'].min())\ndf['date_ratio'] = df['date'] / df['confirmed']"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'quality' using the IQR method, then one-hot encode the categorical column 'quality', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nQ1 = df['quality'].quantile(0.25)\nQ3 = df['quality'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['quality'] < (Q1 - 1.5*IQR)) | (df['quality'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['quality'], prefix=['quality'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then normalize the 'date_time' column using min-max scaling, then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['date_time_scaled'] = (df['date_time'] - df['date_time'].min()) / (df['date_time'].max() - df['date_time'].min())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Close', then plot a histogram of 'Close', then create a new feature 'Low_ratio' as the ratio of 'Low' to 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['Close'].hist()\nplt.xlabel('Close')\nplt.ylabel('Frequency')\nplt.show()\ndf['Low_ratio'] = df['Low'] / df['Close']"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'post_id', then evaluate the model performance using RMSE and R\u00b2 score, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf = pd.get_dummies(df, columns=['post_id'], prefix=['post_id'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nprint(df.describe())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then one-hot encode the categorical column 'MonthlyCharges', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Churn'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf = pd.get_dummies(df, columns=['MonthlyCharges'], prefix=['MonthlyCharges'])\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then create a new feature 'oldbalanceOrg_ratio' as the ratio of 'oldbalanceOrg' to 'isFraud', then split the data into training and testing sets with an 80-20 split, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['oldbalanceOrg_ratio'] = df['oldbalanceOrg'] / df['isFraud']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['amount'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'sepal_width', then create a new feature 'petal_length_ratio' as the ratio of 'petal_length' to 'species', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf = pd.get_dummies(df, columns=['sepal_width'], prefix=['sepal_width'])\ndf['petal_length_ratio'] = df['petal_length'] / df['species']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then handle missing values in 'rain_1h' by imputing with median, then compute TF-IDF features for column 'date_time' and display top 10 words, then detect outliers in 'traffic_volume' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['rain_1h'].fillna(df['rain_1h'].median(), inplace=True)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date_time'])\nprint(vect.get_feature_names_out())\nQ1 = df['traffic_volume'].quantile(0.25)\nQ3 = df['traffic_volume'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['traffic_volume'] < (Q1 - 1.5*IQR)) | (df['traffic_volume'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'text', then plot a histogram of 'likes', then create a new feature 'shares_ratio' as the ratio of 'shares' to 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['likes'].hist()\nplt.xlabel('likes')\nplt.ylabel('Frequency')\nplt.show()\ndf['shares_ratio'] = df['shares'] / df['text']"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then plot a histogram of 'confirmed', then train a Random Forest Classifier to predict 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['confirmed'].hist()\nplt.xlabel('confirmed')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'confirmed', then display feature importances from the Random Forest model, then plot a histogram of 'recovered'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['recovered'].hist()\nplt.xlabel('recovered')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'sensor_value', then plot a histogram of 'timestamp'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['location'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['timestamp'].hist()\nplt.xlabel('timestamp')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then handle missing values in 'consumption' by imputing with median, then detect outliers in 'pressure' using the IQR method, then plot a histogram of 'humidity'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['consumption'].fillna(df['consumption'].median(), inplace=True)\nQ1 = df['pressure'].quantile(0.25)\nQ3 = df['pressure'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['pressure'] < (Q1 - 1.5*IQR)) | (df['pressure'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['humidity'].hist()\nplt.xlabel('humidity')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'quality' by removing punctuation and stopwords, then detect outliers in 'heart_rate' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['quality_clean'] = df['quality'].apply(clean)\nQ1 = df['heart_rate'].quantile(0.25)\nQ3 = df['heart_rate'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['heart_rate'] < (Q1 - 1.5*IQR)) | (df['heart_rate'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then normalize the 'temperature' column using min-max scaling, then display summary statistics of all numeric columns using df.describe(), then train a Random Forest Classifier to predict 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['temperature_scaled'] = (df['temperature'] - df['temperature'].min()) / (df['temperature'].max() - df['temperature'].min())\nprint(df.describe())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then detect outliers in 'wind_speed' using the IQR method, then normalize the 'humidity' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['wind_speed'].quantile(0.25)\nQ3 = df['wind_speed'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['wind_speed'] < (Q1 - 1.5*IQR)) | (df['wind_speed'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['humidity_scaled'] = (df['humidity'] - df['humidity'].min()) / (df['humidity'].max() - df['humidity'].min())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then normalize the 'precipitation' column using min-max scaling, then create a new feature 'wind_speed_ratio' as the ratio of 'wind_speed' to 'temperature', then train a Linear Regression model to predict 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['precipitation_scaled'] = (df['precipitation'] - df['precipitation'].min()) / (df['precipitation'].max() - df['precipitation'].min())\ndf['wind_speed_ratio'] = df['wind_speed'] / df['temperature']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Revenue', then display feature importances from the Random Forest model, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'isFraud', then compute TF-IDF features for column 'amount' and display top 10 words, then detect outliers in 'newbalanceOrig' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['amount'])\nprint(vect.get_feature_names_out())\nQ1 = df['newbalanceOrig'].quantile(0.25)\nQ3 = df['newbalanceOrig'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['newbalanceOrig'] < (Q1 - 1.5*IQR)) | (df['newbalanceOrig'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then clean text data in column 'user_id' by removing punctuation and stopwords, then display feature importances from the Random Forest model, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['user_id_clean'] = df['user_id'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then detect outliers in 'humidity' using the IQR method, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nQ1 = df['humidity'].quantile(0.25)\nQ3 = df['humidity'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['humidity'] < (Q1 - 1.5*IQR)) | (df['humidity'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Random Forest Classifier to predict 'total_amount', then handle missing values in 'product_id' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['product_id'].fillna(df['product_id'].median(), inplace=True)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then detect outliers in 'job' using the IQR method, then handle missing values in 'age' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['job'].quantile(0.25)\nQ3 = df['job'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['job'] < (Q1 - 1.5*IQR)) | (df['job'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['age'].fillna(df['age'].median(), inplace=True)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then compute TF-IDF features for column 'status' and display top 10 words, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nprint(df.describe())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['status'])\nprint(vect.get_feature_names_out())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['location'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'temp' using the IQR method, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['temp'].quantile(0.25)\nQ3 = df['temp'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['temp'] < (Q1 - 1.5*IQR)) | (df['temp'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sensor_value', then display feature importances from the Random Forest model, then plot a histogram of 'timestamp'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['timestamp'].hist()\nplt.xlabel('timestamp')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'arrival_delay', then handle missing values in 'arrival_delay' by imputing with median, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['arrival_delay'].fillna(df['arrival_delay'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then plot a histogram of 'humidity', then clean text data in column 'pressure' by removing punctuation and stopwords, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['humidity'].hist()\nplt.xlabel('humidity')\nplt.ylabel('Frequency')\nplt.show()\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['pressure_clean'] = df['pressure'].apply(clean)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then clean text data in column 'YearBuilt' by removing punctuation and stopwords, then plot a histogram of 'OverallQual', then detect outliers in 'YearBuilt' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['YearBuilt_clean'] = df['YearBuilt'].apply(clean)\ndf['OverallQual'].hist()\nplt.xlabel('OverallQual')\nplt.ylabel('Frequency')\nplt.show()\nQ1 = df['YearBuilt'].quantile(0.25)\nQ3 = df['YearBuilt'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['YearBuilt'] < (Q1 - 1.5*IQR)) | (df['YearBuilt'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'temperature', then display summary statistics of all numeric columns using df.describe(), then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then detect outliers in 'pm2_5' using the IQR method, then normalize the 'date' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nQ1 = df['pm2_5'].quantile(0.25)\nQ3 = df['pm2_5'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['pm2_5'] < (Q1 - 1.5*IQR)) | (df['pm2_5'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['date_scaled'] = (df['date'] - df['date'].min()) / (df['date'].max() - df['date'].min())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'time' by imputing with median, then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['time'].fillna(df['time'].median(), inplace=True)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then compute TF-IDF features for column 'review' and display top 10 words, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nprint(df.describe())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['review'])\nprint(vect.get_feature_names_out())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then compute TF-IDF features for column 'date' and display top 10 words, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date'])\nprint(vect.get_feature_names_out())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'confirmed', then compute TF-IDF features for column 'deaths' and display top 10 words, then plot a histogram of 'country'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['deaths'])\nprint(vect.get_feature_names_out())\ndf['country'].hist()\nplt.xlabel('country')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Revenue', then clean text data in column 'Date' by removing punctuation and stopwords, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Date_clean'] = df['Date'].apply(clean)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'quality' by imputing with median, then normalize the 'time' column using min-max scaling, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['quality'].fillna(df['quality'].median(), inplace=True)\ndf['time_scaled'] = (df['time'] - df['time'].min()) / (df['time'].max() - df['time'].min())\nprint(df.describe())"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'YearBuilt' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['YearBuilt'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then normalize the 'Fare' column using min-max scaling, then clean text data in column 'Age' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['Fare_scaled'] = (df['Fare'] - df['Fare'].min()) / (df['Fare'].max() - df['Fare'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Age_clean'] = df['Age'].apply(clean)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then plot a histogram of 'sales', then handle missing values in 'store' by imputing with median, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['sales'].hist()\nplt.xlabel('sales')\nplt.ylabel('Frequency')\nplt.show()\ndf['store'].fillna(df['store'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'sepal_width' and display top 10 words, then create a new feature 'petal_width_ratio' as the ratio of 'petal_width' to 'species', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sepal_width'])\nprint(vect.get_feature_names_out())\ndf['petal_width_ratio'] = df['petal_width'] / df['species']\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Random Forest Classifier to predict 'arrival_delay', then handle missing values in 'departure_delay' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['departure_delay'].fillna(df['departure_delay'].median(), inplace=True)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then plot a histogram of 'ContractType', then clean text data in column 'TotalCharges' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['ContractType'].hist()\nplt.xlabel('ContractType')\nplt.ylabel('Frequency')\nplt.show()\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['TotalCharges_clean'] = df['TotalCharges'].apply(clean)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then create a new feature 'departure_delay_ratio' as the ratio of 'departure_delay' to 'arrival_delay', then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'carrier' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['departure_delay_ratio'] = df['departure_delay'] / df['arrival_delay']\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['carrier_scaled'] = (df['carrier'] - df['carrier'].min()) / (df['carrier'].max() - df['carrier'].min())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then normalize the 'transaction_id' column using min-max scaling, then train a Random Forest Classifier to predict 'total_amount', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['transaction_id_scaled'] = (df['transaction_id'] - df['transaction_id'].min()) / (df['transaction_id'].max() - df['transaction_id'].min())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'quality', then perform time-series forecasting using ARIMA to predict the next 12 periods, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['quality'].hist()\nplt.xlabel('quality')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['quality'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then clean text data in column 'sepal_width' by removing punctuation and stopwords, then normalize the 'sepal_length' column using min-max scaling, then detect outliers in 'petal_width' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sepal_width_clean'] = df['sepal_width'].apply(clean)\ndf['sepal_length_scaled'] = (df['sepal_length'] - df['sepal_length'].min()) / (df['sepal_length'].max() - df['sepal_length'].min())\nQ1 = df['petal_width'].quantile(0.25)\nQ3 = df['petal_width'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['petal_width'] < (Q1 - 1.5*IQR)) | (df['petal_width'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'date', then clean text data in column 'date' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nprint(df.describe())\ndf['date'].hist()\nplt.xlabel('date')\nplt.ylabel('Frequency')\nplt.show()\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then normalize the 'user_id' column using min-max scaling, then perform K-Means clustering with k=3 on numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['user_id_scaled'] = (df['user_id'] - df['user_id'].min()) / (df['user_id'].max() - df['user_id'].min())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['user_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'sales', then clean text data in column 'customers' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['customers_clean'] = df['customers'].apply(clean)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'deaths'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['country'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['deaths'], prefix=['deaths'])"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'o3', then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'no2'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf = pd.get_dummies(df, columns=['o3'], prefix=['o3'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['no2'].hist()\nplt.xlabel('no2')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sentiment', then normalize the 'length' column using min-max scaling, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['length_scaled'] = (df['length'] - df['length'].min()) / (df['length'].max() - df['length'].min())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then handle missing values in 'sepal_width' by imputing with median, then perform K-Means clustering with k=3 on numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['sepal_width'].fillna(df['sepal_width'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sepal_width'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then plot a histogram of 'date_time', then compute TF-IDF features for column 'traffic_volume' and display top 10 words, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['date_time'].hist()\nplt.xlabel('date_time')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['traffic_volume'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split, then create a new feature 'UnitsSold_ratio' as the ratio of 'UnitsSold' to 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Revenue'])\ny = df['Revenue']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['UnitsSold_ratio'] = df['UnitsSold'] / df['Revenue']"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then create a new feature 'temp_ratio' as the ratio of 'temp' to 'traffic_volume', then detect outliers in 'snow_1h' using the IQR method, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['temp_ratio'] = df['temp'] / df['traffic_volume']\nQ1 = df['snow_1h'].quantile(0.25)\nQ3 = df['snow_1h'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['snow_1h'] < (Q1 - 1.5*IQR)) | (df['snow_1h'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then normalize the 'temperature' column using min-max scaling, then display feature importances from the Random Forest model, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['temperature_scaled'] = (df['temperature'] - df['temperature'].min()) / (df['temperature'].max() - df['temperature'].min())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split, then detect outliers in 'rain_1h' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['rain_1h'].quantile(0.25)\nQ3 = df['rain_1h'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['rain_1h'] < (Q1 - 1.5*IQR)) | (df['rain_1h'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'sensor_value' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nQ1 = df['sensor_value'].quantile(0.25)\nQ3 = df['sensor_value'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sensor_value'] < (Q1 - 1.5*IQR)) | (df['sensor_value'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Region'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'sensor_value' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nprint(df.describe())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['sensor_value'].quantile(0.25)\nQ3 = df['sensor_value'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sensor_value'] < (Q1 - 1.5*IQR)) | (df['sensor_value'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then display summary statistics of all numeric columns using df.describe(), then train a Random Forest Classifier to predict 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ncorr = df.corr()\nprint(corr)\nprint(df.describe())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'y', then display feature importances from the Random Forest model, then train a Random Forest Classifier to predict 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'sensor_value', then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf = pd.get_dummies(df, columns=['sensor_value'], prefix=['sensor_value'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'recovered', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['recovered'], prefix=['recovered'])\nprint(df.describe())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then create a new feature 'heart_rate_ratio' as the ratio of 'heart_rate' to 'ecg_reading', then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'heart_rate' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['heart_rate_ratio'] = df['heart_rate'] / df['ecg_reading']\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['heart_rate'].quantile(0.25)\nQ3 = df['heart_rate'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['heart_rate'] < (Q1 - 1.5*IQR)) | (df['heart_rate'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'ecg_reading', then detect outliers in 'quality' using the IQR method, then compute TF-IDF features for column 'ecg_reading' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['quality'].quantile(0.25)\nQ3 = df['quality'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['quality'] < (Q1 - 1.5*IQR)) | (df['quality'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['ecg_reading'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then create a new feature 'Low_ratio' as the ratio of 'Low' to 'Close', then display feature importances from the Random Forest model, then clean text data in column 'Open' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Low_ratio'] = df['Low'] / df['Close']\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Open_clean'] = df['Open'].apply(clean)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'Age' and display top 10 words, then handle missing values in 'Fare' by imputing with median, then train a Linear Regression model to predict 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Age'])\nprint(vect.get_feature_names_out())\ndf['Fare'].fillna(df['Fare'].median(), inplace=True)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Close', then normalize the 'Open' column using min-max scaling, then detect outliers in 'Date' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['Open_scaled'] = (df['Open'] - df['Open'].min()) / (df['Open'].max() - df['Open'].min())\nQ1 = df['Date'].quantile(0.25)\nQ3 = df['Date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Date'] < (Q1 - 1.5*IQR)) | (df['Date'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then normalize the 'isFraud' column using min-max scaling, then train a Linear Regression model to predict 'isFraud', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['isFraud_scaled'] = (df['isFraud'] - df['isFraud'].min()) / (df['isFraud'].max() - df['isFraud'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then clean text data in column 'user_id' by removing punctuation and stopwords, then split the data into training and testing sets with an 80-20 split, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['user_id_clean'] = df['user_id'].apply(clean)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['text'])\ny = df['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['post_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'Date' using the IQR method, then clean text data in column 'Product' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nprint(df.describe())\nQ1 = df['Date'].quantile(0.25)\nQ3 = df['Date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Date'] < (Q1 - 1.5*IQR)) | (df['Date'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Product_clean'] = df['Product'].apply(clean)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then create a new feature 'Survived_ratio' as the ratio of 'Survived' to 'Survived', then evaluate the model performance using RMSE and R\u00b2 score, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Survived_ratio'] = df['Survived'] / df['Survived']\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Fare'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'y', then normalize the 'y' column using min-max scaling, then one-hot encode the categorical column 'job'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['y_scaled'] = (df['y'] - df['y'].min()) / (df['y'].max() - df['y'].min())\ndf = pd.get_dummies(df, columns=['job'], prefix=['job'])"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then handle missing values in 'recovered' by imputing with median, then perform time-series forecasting using ARIMA to predict the next 12 periods, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['recovered'].fillna(df['recovered'].median(), inplace=True)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['confirmed'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then handle missing values in 'no2' by imputing with median, then plot a histogram of 'no2', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['no2'].fillna(df['no2'].median(), inplace=True)\ndf['no2'].hist()\nplt.xlabel('no2')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then clean text data in column 'humidity' by removing punctuation and stopwords, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['humidity_clean'] = df['humidity'].apply(clean)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model, then plot a histogram of 'likes'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['likes'].hist()\nplt.xlabel('likes')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then create a new feature 'sepal_length_ratio' as the ratio of 'sepal_length' to 'species', then train a Random Forest Classifier to predict 'species', then one-hot encode the categorical column 'sepal_length'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['sepal_length_ratio'] = df['sepal_length'] / df['species']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['sepal_length'], prefix=['sepal_length'])"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then train a Linear Regression model to predict 'sentiment', then handle missing values in 'rating' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['rating'].fillna(df['rating'].median(), inplace=True)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then plot a histogram of 'length', then split the data into training and testing sets with an 80-20 split, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['length'].hist()\nplt.xlabel('length')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'precipitation' by removing punctuation and stopwords, then calculate the correlation matrix for numeric features, then normalize the 'precipitation' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['precipitation_clean'] = df['precipitation'].apply(clean)\ncorr = df.corr()\nprint(corr)\ndf['precipitation_scaled'] = (df['precipitation'] - df['precipitation'].min()) / (df['precipitation'].max() - df['precipitation'].min())"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then create a new feature 'quantity_ratio' as the ratio of 'quantity' to 'total_amount', then clean text data in column 'product_id' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['quantity_ratio'] = df['quantity'] / df['total_amount']\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['product_id_clean'] = df['product_id'].apply(clean)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'shares_ratio' as the ratio of 'shares' to 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['text'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['shares_ratio'] = df['shares'] / df['text']"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'y', then plot a histogram of 'age'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['age'].hist()\nplt.xlabel('age')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'sepal_length' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sepal_length'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Random Forest Classifier to predict 'SalePrice', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nprint(df.describe())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['YearBuilt'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then handle missing values in 'day_of_week' by imputing with median, then plot a histogram of 'store', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['day_of_week'].fillna(df['day_of_week'].median(), inplace=True)\ndf['store'].hist()\nplt.xlabel('store')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split, then handle missing values in 'Neighborhood' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['LotArea'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['Neighborhood'].fillna(df['Neighborhood'].median(), inplace=True)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then create a new feature 'job_ratio' as the ratio of 'job' to 'y', then compute TF-IDF features for column 'marital' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['job_ratio'] = df['job'] / df['y']\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['marital'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'date_time' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nprint(df.describe())\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date_time'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then normalize the 'Open' column using min-max scaling, then train a Random Forest Classifier to predict 'Close', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Open_scaled'] = (df['Open'] - df['Open'].min()) / (df['Open'].max() - df['Open'].min())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then create a new feature 'traffic_volume_ratio' as the ratio of 'traffic_volume' to 'traffic_volume', then compute TF-IDF features for column 'temp' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['traffic_volume_ratio'] = df['traffic_volume'] / df['traffic_volume']\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['temp'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then plot a histogram of 'review', then compute TF-IDF features for column 'review' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ncorr = df.corr()\nprint(corr)\ndf['review'].hist()\nplt.xlabel('review')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['review'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Random Forest Classifier to predict 'SalePrice', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'patient_id' and display top 10 words, then display feature importances from the Random Forest model, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['patient_id'])\nprint(vect.get_feature_names_out())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['ecg_reading'])\ny = df['ecg_reading']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Survived', then perform K-Means clustering with k=3 on numeric features, then create a new feature 'Pclass_ratio' as the ratio of 'Pclass' to 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf = pd.get_dummies(df, columns=['Survived'], prefix=['Survived'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['Pclass_ratio'] = df['Pclass'] / df['Survived']"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nprint(df.describe())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['quality'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then compute TF-IDF features for column 'OverallQual' and display top 10 words, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['OverallQual'])\nprint(vect.get_feature_names_out())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'sentiment', then handle missing values in 'rating' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['rating'].fillna(df['rating'].median(), inplace=True)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'review' and display top 10 words, then plot a histogram of 'length', then normalize the 'genre' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['review'])\nprint(vect.get_feature_names_out())\ndf['length'].hist()\nplt.xlabel('length')\nplt.ylabel('Frequency')\nplt.show()\ndf['genre_scaled'] = (df['genre'] - df['genre'].min()) / (df['genre'].max() - df['genre'].min())"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then compute TF-IDF features for column 'Age' and display top 10 words, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Age'])\nprint(vect.get_feature_names_out())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'text' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['shares'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['text'].quantile(0.25)\nQ3 = df['text'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['text'] < (Q1 - 1.5*IQR)) | (df['text'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then compute TF-IDF features for column 'Sex' and display top 10 words, then plot a histogram of 'Pclass'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Sex'])\nprint(vect.get_feature_names_out())\ndf['Pclass'].hist()\nplt.xlabel('Pclass')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'text', then compute TF-IDF features for column 'shares' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['text'])\ny = df['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['shares'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Volume', then train a Random Forest Classifier to predict 'Close', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf = pd.get_dummies(df, columns=['Volume'], prefix=['Volume'])\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'pressure' and display top 10 words, then display summary statistics of all numeric columns using df.describe(), then normalize the 'pressure' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['pressure'])\nprint(vect.get_feature_names_out())\nprint(df.describe())\ndf['pressure_scaled'] = (df['pressure'] - df['pressure'].min()) / (df['pressure'].max() - df['pressure'].min())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'Revenue' using the IQR method, then compute TF-IDF features for column 'Date' and display top 10 words, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nQ1 = df['Revenue'].quantile(0.25)\nQ3 = df['Revenue'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Revenue'] < (Q1 - 1.5*IQR)) | (df['Revenue'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Date'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Revenue'])\ny = df['Revenue']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'date', then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf = pd.get_dummies(df, columns=['date'], prefix=['date'])\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'patient_id' and display top 10 words, then one-hot encode the categorical column 'heart_rate', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['patient_id'])\nprint(vect.get_feature_names_out())\ndf = pd.get_dummies(df, columns=['heart_rate'], prefix=['heart_rate'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then handle missing values in 'Date' by imputing with median, then evaluate the model performance using RMSE and R\u00b2 score, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Date'].fillna(df['Date'].median(), inplace=True)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'date'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['date'], prefix=['date'])"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then clean text data in column 'UnitsSold' by removing punctuation and stopwords, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['UnitsSold_clean'] = df['UnitsSold'].apply(clean)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then handle missing values in 'text' by imputing with median, then train a Linear Regression model to predict 'text', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['text'].fillna(df['text'].median(), inplace=True)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['text'])\ny = df['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'date', then train a Random Forest Classifier to predict 'consumption', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf = pd.get_dummies(df, columns=['date'], prefix=['date'])\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nprint(df.describe())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe(), then train a Random Forest Classifier to predict 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'total_amount', then plot a histogram of 'customer_id', then clean text data in column 'transaction_id' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['customer_id'].hist()\nplt.xlabel('customer_id')\nplt.ylabel('Frequency')\nplt.show()\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['transaction_id_clean'] = df['transaction_id'].apply(clean)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Pclass', then handle missing values in 'Survived' by imputing with median, then train a Linear Regression model to predict 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf = pd.get_dummies(df, columns=['Pclass'], prefix=['Pclass'])\ndf['Survived'].fillna(df['Survived'].median(), inplace=True)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Random Forest Classifier to predict 'Revenue', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'amount', then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf = pd.get_dummies(df, columns=['amount'], prefix=['amount'])\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['oldbalanceOrg'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'sensor_value', then display summary statistics of all numeric columns using df.describe(), then compute TF-IDF features for column 'sensor_value' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['sensor_value'].hist()\nplt.xlabel('sensor_value')\nplt.ylabel('Frequency')\nplt.show()\nprint(df.describe())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sensor_value'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Age', then train a Random Forest Classifier to predict 'Survived', then normalize the 'Age' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf = pd.get_dummies(df, columns=['Age'], prefix=['Age'])\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['Age_scaled'] = (df['Age'] - df['Age'].min()) / (df['Age'].max() - df['Age'].min())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then plot a histogram of 'country', then handle missing values in 'recovered' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ncorr = df.corr()\nprint(corr)\ndf['country'].hist()\nplt.xlabel('country')\nplt.ylabel('Frequency')\nplt.show()\ndf['recovered'].fillna(df['recovered'].median(), inplace=True)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'sales', then compute TF-IDF features for column 'open' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['open'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then create a new feature 'text_ratio' as the ratio of 'text' to 'text', then train a Random Forest Classifier to predict 'text', then clean text data in column 'likes' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['text_ratio'] = df['text'] / df['text']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['likes_clean'] = df['likes'].apply(clean)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then detect outliers in 'pm2_5' using the IQR method, then perform K-Means clustering with k=3 on numeric features, then handle missing values in 'date' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nQ1 = df['pm2_5'].quantile(0.25)\nQ3 = df['pm2_5'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['pm2_5'] < (Q1 - 1.5*IQR)) | (df['pm2_5'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['date'].fillna(df['date'].median(), inplace=True)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then create a new feature 'High_ratio' as the ratio of 'High' to 'Close', then clean text data in column 'Date' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['High_ratio'] = df['High'] / df['Close']\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Date_clean'] = df['Date'].apply(clean)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'consumption', then create a new feature 'date_ratio' as the ratio of 'date' to 'consumption', then detect outliers in 'consumption' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['date_ratio'] = df['date'] / df['consumption']\nQ1 = df['consumption'].quantile(0.25)\nQ3 = df['consumption'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['consumption'] < (Q1 - 1.5*IQR)) | (df['consumption'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'High' and display top 10 words, then perform K-Means clustering with k=3 on numeric features, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['High'])\nprint(vect.get_feature_names_out())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'distance'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['distance'].hist()\nplt.xlabel('distance')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'SalePrice', then display summary statistics of all numeric columns using df.describe(), then one-hot encode the categorical column 'LotArea'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())\ndf = pd.get_dummies(df, columns=['LotArea'], prefix=['LotArea'])"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then create a new feature 'patient_id_ratio' as the ratio of 'patient_id' to 'ecg_reading', then detect outliers in 'patient_id' using the IQR method, then compute TF-IDF features for column 'time' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['patient_id_ratio'] = df['patient_id'] / df['ecg_reading']\nQ1 = df['patient_id'].quantile(0.25)\nQ3 = df['patient_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['patient_id'] < (Q1 - 1.5*IQR)) | (df['patient_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['time'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then handle missing values in 'TotalCharges' by imputing with median, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'genre' by imputing with median, then plot a histogram of 'length'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nprint(df.describe())\ndf['genre'].fillna(df['genre'].median(), inplace=True)\ndf['length'].hist()\nplt.xlabel('length')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'consumption' and display top 10 words, then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['consumption'])\nprint(vect.get_feature_names_out())\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'Churn' and display top 10 words, then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'ContractType' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Churn'])\nprint(vect.get_feature_names_out())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['MonthlyCharges'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['ContractType'].quantile(0.25)\nQ3 = df['ContractType'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['ContractType'] < (Q1 - 1.5*IQR)) | (df['ContractType'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Revenue', then evaluate the model performance using RMSE and R\u00b2 score, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'timestamp' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe(), then train a Random Forest Classifier to predict 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['timestamp_clean'] = df['timestamp'].apply(clean)\nprint(df.describe())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'arrival_delay', then train a Random Forest Classifier to predict 'arrival_delay', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf = pd.get_dummies(df, columns=['arrival_delay'], prefix=['arrival_delay'])\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'patient_id', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['patient_id'], prefix=['patient_id'])\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['time'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Survived', then display feature importances from the Random Forest model, then detect outliers in 'Survived' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['Survived'].quantile(0.25)\nQ3 = df['Survived'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Survived'] < (Q1 - 1.5*IQR)) | (df['Survived'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'traffic_volume', then train a Random Forest Classifier to predict 'traffic_volume', then train a Linear Regression model to predict 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf = pd.get_dummies(df, columns=['traffic_volume'], prefix=['traffic_volume'])\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then detect outliers in 'sepal_length' using the IQR method, then plot a histogram of 'petal_length', then train a Linear Regression model to predict 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nQ1 = df['sepal_length'].quantile(0.25)\nQ3 = df['sepal_length'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sepal_length'] < (Q1 - 1.5*IQR)) | (df['sepal_length'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['petal_length'].hist()\nplt.xlabel('petal_length')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then handle missing values in 'marital' by imputing with median, then train a Random Forest Classifier to predict 'y', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['marital'].fillna(df['marital'].median(), inplace=True)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then handle missing values in 'deaths' by imputing with median, then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['deaths'].fillna(df['deaths'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'marital', then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf = pd.get_dummies(df, columns=['marital'], prefix=['marital'])\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'Open' using the IQR method, then compute TF-IDF features for column 'Volume' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nprint(df.describe())\nQ1 = df['Open'].quantile(0.25)\nQ3 = df['Open'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Open'] < (Q1 - 1.5*IQR)) | (df['Open'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Volume'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Revenue', then clean text data in column 'UnitsSold' by removing punctuation and stopwords, then plot a histogram of 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['UnitsSold_clean'] = df['UnitsSold'].apply(clean)\ndf['Revenue'].hist()\nplt.xlabel('Revenue')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display feature importances from the Random Forest model, then handle missing values in 'shares' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['shares'].fillna(df['shares'].median(), inplace=True)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then plot a histogram of 'marital', then compute TF-IDF features for column 'marital' and display top 10 words, then normalize the 'education' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['marital'].hist()\nplt.xlabel('marital')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['marital'])\nprint(vect.get_feature_names_out())\ndf['education_scaled'] = (df['education'] - df['education'].min()) / (df['education'].max() - df['education'].min())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then compute TF-IDF features for column 'deaths' and display top 10 words, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['deaths'])\nprint(vect.get_feature_names_out())\nprint(df.describe())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then detect outliers in 'carrier' using the IQR method, then handle missing values in 'departure_delay' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ncorr = df.corr()\nprint(corr)\nQ1 = df['carrier'].quantile(0.25)\nQ3 = df['carrier'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['carrier'] < (Q1 - 1.5*IQR)) | (df['carrier'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['departure_delay'].fillna(df['departure_delay'].median(), inplace=True)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Churn', then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then clean text data in column 'Age' by removing punctuation and stopwords, then plot a histogram of 'Fare'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nprint(df.describe())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Age_clean'] = df['Age'].apply(clean)\ndf['Fare'].hist()\nplt.xlabel('Fare')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then plot a histogram of 'flight', then train a Linear Regression model to predict 'arrival_delay', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['flight'].hist()\nplt.xlabel('flight')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'Churn', then create a new feature 'Churn_ratio' as the ratio of 'Churn' to 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['Churn_ratio'] = df['Churn'] / df['Churn']"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform K-Means clustering with k=3 on numeric features, then compute TF-IDF features for column 'sensor_value' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nprint(df.describe())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sensor_value'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'likes' and display top 10 words, then split the data into training and testing sets with an 80-20 split, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['likes'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['text'])\ny = df['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then clean text data in column 'store' by removing punctuation and stopwords, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nprint(df.describe())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['store_clean'] = df['store'].apply(clean)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'deaths' and display top 10 words, then plot a histogram of 'date', then one-hot encode the categorical column 'date'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['deaths'])\nprint(vect.get_feature_names_out())\ndf['date'].hist()\nplt.xlabel('date')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['date'], prefix=['date'])"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'ecg_reading' and display top 10 words, then perform K-Means clustering with k=3 on numeric features, then train a Linear Regression model to predict 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['ecg_reading'])\nprint(vect.get_feature_names_out())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then normalize the 'pressure' column using min-max scaling, then create a new feature 'pressure_ratio' as the ratio of 'pressure' to 'consumption', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['pressure_scaled'] = (df['pressure'] - df['pressure'].min()) / (df['pressure'].max() - df['pressure'].min())\ndf['pressure_ratio'] = df['pressure'] / df['consumption']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then handle missing values in 'sentiment' by imputing with median, then train a Linear Regression model to predict 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['review'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['sentiment'].fillna(df['sentiment'].median(), inplace=True)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then display feature importances from the Random Forest model, then detect outliers in 'isFraud' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nprint(df.describe())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['isFraud'].quantile(0.25)\nQ3 = df['isFraud'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['isFraud'] < (Q1 - 1.5*IQR)) | (df['isFraud'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'customer_id', then train a Random Forest Classifier to predict 'total_amount', then clean text data in column 'quantity' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf = pd.get_dummies(df, columns=['customer_id'], prefix=['customer_id'])\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['quantity_clean'] = df['quantity'].apply(clean)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then create a new feature 'education_ratio' as the ratio of 'education' to 'y', then handle missing values in 'job' by imputing with median, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['education_ratio'] = df['education'] / df['y']\ndf['job'].fillna(df['job'].median(), inplace=True)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'Region' and display top 10 words, then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Region'])\nprint(vect.get_feature_names_out())\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then plot a histogram of 'confirmed', then perform time-series forecasting using ARIMA to predict the next 12 periods, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['confirmed'].hist()\nplt.xlabel('confirmed')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nprint(df.describe())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then one-hot encode the categorical column 'o3', then detect outliers in 'pm2_5' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf = pd.get_dummies(df, columns=['o3'], prefix=['o3'])\nQ1 = df['pm2_5'].quantile(0.25)\nQ3 = df['pm2_5'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['pm2_5'] < (Q1 - 1.5*IQR)) | (df['pm2_5'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['age'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'ecg_reading', then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'patient_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['patient_id'], prefix=['patient_id'])"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'snow_1h' and display top 10 words, then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'date_time' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['snow_1h'])\nprint(vect.get_feature_names_out())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['date_time'].quantile(0.25)\nQ3 = df['date_time'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date_time'] < (Q1 - 1.5*IQR)) | (df['date_time'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then detect outliers in 'text' using the IQR method, then calculate the correlation matrix for numeric features, then one-hot encode the categorical column 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nQ1 = df['text'].quantile(0.25)\nQ3 = df['text'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['text'] < (Q1 - 1.5*IQR)) | (df['text'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ncorr = df.corr()\nprint(corr)\ndf = pd.get_dummies(df, columns=['text'], prefix=['text'])"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then create a new feature 'sensor_value_ratio' as the ratio of 'sensor_value' to 'sensor_value', then display feature importances from the Random Forest model, then one-hot encode the categorical column 'device_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['sensor_value_ratio'] = df['sensor_value'] / df['sensor_value']\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf = pd.get_dummies(df, columns=['device_id'], prefix=['device_id'])"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'sales_ratio' as the ratio of 'sales' to 'sales', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['open'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['sales_ratio'] = df['sales'] / df['sales']\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then clean text data in column 'Volume' by removing punctuation and stopwords, then normalize the 'Volume' column using min-max scaling, then plot a histogram of 'Open'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Volume_clean'] = df['Volume'].apply(clean)\ndf['Volume_scaled'] = (df['Volume'] - df['Volume'].min()) / (df['Volume'].max() - df['Volume'].min())\ndf['Open'].hist()\nplt.xlabel('Open')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'rating' and display top 10 words, then display feature importances from the Random Forest model, then handle missing values in 'genre' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['rating'])\nprint(vect.get_feature_names_out())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['genre'].fillna(df['genre'].median(), inplace=True)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Survived', then calculate the correlation matrix for numeric features, then one-hot encode the categorical column 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)\ndf = pd.get_dummies(df, columns=['Survived'], prefix=['Survived'])"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Close', then compute TF-IDF features for column 'Close' and display top 10 words, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Close'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Close'])\ny = df['Close']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then compute TF-IDF features for column 'Churn' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['tenure'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Churn'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then normalize the 'Survived' column using min-max scaling, then handle missing values in 'Survived' by imputing with median, then detect outliers in 'Survived' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Survived_scaled'] = (df['Survived'] - df['Survived'].min()) / (df['Survived'].max() - df['Survived'].min())\ndf['Survived'].fillna(df['Survived'].median(), inplace=True)\nQ1 = df['Survived'].quantile(0.25)\nQ3 = df['Survived'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Survived'] < (Q1 - 1.5*IQR)) | (df['Survived'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then handle missing values in 'Survived' by imputing with median, then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'Sex' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Survived'].fillna(df['Survived'].median(), inplace=True)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Survived'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['Sex'].quantile(0.25)\nQ3 = df['Sex'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Sex'] < (Q1 - 1.5*IQR)) | (df['Sex'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split, then handle missing values in 'customer_id' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['customer_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['customer_id'].fillna(df['customer_id'].median(), inplace=True)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'SalePrice', then split the data into training and testing sets with an 80-20 split, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'consumption', then compute TF-IDF features for column 'humidity' and display top 10 words, then plot a histogram of 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['humidity'])\nprint(vect.get_feature_names_out())\ndf['consumption'].hist()\nplt.xlabel('consumption')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then normalize the 'day_of_week' column using min-max scaling, then detect outliers in 'store' using the IQR method, then clean text data in column 'day_of_week' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['day_of_week_scaled'] = (df['day_of_week'] - df['day_of_week'].min()) / (df['day_of_week'].max() - df['day_of_week'].min())\nQ1 = df['store'].quantile(0.25)\nQ3 = df['store'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['store'] < (Q1 - 1.5*IQR)) | (df['store'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['day_of_week_clean'] = df['day_of_week'].apply(clean)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then clean text data in column 'date' by removing punctuation and stopwords, then handle missing values in 'deaths' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)\ndf['deaths'].fillna(df['deaths'].median(), inplace=True)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'Pclass' using the IQR method, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['Pclass'].quantile(0.25)\nQ3 = df['Pclass'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Pclass'] < (Q1 - 1.5*IQR)) | (df['Pclass'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nprint(df.describe())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then compute TF-IDF features for column 'likes' and display top 10 words, then train a Linear Regression model to predict 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nprint(df.describe())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['likes'])\nprint(vect.get_feature_names_out())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then normalize the 'Region' column using min-max scaling, then detect outliers in 'Product' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['Region_scaled'] = (df['Region'] - df['Region'].min()) / (df['Region'].max() - df['Region'].min())\nQ1 = df['Product'].quantile(0.25)\nQ3 = df['Product'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Product'] < (Q1 - 1.5*IQR)) | (df['Product'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then evaluate the model performance using RMSE and R\u00b2 score, then clean text data in column 'date' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'Revenue' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe(), then create a new feature 'Date_ratio' as the ratio of 'Date' to 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Revenue_clean'] = df['Revenue'].apply(clean)\nprint(df.describe())\ndf['Date_ratio'] = df['Date'] / df['Revenue']"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then clean text data in column 'deaths' by removing punctuation and stopwords, then perform time-series forecasting using ARIMA to predict the next 12 periods, then plot a histogram of 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['deaths_clean'] = df['deaths'].apply(clean)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['confirmed'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['confirmed'].hist()\nplt.xlabel('confirmed')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'temperature' and display top 10 words, then normalize the 'date' column using min-max scaling, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['temperature'])\nprint(vect.get_feature_names_out())\ndf['date_scaled'] = (df['date'] - df['date'].min()) / (df['date'].max() - df['date'].min())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then normalize the 'device_id' column using min-max scaling, then plot a histogram of 'status'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ncorr = df.corr()\nprint(corr)\ndf['device_id_scaled'] = (df['device_id'] - df['device_id'].min()) / (df['device_id'].max() - df['device_id'].min())\ndf['status'].hist()\nplt.xlabel('status')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'no2', then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf = pd.get_dummies(df, columns=['no2'], prefix=['no2'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then detect outliers in 'review' using the IQR method, then calculate the correlation matrix for numeric features, then handle missing values in 'length' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nQ1 = df['review'].quantile(0.25)\nQ3 = df['review'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['review'] < (Q1 - 1.5*IQR)) | (df['review'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ncorr = df.corr()\nprint(corr)\ndf['length'].fillna(df['length'].median(), inplace=True)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then normalize the 'country' column using min-max scaling, then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['country_scaled'] = (df['country'] - df['country'].min()) / (df['country'].max() - df['country'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then handle missing values in 'arrival_delay' by imputing with median, then clean text data in column 'departure_delay' by removing punctuation and stopwords, then train a Linear Regression model to predict 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['arrival_delay'].fillna(df['arrival_delay'].median(), inplace=True)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['departure_delay_clean'] = df['departure_delay'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then normalize the 'post_id' column using min-max scaling, then train a Linear Regression model to predict 'text', then compute TF-IDF features for column 'user_id' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['post_id_scaled'] = (df['post_id'] - df['post_id'].min()) / (df['post_id'].max() - df['post_id'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['user_id'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'y', then detect outliers in 'y' using the IQR method, then handle missing values in 'education' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf = pd.get_dummies(df, columns=['y'], prefix=['y'])\nQ1 = df['y'].quantile(0.25)\nQ3 = df['y'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['y'] < (Q1 - 1.5*IQR)) | (df['y'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['education'].fillna(df['education'].median(), inplace=True)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then normalize the 'pm2_5' column using min-max scaling, then handle missing values in 'o3' by imputing with median, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['pm2_5_scaled'] = (df['pm2_5'] - df['pm2_5'].min()) / (df['pm2_5'].max() - df['pm2_5'].min())\ndf['o3'].fillna(df['o3'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then perform K-Means clustering with k=3 on numeric features, then compute TF-IDF features for column 'traffic_volume' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['traffic_volume'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Low'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then display summary statistics of all numeric columns using df.describe(), then normalize the 'quantity' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nprint(df.describe())\ndf['quantity_scaled'] = (df['quantity'] - df['quantity'].min()) / (df['quantity'].max() - df['quantity'].min())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'pm2_5', then evaluate the model performance using RMSE and R\u00b2 score, then create a new feature 'no2_ratio' as the ratio of 'no2' to 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf = pd.get_dummies(df, columns=['pm2_5'], prefix=['pm2_5'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['no2_ratio'] = df['no2'] / df['pm2_5']"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then clean text data in column 'job' by removing punctuation and stopwords, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['job_clean'] = df['job'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then one-hot encode the categorical column 'TotalCharges', then clean text data in column 'TotalCharges' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf = pd.get_dummies(df, columns=['TotalCharges'], prefix=['TotalCharges'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['TotalCharges_clean'] = df['TotalCharges'].apply(clean)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['oldbalanceOrg'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then train a Random Forest Classifier to predict 'Revenue', then handle missing values in 'Region' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['Region'].fillna(df['Region'].median(), inplace=True)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then create a new feature 'tenure_ratio' as the ratio of 'tenure' to 'Churn', then train a Random Forest Classifier to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nprint(df.describe())\ndf['tenure_ratio'] = df['tenure'] / df['Churn']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then clean text data in column 'sales' by removing punctuation and stopwords, then display feature importances from the Random Forest model, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sales_clean'] = df['sales'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'so2' and display top 10 words, then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['so2'])\nprint(vect.get_feature_names_out())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['o3'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then clean text data in column 'user_id' by removing punctuation and stopwords, then train a Linear Regression model to predict 'text', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['user_id_clean'] = df['user_id'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'timestamp', then detect outliers in 'sensor_value' using the IQR method, then one-hot encode the categorical column 'status'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['timestamp'].hist()\nplt.xlabel('timestamp')\nplt.ylabel('Frequency')\nplt.show()\nQ1 = df['sensor_value'].quantile(0.25)\nQ3 = df['sensor_value'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sensor_value'] < (Q1 - 1.5*IQR)) | (df['sensor_value'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['status'], prefix=['status'])"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'recovered', then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf = pd.get_dummies(df, columns=['recovered'], prefix=['recovered'])\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['country'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Churn', then compute TF-IDF features for column 'Churn' and display top 10 words, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Churn'])\nprint(vect.get_feature_names_out())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'Product' by removing punctuation and stopwords, then train a Random Forest Classifier to predict 'Revenue', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Product_clean'] = df['Product'].apply(clean)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then detect outliers in 'pressure' using the IQR method, then train a Linear Regression model to predict 'consumption', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nQ1 = df['pressure'].quantile(0.25)\nQ3 = df['pressure'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['pressure'] < (Q1 - 1.5*IQR)) | (df['pressure'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'temperature', then evaluate the model performance using RMSE and R\u00b2 score, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then detect outliers in 'Pclass' using the IQR method, then clean text data in column 'Fare' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['Pclass'].quantile(0.25)\nQ3 = df['Pclass'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Pclass'] < (Q1 - 1.5*IQR)) | (df['Pclass'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Fare_clean'] = df['Fare'].apply(clean)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Revenue', then plot a histogram of 'Region', then create a new feature 'UnitsSold_ratio' as the ratio of 'UnitsSold' to 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['Region'].hist()\nplt.xlabel('Region')\nplt.ylabel('Frequency')\nplt.show()\ndf['UnitsSold_ratio'] = df['UnitsSold'] / df['Revenue']"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then handle missing values in 'likes' by imputing with median, then one-hot encode the categorical column 'shares'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['likes'].fillna(df['likes'].median(), inplace=True)\ndf = pd.get_dummies(df, columns=['shares'], prefix=['shares'])"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split, then one-hot encode the categorical column 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['amount'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf = pd.get_dummies(df, columns=['isFraud'], prefix=['isFraud'])"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features, then plot a histogram of 'pressure'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)\ndf['pressure'].hist()\nplt.xlabel('pressure')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then one-hot encode the categorical column 'job', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ncorr = df.corr()\nprint(corr)\ndf = pd.get_dummies(df, columns=['job'], prefix=['job'])\nprint(df.describe())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)\nprint(df.describe())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then create a new feature 'Churn_ratio' as the ratio of 'Churn' to 'Churn', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nprint(df.describe())\ndf['Churn_ratio'] = df['Churn'] / df['Churn']\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then detect outliers in 'open' using the IQR method, then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nQ1 = df['open'].quantile(0.25)\nQ3 = df['open'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['open'] < (Q1 - 1.5*IQR)) | (df['open'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['customers'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then detect outliers in 'genre' using the IQR method, then handle missing values in 'length' by imputing with median, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nQ1 = df['genre'].quantile(0.25)\nQ3 = df['genre'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['genre'] < (Q1 - 1.5*IQR)) | (df['genre'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['length'].fillna(df['length'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'genre' and display top 10 words, then clean text data in column 'length' by removing punctuation and stopwords, then normalize the 'rating' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['genre'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['length_clean'] = df['length'].apply(clean)\ndf['rating_scaled'] = (df['rating'] - df['rating'].min()) / (df['rating'].max() - df['rating'].min())"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'total_amount', then create a new feature 'customer_id_ratio' as the ratio of 'customer_id' to 'total_amount', then handle missing values in 'transaction_id' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['customer_id_ratio'] = df['customer_id'] / df['total_amount']\ndf['transaction_id'].fillna(df['transaction_id'].median(), inplace=True)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then handle missing values in 'humidity' by imputing with median, then one-hot encode the categorical column 'consumption', then plot a histogram of 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['humidity'].fillna(df['humidity'].median(), inplace=True)\ndf = pd.get_dummies(df, columns=['consumption'], prefix=['consumption'])\ndf['temperature'].hist()\nplt.xlabel('temperature')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then compute TF-IDF features for column 'shares' and display top 10 words, then create a new feature 'user_id_ratio' as the ratio of 'user_id' to 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['text'])\ny = df['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['shares'])\nprint(vect.get_feature_names_out())\ndf['user_id_ratio'] = df['user_id'] / df['text']"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then normalize the 'tenure' column using min-max scaling, then create a new feature 'tenure_ratio' as the ratio of 'tenure' to 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['tenure_scaled'] = (df['tenure'] - df['tenure'].min()) / (df['tenure'].max() - df['tenure'].min())\ndf['tenure_ratio'] = df['tenure'] / df['Churn']"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then normalize the 'newbalanceOrig' column using min-max scaling, then compute TF-IDF features for column 'newbalanceOrig' and display top 10 words, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['newbalanceOrig_scaled'] = (df['newbalanceOrig'] - df['newbalanceOrig'].min()) / (df['newbalanceOrig'].max() - df['newbalanceOrig'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['newbalanceOrig'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then normalize the 'user_id' column using min-max scaling, then display summary statistics of all numeric columns using df.describe(), then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['user_id_scaled'] = (df['user_id'] - df['user_id'].min()) / (df['user_id'].max() - df['user_id'].min())\nprint(df.describe())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['likes'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Fare', then one-hot encode the categorical column 'Sex', then handle missing values in 'Pclass' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Fare'].hist()\nplt.xlabel('Fare')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['Sex'], prefix=['Sex'])\ndf['Pclass'].fillna(df['Pclass'].median(), inplace=True)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then display feature importances from the Random Forest model, then compute TF-IDF features for column 'tenure' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nprint(df.describe())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['tenure'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then create a new feature 'temp_ratio' as the ratio of 'temp' to 'traffic_volume', then detect outliers in 'snow_1h' using the IQR method, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['temp_ratio'] = df['temp'] / df['traffic_volume']\nQ1 = df['snow_1h'].quantile(0.25)\nQ3 = df['snow_1h'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['snow_1h'] < (Q1 - 1.5*IQR)) | (df['snow_1h'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then plot a histogram of 'recovered', then clean text data in column 'recovered' by removing punctuation and stopwords, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['recovered'].hist()\nplt.xlabel('recovered')\nplt.ylabel('Frequency')\nplt.show()\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['recovered_clean'] = df['recovered'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform time-series forecasting using ARIMA to predict the next 12 periods, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nprint(df.describe())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['likes'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then clean text data in column 'user_id' by removing punctuation and stopwords, then one-hot encode the categorical column 'likes'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['text'])\ny = df['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['user_id_clean'] = df['user_id'].apply(clean)\ndf = pd.get_dummies(df, columns=['likes'], prefix=['likes'])"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then detect outliers in 'OverallQual' using the IQR method, then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nQ1 = df['OverallQual'].quantile(0.25)\nQ3 = df['OverallQual'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['OverallQual'] < (Q1 - 1.5*IQR)) | (df['OverallQual'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'Churn', then compute TF-IDF features for column 'ContractType' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['ContractType'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then create a new feature 'age_ratio' as the ratio of 'age' to 'y', then evaluate the model performance using RMSE and R\u00b2 score, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['age_ratio'] = df['age'] / df['y']\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then calculate the correlation matrix for numeric features, then normalize the 'OverallQual' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['OverallQual'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ncorr = df.corr()\nprint(corr)\ndf['OverallQual_scaled'] = (df['OverallQual'] - df['OverallQual'].min()) / (df['OverallQual'].max() - df['OverallQual'].min())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'sentiment', then create a new feature 'length_ratio' as the ratio of 'length' to 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['length_ratio'] = df['length'] / df['sentiment']"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features, then plot a histogram of 'transaction_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)\ndf['transaction_id'].hist()\nplt.xlabel('transaction_id')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then handle missing values in 'review' by imputing with median, then clean text data in column 'genre' by removing punctuation and stopwords, then normalize the 'rating' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['review'].fillna(df['review'].median(), inplace=True)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['genre_clean'] = df['genre'].apply(clean)\ndf['rating_scaled'] = (df['rating'] - df['rating'].min()) / (df['rating'].max() - df['rating'].min())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features, then normalize the 'Date' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)\ndf['Date_scaled'] = (df['Date'] - df['Date'].min()) / (df['Date'].max() - df['Date'].min())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then normalize the 'Region' column using min-max scaling, then one-hot encode the categorical column 'UnitsSold', then detect outliers in 'Date' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Region_scaled'] = (df['Region'] - df['Region'].min()) / (df['Region'].max() - df['Region'].min())\ndf = pd.get_dummies(df, columns=['UnitsSold'], prefix=['UnitsSold'])\nQ1 = df['Date'].quantile(0.25)\nQ3 = df['Date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Date'] < (Q1 - 1.5*IQR)) | (df['Date'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then handle missing values in 'amount' by imputing with median, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['isFraud'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['amount'].fillna(df['amount'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['temperature'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then plot a histogram of 'tenure', then clean text data in column 'tenure' by removing punctuation and stopwords, then handle missing values in 'TotalCharges' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['tenure'].hist()\nplt.xlabel('tenure')\nplt.ylabel('Frequency')\nplt.show()\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['tenure_clean'] = df['tenure'].apply(clean)\ndf['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'SalePrice', then clean text data in column 'Neighborhood' by removing punctuation and stopwords, then plot a histogram of 'Neighborhood'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Neighborhood_clean'] = df['Neighborhood'].apply(clean)\ndf['Neighborhood'].hist()\nplt.xlabel('Neighborhood')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'y' and display top 10 words, then train a Random Forest Classifier to predict 'y', then plot a histogram of 'marital'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['y'])\nprint(vect.get_feature_names_out())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['marital'].hist()\nplt.xlabel('marital')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then plot a histogram of 'product_id', then handle missing values in 'product_id' by imputing with median, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['product_id'].hist()\nplt.xlabel('product_id')\nplt.ylabel('Frequency')\nplt.show()\ndf['product_id'].fillna(df['product_id'].median(), inplace=True)\nprint(df.describe())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['text'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'YearBuilt' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['YearBuilt_scaled'] = (df['YearBuilt'] - df['YearBuilt'].min()) / (df['YearBuilt'].max() - df['YearBuilt'].min())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'species', then train a Random Forest Classifier to predict 'species', then normalize the 'species' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['species_scaled'] = (df['species'] - df['species'].min()) / (df['species'].max() - df['species'].min())"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'status', then train a Linear Regression model to predict 'sensor_value', then one-hot encode the categorical column 'status'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['status'].hist()\nplt.xlabel('status')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['status'], prefix=['status'])"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then plot a histogram of 'petal_length', then one-hot encode the categorical column 'sepal_length'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['petal_length'].hist()\nplt.xlabel('petal_length')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['sepal_length'], prefix=['sepal_length'])"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'UnitsSold' by imputing with median, then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'UnitsSold'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['UnitsSold'].fillna(df['UnitsSold'].median(), inplace=True)\nprint(df.describe())\ndf['UnitsSold'].hist()\nplt.xlabel('UnitsSold')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then normalize the 'humidity' column using min-max scaling, then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'date'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['humidity_scaled'] = (df['humidity'] - df['humidity'].min()) / (df['humidity'].max() - df['humidity'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['date'].hist()\nplt.xlabel('date')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Volume', then normalize the 'Low' column using min-max scaling, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf = pd.get_dummies(df, columns=['Volume'], prefix=['Volume'])\ndf['Low_scaled'] = (df['Low'] - df['Low'].min()) / (df['Low'].max() - df['Low'].min())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then create a new feature 'consumption_ratio' as the ratio of 'consumption' to 'consumption', then compute TF-IDF features for column 'humidity' and display top 10 words, then train a Linear Regression model to predict 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['consumption_ratio'] = df['consumption'] / df['consumption']\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['humidity'])\nprint(vect.get_feature_names_out())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then detect outliers in 'Open' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'Close' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nQ1 = df['Open'].quantile(0.25)\nQ3 = df['Open'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Open'] < (Q1 - 1.5*IQR)) | (df['Open'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['Close_scaled'] = (df['Close'] - df['Close'].min()) / (df['Close'].max() - df['Close'].min())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then handle missing values in 'precipitation' by imputing with median, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['precipitation'].fillna(df['precipitation'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then create a new feature 'amount_ratio' as the ratio of 'amount' to 'isFraud', then train a Linear Regression model to predict 'isFraud', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['amount_ratio'] = df['amount'] / df['isFraud']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then detect outliers in 'Churn' using the IQR method, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['Churn'].quantile(0.25)\nQ3 = df['Churn'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Churn'] < (Q1 - 1.5*IQR)) | (df['Churn'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then create a new feature 'petal_width_ratio' as the ratio of 'petal_width' to 'species', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nprint(df.describe())\ndf['petal_width_ratio'] = df['petal_width'] / df['species']\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'o3', then detect outliers in 'pm10' using the IQR method, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf = pd.get_dummies(df, columns=['o3'], prefix=['o3'])\nQ1 = df['pm10'].quantile(0.25)\nQ3 = df['pm10'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['pm10'] < (Q1 - 1.5*IQR)) | (df['pm10'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'traffic_volume', then clean text data in column 'traffic_volume' by removing punctuation and stopwords, then train a Random Forest Classifier to predict 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['traffic_volume_clean'] = df['traffic_volume'].apply(clean)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'location' and display top 10 words, then detect outliers in 'device_id' using the IQR method, then one-hot encode the categorical column 'location'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['location'])\nprint(vect.get_feature_names_out())\nQ1 = df['device_id'].quantile(0.25)\nQ3 = df['device_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['device_id'] < (Q1 - 1.5*IQR)) | (df['device_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['location'], prefix=['location'])"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then plot a histogram of 'rating', then detect outliers in 'sentiment' using the IQR method, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['rating'].hist()\nplt.xlabel('rating')\nplt.ylabel('Frequency')\nplt.show()\nQ1 = df['sentiment'].quantile(0.25)\nQ3 = df['sentiment'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sentiment'] < (Q1 - 1.5*IQR)) | (df['sentiment'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'Survived', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then split the data into training and testing sets with an 80-20 split, then compute TF-IDF features for column 'patient_id' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['ecg_reading'])\ny = df['ecg_reading']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['patient_id'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then clean text data in column 'job' by removing punctuation and stopwords, then compute TF-IDF features for column 'education' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['job_clean'] = df['job'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['education'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then detect outliers in 'arrival_delay' using the IQR method, then one-hot encode the categorical column 'distance', then plot a histogram of 'flight'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nQ1 = df['arrival_delay'].quantile(0.25)\nQ3 = df['arrival_delay'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['arrival_delay'] < (Q1 - 1.5*IQR)) | (df['arrival_delay'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['distance'], prefix=['distance'])\ndf['flight'].hist()\nplt.xlabel('flight')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then detect outliers in 'carrier' using the IQR method, then normalize the 'departure_delay' column using min-max scaling, then train a Random Forest Classifier to predict 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nQ1 = df['carrier'].quantile(0.25)\nQ3 = df['carrier'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['carrier'] < (Q1 - 1.5*IQR)) | (df['carrier'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['departure_delay_scaled'] = (df['departure_delay'] - df['departure_delay'].min()) / (df['departure_delay'].max() - df['departure_delay'].min())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'consumption', then display summary statistics of all numeric columns using df.describe(), then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nprint(df.describe())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then create a new feature 'precipitation_ratio' as the ratio of 'precipitation' to 'temperature', then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['precipitation_ratio'] = df['precipitation'] / df['temperature']\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sensor_value', then plot a histogram of 'location', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['location'].hist()\nplt.xlabel('location')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['status'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['age'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then create a new feature 'ContractType_ratio' as the ratio of 'ContractType' to 'Churn', then clean text data in column 'tenure' by removing punctuation and stopwords, then detect outliers in 'MonthlyCharges' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['ContractType_ratio'] = df['ContractType'] / df['Churn']\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['tenure_clean'] = df['tenure'].apply(clean)\nQ1 = df['MonthlyCharges'].quantile(0.25)\nQ3 = df['MonthlyCharges'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['MonthlyCharges'] < (Q1 - 1.5*IQR)) | (df['MonthlyCharges'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then create a new feature 'Date_ratio' as the ratio of 'Date' to 'Close', then display feature importances from the Random Forest model, then plot a histogram of 'Date'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Date_ratio'] = df['Date'] / df['Close']\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['Date'].hist()\nplt.xlabel('Date')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then clean text data in column 'temperature' by removing punctuation and stopwords, then create a new feature 'pressure_ratio' as the ratio of 'pressure' to 'consumption', then plot a histogram of 'date'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['temperature_clean'] = df['temperature'].apply(clean)\ndf['pressure_ratio'] = df['pressure'] / df['consumption']\ndf['date'].hist()\nplt.xlabel('date')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'store', then detect outliers in 'day_of_week' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['store'].hist()\nplt.xlabel('store')\nplt.ylabel('Frequency')\nplt.show()\nQ1 = df['day_of_week'].quantile(0.25)\nQ3 = df['day_of_week'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['day_of_week'] < (Q1 - 1.5*IQR)) | (df['day_of_week'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then handle missing values in 'shares' by imputing with median, then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['shares'].fillna(df['shares'].median(), inplace=True)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then clean text data in column 'time' by removing punctuation and stopwords, then train a Linear Regression model to predict 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['time_clean'] = df['time'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then normalize the 'review' column using min-max scaling, then split the data into training and testing sets with an 80-20 split, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['review_scaled'] = (df['review'] - df['review'].min()) / (df['review'].max() - df['review'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then clean text data in column 'quantity' by removing punctuation and stopwords, then plot a histogram of 'total_amount', then train a Random Forest Classifier to predict 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['quantity_clean'] = df['quantity'].apply(clean)\ndf['total_amount'].hist()\nplt.xlabel('total_amount')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then evaluate the model performance using RMSE and R\u00b2 score, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Open'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then normalize the 'newbalanceOrig' column using min-max scaling, then compute TF-IDF features for column 'time' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nprint(df.describe())\ndf['newbalanceOrig_scaled'] = (df['newbalanceOrig'] - df['newbalanceOrig'].min()) / (df['newbalanceOrig'].max() - df['newbalanceOrig'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['time'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then one-hot encode the categorical column 'Open', then create a new feature 'High_ratio' as the ratio of 'High' to 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ncorr = df.corr()\nprint(corr)\ndf = pd.get_dummies(df, columns=['Open'], prefix=['Open'])\ndf['High_ratio'] = df['High'] / df['Close']"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then normalize the 'no2' column using min-max scaling, then create a new feature 'so2_ratio' as the ratio of 'so2' to 'pm2_5', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['no2_scaled'] = (df['no2'] - df['no2'].min()) / (df['no2'].max() - df['no2'].min())\ndf['so2_ratio'] = df['so2'] / df['pm2_5']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then create a new feature 'wind_speed_ratio' as the ratio of 'wind_speed' to 'temperature', then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'date' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['wind_speed_ratio'] = df['wind_speed'] / df['temperature']\nprint(df.describe())\ndf['date'].fillna(df['date'].median(), inplace=True)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Fare', then split the data into training and testing sets with an 80-20 split, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Fare'].hist()\nplt.xlabel('Fare')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then detect outliers in 'MonthlyCharges' using the IQR method, then normalize the 'MonthlyCharges' column using min-max scaling, then train a Linear Regression model to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nQ1 = df['MonthlyCharges'].quantile(0.25)\nQ3 = df['MonthlyCharges'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['MonthlyCharges'] < (Q1 - 1.5*IQR)) | (df['MonthlyCharges'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['MonthlyCharges_scaled'] = (df['MonthlyCharges'] - df['MonthlyCharges'].min()) / (df['MonthlyCharges'].max() - df['MonthlyCharges'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Random Forest Classifier to predict 'sensor_value', then train a Linear Regression model to predict 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then plot a histogram of 'sentiment', then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['sentiment'].hist()\nplt.xlabel('sentiment')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sentiment'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then clean text data in column 'open' by removing punctuation and stopwords, then calculate the correlation matrix for numeric features, then create a new feature 'day_of_week_ratio' as the ratio of 'day_of_week' to 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['open_clean'] = df['open'].apply(clean)\ncorr = df.corr()\nprint(corr)\ndf['day_of_week_ratio'] = df['day_of_week'] / df['sales']"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'post_id', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nprint(df.describe())\ndf['post_id'].hist()\nplt.xlabel('post_id')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then detect outliers in 'review' using the IQR method, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ncorr = df.corr()\nprint(corr)\nQ1 = df['review'].quantile(0.25)\nQ3 = df['review'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['review'] < (Q1 - 1.5*IQR)) | (df['review'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then one-hot encode the categorical column 'tenure', then normalize the 'TotalCharges' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nprint(df.describe())\ndf = pd.get_dummies(df, columns=['tenure'], prefix=['tenure'])\ndf['TotalCharges_scaled'] = (df['TotalCharges'] - df['TotalCharges'].min()) / (df['TotalCharges'].max() - df['TotalCharges'].min())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then one-hot encode the categorical column 'length', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ncorr = df.corr()\nprint(corr)\ndf = pd.get_dummies(df, columns=['length'], prefix=['length'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then normalize the 'Fare' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Fare_scaled'] = (df['Fare'] - df['Fare'].min()) / (df['Fare'].max() - df['Fare'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then clean text data in column 'date_time' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date_time'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_time_clean'] = df['date_time'].apply(clean)\nprint(df.describe())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'ecg_reading', then calculate the correlation matrix for numeric features, then normalize the 'heart_rate' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)\ndf['heart_rate_scaled'] = (df['heart_rate'] - df['heart_rate'].min()) / (df['heart_rate'].max() - df['heart_rate'].min())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then handle missing values in 'consumption' by imputing with median, then split the data into training and testing sets with an 80-20 split, then create a new feature 'temperature_ratio' as the ratio of 'temperature' to 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['consumption'].fillna(df['consumption'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['temperature_ratio'] = df['temperature'] / df['consumption']"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then evaluate the model performance using RMSE and R\u00b2 score, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Churn'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then create a new feature 'recovered_ratio' as the ratio of 'recovered' to 'confirmed', then split the data into training and testing sets with an 80-20 split, then detect outliers in 'date' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['recovered_ratio'] = df['recovered'] / df['confirmed']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then plot a histogram of 'Age', then one-hot encode the categorical column 'Fare'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ncorr = df.corr()\nprint(corr)\ndf['Age'].hist()\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['Fare'], prefix=['Fare'])"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then detect outliers in 'arrival_delay' using the IQR method, then display summary statistics of all numeric columns using df.describe(), then one-hot encode the categorical column 'carrier'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nQ1 = df['arrival_delay'].quantile(0.25)\nQ3 = df['arrival_delay'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['arrival_delay'] < (Q1 - 1.5*IQR)) | (df['arrival_delay'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nprint(df.describe())\ndf = pd.get_dummies(df, columns=['carrier'], prefix=['carrier'])"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then create a new feature 'traffic_volume_ratio' as the ratio of 'traffic_volume' to 'traffic_volume', then perform K-Means clustering with k=3 on numeric features, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['traffic_volume_ratio'] = df['traffic_volume'] / df['traffic_volume']\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Product', then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf = pd.get_dummies(df, columns=['Product'], prefix=['Product'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then split the data into training and testing sets with an 80-20 split, then clean text data in column 'sentiment' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nprint(df.describe())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sentiment_clean'] = df['sentiment'].apply(clean)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'total_amount', then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then plot a histogram of 'patient_id', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['patient_id'].hist()\nplt.xlabel('patient_id')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then create a new feature 'transaction_id_ratio' as the ratio of 'transaction_id' to 'total_amount', then display summary statistics of all numeric columns using df.describe(), then train a Random Forest Classifier to predict 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['transaction_id_ratio'] = df['transaction_id'] / df['total_amount']\nprint(df.describe())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then detect outliers in 'customer_id' using the IQR method, then display summary statistics of all numeric columns using df.describe(), then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nQ1 = df['customer_id'].quantile(0.25)\nQ3 = df['customer_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['customer_id'] < (Q1 - 1.5*IQR)) | (df['customer_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nprint(df.describe())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'temperature', then one-hot encode the categorical column 'humidity', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['humidity'], prefix=['humidity'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then create a new feature 'product_id_ratio' as the ratio of 'product_id' to 'total_amount', then clean text data in column 'total_amount' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['product_id_ratio'] = df['product_id'] / df['total_amount']\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['total_amount_clean'] = df['total_amount'].apply(clean)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'consumption', then handle missing values in 'date' by imputing with median, then create a new feature 'date_ratio' as the ratio of 'date' to 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['date'].fillna(df['date'].median(), inplace=True)\ndf['date_ratio'] = df['date'] / df['consumption']"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then clean text data in column 'date_time' by removing punctuation and stopwords, then handle missing values in 'traffic_volume' by imputing with median, then train a Linear Regression model to predict 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_time_clean'] = df['date_time'].apply(clean)\ndf['traffic_volume'].fillna(df['traffic_volume'].median(), inplace=True)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'High', then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf = pd.get_dummies(df, columns=['High'], prefix=['High'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'Product' using the IQR method, then compute TF-IDF features for column 'Date' and display top 10 words, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nQ1 = df['Product'].quantile(0.25)\nQ3 = df['Product'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Product'] < (Q1 - 1.5*IQR)) | (df['Product'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Date'])\nprint(vect.get_feature_names_out())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then detect outliers in 'departure_delay' using the IQR method, then display summary statistics of all numeric columns using df.describe(), then compute TF-IDF features for column 'distance' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nQ1 = df['departure_delay'].quantile(0.25)\nQ3 = df['departure_delay'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['departure_delay'] < (Q1 - 1.5*IQR)) | (df['departure_delay'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nprint(df.describe())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['distance'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then handle missing values in 'user_id' by imputing with median, then train a Linear Regression model to predict 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['user_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['user_id'].fillna(df['user_id'].median(), inplace=True)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'y', then evaluate the model performance using RMSE and R\u00b2 score, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['y'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then detect outliers in 'oldbalanceOrg' using the IQR method, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['oldbalanceOrg'].quantile(0.25)\nQ3 = df['oldbalanceOrg'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['oldbalanceOrg'] < (Q1 - 1.5*IQR)) | (df['oldbalanceOrg'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then split the data into training and testing sets with an 80-20 split, then handle missing values in 'species' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['species'].fillna(df['species'].median(), inplace=True)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then create a new feature 'country_ratio' as the ratio of 'country' to 'confirmed', then normalize the 'recovered' column using min-max scaling, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['country_ratio'] = df['country'] / df['confirmed']\ndf['recovered_scaled'] = (df['recovered'] - df['recovered'].min()) / (df['recovered'].max() - df['recovered'].min())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'pm2_5', then train a Random Forest Classifier to predict 'pm2_5', then normalize the 'date' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['date_scaled'] = (df['date'] - df['date'].min()) / (df['date'].max() - df['date'].min())"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then detect outliers in 'Survived' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nQ1 = df['Survived'].quantile(0.25)\nQ3 = df['Survived'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Survived'] < (Q1 - 1.5*IQR)) | (df['Survived'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Survived'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'time' using the IQR method, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['time'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['time'].quantile(0.25)\nQ3 = df['time'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['time'] < (Q1 - 1.5*IQR)) | (df['time'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then create a new feature 'MonthlyCharges_ratio' as the ratio of 'MonthlyCharges' to 'Churn', then plot a histogram of 'Churn', then train a Linear Regression model to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['MonthlyCharges_ratio'] = df['MonthlyCharges'] / df['Churn']\ndf['Churn'].hist()\nplt.xlabel('Churn')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'YearBuilt', then train a Random Forest Classifier to predict 'SalePrice'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['YearBuilt'], prefix=['YearBuilt'])\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Close', then train a Random Forest Classifier to predict 'Close', then normalize the 'Low' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['Low_scaled'] = (df['Low'] - df['Low'].min()) / (df['Low'].max() - df['Low'].min())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'y' and display top 10 words, then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['y'])\nprint(vect.get_feature_names_out())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['y'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then plot a histogram of 'product_id', then handle missing values in 'total_amount' by imputing with median, then compute TF-IDF features for column 'customer_id' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['product_id'].hist()\nplt.xlabel('product_id')\nplt.ylabel('Frequency')\nplt.show()\ndf['total_amount'].fillna(df['total_amount'].median(), inplace=True)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['customer_id'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then compute TF-IDF features for column 'quantity' and display top 10 words, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['quantity'])\nprint(vect.get_feature_names_out())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['customer_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'patient_id' by imputing with median, then plot a histogram of 'time', then compute TF-IDF features for column 'quality' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['patient_id'].fillna(df['patient_id'].median(), inplace=True)\ndf['time'].hist()\nplt.xlabel('time')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['quality'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['temperature'], prefix=['temperature'])"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'amount' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['oldbalanceOrg'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['amount'].quantile(0.25)\nQ3 = df['amount'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['amount'] < (Q1 - 1.5*IQR)) | (df['amount'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then create a new feature 'consumption_ratio' as the ratio of 'consumption' to 'consumption', then normalize the 'consumption' column using min-max scaling, then clean text data in column 'temperature' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['consumption_ratio'] = df['consumption'] / df['consumption']\ndf['consumption_scaled'] = (df['consumption'] - df['consumption'].min()) / (df['consumption'].max() - df['consumption'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['temperature_clean'] = df['temperature'].apply(clean)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then plot a histogram of 'recovered', then create a new feature 'country_ratio' as the ratio of 'country' to 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['recovered'].hist()\nplt.xlabel('recovered')\nplt.ylabel('Frequency')\nplt.show()\ndf['country_ratio'] = df['country'] / df['confirmed']"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then clean text data in column 'sepal_length' by removing punctuation and stopwords, then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'petal_length' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sepal_length_clean'] = df['sepal_length'].apply(clean)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sepal_width'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['petal_length'].quantile(0.25)\nQ3 = df['petal_length'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['petal_length'] < (Q1 - 1.5*IQR)) | (df['petal_length'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'traffic_volume', then compute TF-IDF features for column 'snow_1h' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['snow_1h'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then display feature importances from the Random Forest model, then create a new feature 'product_id_ratio' as the ratio of 'product_id' to 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nprint(df.describe())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['product_id_ratio'] = df['product_id'] / df['total_amount']"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'arrival_delay', then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf = pd.get_dummies(df, columns=['arrival_delay'], prefix=['arrival_delay'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display feature importances from the Random Forest model, then create a new feature 'humidity_ratio' as the ratio of 'humidity' to 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['humidity_ratio'] = df['humidity'] / df['consumption']"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then create a new feature 'Sex_ratio' as the ratio of 'Sex' to 'Survived', then train a Random Forest Classifier to predict 'Survived', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Sex_ratio'] = df['Sex'] / df['Survived']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'arrival_delay', then split the data into training and testing sets with an 80-20 split, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf = pd.get_dummies(df, columns=['arrival_delay'], prefix=['arrival_delay'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'status' using the IQR method, then train a Random Forest Classifier to predict 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nprint(df.describe())\nQ1 = df['status'].quantile(0.25)\nQ3 = df['status'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['status'] < (Q1 - 1.5*IQR)) | (df['status'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then handle missing values in 'MonthlyCharges' by imputing with median, then perform K-Means clustering with k=3 on numeric features, then train a Linear Regression model to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['MonthlyCharges'].fillna(df['MonthlyCharges'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Neighborhood', then calculate the correlation matrix for numeric features, then create a new feature 'LotArea_ratio' as the ratio of 'LotArea' to 'SalePrice'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['Neighborhood'].hist()\nplt.xlabel('Neighborhood')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)\ndf['LotArea_ratio'] = df['LotArea'] / df['SalePrice']"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Random Forest Classifier to predict 'sales', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nprint(df.describe())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'confirmed', then plot a histogram of 'confirmed', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf = pd.get_dummies(df, columns=['confirmed'], prefix=['confirmed'])\ndf['confirmed'].hist()\nplt.xlabel('confirmed')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then clean text data in column 'pm2_5' by removing punctuation and stopwords, then evaluate the model performance using RMSE and R\u00b2 score, then create a new feature 'pm2_5_ratio' as the ratio of 'pm2_5' to 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['pm2_5_clean'] = df['pm2_5'].apply(clean)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['pm2_5_ratio'] = df['pm2_5'] / df['pm2_5']"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'date', then detect outliers in 'deaths' using the IQR method, then normalize the 'deaths' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf = pd.get_dummies(df, columns=['date'], prefix=['date'])\nQ1 = df['deaths'].quantile(0.25)\nQ3 = df['deaths'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['deaths'] < (Q1 - 1.5*IQR)) | (df['deaths'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['deaths_scaled'] = (df['deaths'] - df['deaths'].min()) / (df['deaths'].max() - df['deaths'].min())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'sentiment', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then calculate the correlation matrix for numeric features, then clean text data in column 'pm2_5' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ncorr = df.corr()\nprint(corr)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['pm2_5_clean'] = df['pm2_5'].apply(clean)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then handle missing values in 'time' by imputing with median, then clean text data in column 'time' by removing punctuation and stopwords, then train a Linear Regression model to predict 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['time'].fillna(df['time'].median(), inplace=True)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['time_clean'] = df['time'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then normalize the 'Date' column using min-max scaling, then display summary statistics of all numeric columns using df.describe(), then create a new feature 'Revenue_ratio' as the ratio of 'Revenue' to 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Date_scaled'] = (df['Date'] - df['Date'].min()) / (df['Date'].max() - df['Date'].min())\nprint(df.describe())\ndf['Revenue_ratio'] = df['Revenue'] / df['Revenue']"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then clean text data in column 'Fare' by removing punctuation and stopwords, then compute TF-IDF features for column 'Age' and display top 10 words, then train a Linear Regression model to predict 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Fare_clean'] = df['Fare'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Age'])\nprint(vect.get_feature_names_out())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'so2'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['so2'], prefix=['so2'])"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'ContractType', then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'TotalCharges' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf = pd.get_dummies(df, columns=['ContractType'], prefix=['ContractType'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['TotalCharges'].quantile(0.25)\nQ3 = df['TotalCharges'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['TotalCharges'] < (Q1 - 1.5*IQR)) | (df['TotalCharges'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then clean text data in column 'Churn' by removing punctuation and stopwords, then handle missing values in 'ContractType' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Churn_clean'] = df['Churn'].apply(clean)\ndf['ContractType'].fillna(df['ContractType'].median(), inplace=True)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then clean text data in column 'Fare' by removing punctuation and stopwords, then compute TF-IDF features for column 'Pclass' and display top 10 words, then train a Random Forest Classifier to predict 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Fare_clean'] = df['Fare'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Pclass'])\nprint(vect.get_feature_names_out())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'Region' and display top 10 words, then normalize the 'Revenue' column using min-max scaling, then plot a histogram of 'Region'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Region'])\nprint(vect.get_feature_names_out())\ndf['Revenue_scaled'] = (df['Revenue'] - df['Revenue'].min()) / (df['Revenue'].max() - df['Revenue'].min())\ndf['Region'].hist()\nplt.xlabel('Region')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then clean text data in column 'education' by removing punctuation and stopwords, then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['education_clean'] = df['education'].apply(clean)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'arrival_delay', then perform time-series forecasting using ARIMA to predict the next 12 periods, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['distance'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nprint(df.describe())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'timestamp', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['timestamp'], prefix=['timestamp'])\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['status'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then normalize the 'Product' column using min-max scaling, then detect outliers in 'Region' using the IQR method, then handle missing values in 'UnitsSold' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Product_scaled'] = (df['Product'] - df['Product'].min()) / (df['Product'].max() - df['Product'].min())\nQ1 = df['Region'].quantile(0.25)\nQ3 = df['Region'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Region'] < (Q1 - 1.5*IQR)) | (df['Region'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['UnitsSold'].fillna(df['UnitsSold'].median(), inplace=True)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'SalePrice', then clean text data in column 'SalePrice' by removing punctuation and stopwords, then compute TF-IDF features for column 'LotArea' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['SalePrice_clean'] = df['SalePrice'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['LotArea'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'UnitsSold', then train a Random Forest Classifier to predict 'Revenue', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf = pd.get_dummies(df, columns=['UnitsSold'], prefix=['UnitsSold'])\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then evaluate the model performance using RMSE and R\u00b2 score, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['status'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['ContractType'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sales', then calculate the correlation matrix for numeric features, then detect outliers in 'open' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)\nQ1 = df['open'].quantile(0.25)\nQ3 = df['open'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['open'] < (Q1 - 1.5*IQR)) | (df['open'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then clean text data in column 'MonthlyCharges' by removing punctuation and stopwords, then calculate the correlation matrix for numeric features, then normalize the 'Churn' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['MonthlyCharges_clean'] = df['MonthlyCharges'].apply(clean)\ncorr = df.corr()\nprint(corr)\ndf['Churn_scaled'] = (df['Churn'] - df['Churn'].min()) / (df['Churn'].max() - df['Churn'].min())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then create a new feature 'y_ratio' as the ratio of 'y' to 'y', then normalize the 'marital' column using min-max scaling, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['y_ratio'] = df['y'] / df['y']\ndf['marital_scaled'] = (df['marital'] - df['marital'].min()) / (df['marital'].max() - df['marital'].min())\nprint(df.describe())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'so2' and display top 10 words, then detect outliers in 'so2' using the IQR method, then create a new feature 'so2_ratio' as the ratio of 'so2' to 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['so2'])\nprint(vect.get_feature_names_out())\nQ1 = df['so2'].quantile(0.25)\nQ3 = df['so2'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['so2'] < (Q1 - 1.5*IQR)) | (df['so2'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['so2_ratio'] = df['so2'] / df['pm2_5']"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Fare', then train a Random Forest Classifier to predict 'Survived', then train a Linear Regression model to predict 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf = pd.get_dummies(df, columns=['Fare'], prefix=['Fare'])\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nprint(df.describe())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'heart_rate' using the IQR method, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['heart_rate'].quantile(0.25)\nQ3 = df['heart_rate'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['heart_rate'] < (Q1 - 1.5*IQR)) | (df['heart_rate'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sentiment', then normalize the 'length' column using min-max scaling, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['length_scaled'] = (df['length'] - df['length'].min()) / (df['length'].max() - df['length'].min())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then create a new feature 'pm2_5_ratio' as the ratio of 'pm2_5' to 'pm2_5', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['pm2_5_ratio'] = df['pm2_5'] / df['pm2_5']\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'arrival_delay', then perform K-Means clustering with k=3 on numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['arrival_delay'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sentiment', then calculate the correlation matrix for numeric features, then train a Random Forest Classifier to predict 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then normalize the 'shares' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['shares_scaled'] = (df['shares'] - df['shares'].min()) / (df['shares'].max() - df['shares'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then handle missing values in 'snow_1h' by imputing with median, then compute TF-IDF features for column 'temp' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['snow_1h'].fillna(df['snow_1h'].median(), inplace=True)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['temp'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'Neighborhood' and display top 10 words, then train a Linear Regression model to predict 'SalePrice', then create a new feature 'SalePrice_ratio' as the ratio of 'SalePrice' to 'SalePrice'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Neighborhood'])\nprint(vect.get_feature_names_out())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['SalePrice_ratio'] = df['SalePrice'] / df['SalePrice']"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'date' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())\ndf['date'].fillna(df['date'].median(), inplace=True)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'day_of_week' and display top 10 words, then normalize the 'open' column using min-max scaling, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['day_of_week'])\nprint(vect.get_feature_names_out())\ndf['open_scaled'] = (df['open'] - df['open'].min()) / (df['open'].max() - df['open'].min())\nprint(df.describe())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then create a new feature 'Product_ratio' as the ratio of 'Product' to 'Revenue', then compute TF-IDF features for column 'UnitsSold' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['Product_ratio'] = df['Product'] / df['Revenue']\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['UnitsSold'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'text', then create a new feature 'text_ratio' as the ratio of 'text' to 'text', then plot a histogram of 'likes'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['text_ratio'] = df['text'] / df['text']\ndf['likes'].hist()\nplt.xlabel('likes')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then handle missing values in 'rain_1h' by imputing with median, then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['rain_1h'].fillna(df['rain_1h'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then handle missing values in 'Survived' by imputing with median, then evaluate the model performance using RMSE and R\u00b2 score, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Survived'].fillna(df['Survived'].median(), inplace=True)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then detect outliers in 'open' using the IQR method, then handle missing values in 'open' by imputing with median, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nQ1 = df['open'].quantile(0.25)\nQ3 = df['open'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['open'] < (Q1 - 1.5*IQR)) | (df['open'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['open'].fillna(df['open'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Linear Regression model to predict 'y', then normalize the 'education' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nprint(df.describe())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['education_scaled'] = (df['education'] - df['education'].min()) / (df['education'].max() - df['education'].min())"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Survived', then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'Age_ratio' as the ratio of 'Age' to 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Fare'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['Age_ratio'] = df['Age'] / df['Survived']"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'precipitation' column using min-max scaling, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['precipitation_scaled'] = (df['precipitation'] - df['precipitation'].min()) / (df['precipitation'].max() - df['precipitation'].min())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display feature importances from the Random Forest model, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['temperature'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Random Forest Classifier to predict 'Revenue', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Region'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nprint(df.describe())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then detect outliers in 'time' using the IQR method, then plot a histogram of 'patient_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['ecg_reading'])\ny = df['ecg_reading']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['time'].quantile(0.25)\nQ3 = df['time'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['time'] < (Q1 - 1.5*IQR)) | (df['time'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['patient_id'].hist()\nplt.xlabel('patient_id')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Random Forest Classifier to predict 'Close', then normalize the 'Volume' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nprint(df.describe())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['Volume_scaled'] = (df['Volume'] - df['Volume'].min()) / (df['Volume'].max() - df['Volume'].min())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then normalize the 'arrival_delay' column using min-max scaling, then handle missing values in 'distance' by imputing with median, then one-hot encode the categorical column 'carrier'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['arrival_delay_scaled'] = (df['arrival_delay'] - df['arrival_delay'].min()) / (df['arrival_delay'].max() - df['arrival_delay'].min())\ndf['distance'].fillna(df['distance'].median(), inplace=True)\ndf = pd.get_dummies(df, columns=['carrier'], prefix=['carrier'])"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'Age' and display top 10 words, then split the data into training and testing sets with an 80-20 split, then create a new feature 'Pclass_ratio' as the ratio of 'Pclass' to 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Age'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['Pclass_ratio'] = df['Pclass'] / df['Survived']"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then normalize the 'Pclass' column using min-max scaling, then clean text data in column 'Fare' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['Pclass_scaled'] = (df['Pclass'] - df['Pclass'].min()) / (df['Pclass'].max() - df['Pclass'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Fare_clean'] = df['Fare'].apply(clean)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then handle missing values in 'review' by imputing with median, then one-hot encode the categorical column 'sentiment', then detect outliers in 'review' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['review'].fillna(df['review'].median(), inplace=True)\ndf = pd.get_dummies(df, columns=['sentiment'], prefix=['sentiment'])\nQ1 = df['review'].quantile(0.25)\nQ3 = df['review'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['review'] < (Q1 - 1.5*IQR)) | (df['review'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then plot a histogram of 'open', then create a new feature 'customers_ratio' as the ratio of 'customers' to 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ncorr = df.corr()\nprint(corr)\ndf['open'].hist()\nplt.xlabel('open')\nplt.ylabel('Frequency')\nplt.show()\ndf['customers_ratio'] = df['customers'] / df['sales']"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then normalize the 'y' column using min-max scaling, then create a new feature 'marital_ratio' as the ratio of 'marital' to 'y', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['y_scaled'] = (df['y'] - df['y'].min()) / (df['y'].max() - df['y'].min())\ndf['marital_ratio'] = df['marital'] / df['y']\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features, then train a Random Forest Classifier to predict 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'text', then display feature importances from the Random Forest model, then detect outliers in 'user_id' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['user_id'].quantile(0.25)\nQ3 = df['user_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['user_id'] < (Q1 - 1.5*IQR)) | (df['user_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'likes', then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf = pd.get_dummies(df, columns=['likes'], prefix=['likes'])\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then normalize the 'Churn' column using min-max scaling, then clean text data in column 'tenure' by removing punctuation and stopwords, then plot a histogram of 'MonthlyCharges'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['Churn_scaled'] = (df['Churn'] - df['Churn'].min()) / (df['Churn'].max() - df['Churn'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['tenure_clean'] = df['tenure'].apply(clean)\ndf['MonthlyCharges'].hist()\nplt.xlabel('MonthlyCharges')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then create a new feature 'date_ratio' as the ratio of 'date' to 'pm2_5', then normalize the 'pm2_5' column using min-max scaling, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['date_ratio'] = df['date'] / df['pm2_5']\ndf['pm2_5_scaled'] = (df['pm2_5'] - df['pm2_5'].min()) / (df['pm2_5'].max() - df['pm2_5'].min())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['pm2_5'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then normalize the 'deaths' column using min-max scaling, then handle missing values in 'recovered' by imputing with median, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['deaths_scaled'] = (df['deaths'] - df['deaths'].min()) / (df['deaths'].max() - df['deaths'].min())\ndf['recovered'].fillna(df['recovered'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'departure_delay', then split the data into training and testing sets with an 80-20 split, then clean text data in column 'distance' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf = pd.get_dummies(df, columns=['departure_delay'], prefix=['departure_delay'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['distance_clean'] = df['distance'].apply(clean)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'text', then create a new feature 'user_id_ratio' as the ratio of 'user_id' to 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['user_id_ratio'] = df['user_id'] / df['text']"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then handle missing values in 'customers' by imputing with median, then normalize the 'store' column using min-max scaling, then clean text data in column 'open' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['customers'].fillna(df['customers'].median(), inplace=True)\ndf['store_scaled'] = (df['store'] - df['store'].min()) / (df['store'].max() - df['store'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['open_clean'] = df['open'].apply(clean)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then normalize the 'o3' column using min-max scaling, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ncorr = df.corr()\nprint(corr)\ndf['o3_scaled'] = (df['o3'] - df['o3'].min()) / (df['o3'].max() - df['o3'].min())\nprint(df.describe())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then create a new feature 'petal_length_ratio' as the ratio of 'petal_length' to 'species', then compute TF-IDF features for column 'species' and display top 10 words, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['petal_length_ratio'] = df['petal_length'] / df['species']\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['species'])\nprint(vect.get_feature_names_out())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then handle missing values in 'YearBuilt' by imputing with median, then one-hot encode the categorical column 'LotArea', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['YearBuilt'].fillna(df['YearBuilt'].median(), inplace=True)\ndf = pd.get_dummies(df, columns=['LotArea'], prefix=['LotArea'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Churn', then compute TF-IDF features for column 'TotalCharges' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['TotalCharges'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features, then clean text data in column 'length' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['length_clean'] = df['length'].apply(clean)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['timestamp'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then create a new feature 'sentiment_ratio' as the ratio of 'sentiment' to 'sentiment', then calculate the correlation matrix for numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['sentiment_ratio'] = df['sentiment'] / df['sentiment']\ncorr = df.corr()\nprint(corr)\nprint(df.describe())"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then create a new feature 'total_amount_ratio' as the ratio of 'total_amount' to 'total_amount', then compute TF-IDF features for column 'transaction_id' and display top 10 words, then one-hot encode the categorical column 'transaction_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['total_amount_ratio'] = df['total_amount'] / df['total_amount']\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['transaction_id'])\nprint(vect.get_feature_names_out())\ndf = pd.get_dummies(df, columns=['transaction_id'], prefix=['transaction_id'])"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then create a new feature 'age_ratio' as the ratio of 'age' to 'y', then train a Random Forest Classifier to predict 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ncorr = df.corr()\nprint(corr)\ndf['age_ratio'] = df['age'] / df['y']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'MonthlyCharges', then detect outliers in 'tenure' using the IQR method, then plot a histogram of 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf = pd.get_dummies(df, columns=['MonthlyCharges'], prefix=['MonthlyCharges'])\nQ1 = df['tenure'].quantile(0.25)\nQ3 = df['tenure'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['tenure'] < (Q1 - 1.5*IQR)) | (df['tenure'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['Churn'].hist()\nplt.xlabel('Churn')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Survived', then handle missing values in 'Survived' by imputing with median, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Survived'].hist()\nplt.xlabel('Survived')\nplt.ylabel('Frequency')\nplt.show()\ndf['Survived'].fillna(df['Survived'].median(), inplace=True)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then normalize the 'Fare' column using min-max scaling, then plot a histogram of 'Age', then compute TF-IDF features for column 'Sex' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Fare_scaled'] = (df['Fare'] - df['Fare'].min()) / (df['Fare'].max() - df['Fare'].min())\ndf['Age'].hist()\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Sex'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then create a new feature 'device_id_ratio' as the ratio of 'device_id' to 'sensor_value', then detect outliers in 'location' using the IQR method, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['device_id_ratio'] = df['device_id'] / df['sensor_value']\nQ1 = df['location'].quantile(0.25)\nQ3 = df['location'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['location'] < (Q1 - 1.5*IQR)) | (df['location'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nprint(df.describe())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then normalize the 'departure_delay' column using min-max scaling, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nprint(df.describe())\ndf['departure_delay_scaled'] = (df['departure_delay'] - df['departure_delay'].min()) / (df['departure_delay'].max() - df['departure_delay'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then display summary statistics of all numeric columns using df.describe(), then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nprint(df.describe())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'consumption', then one-hot encode the categorical column 'humidity', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['humidity'], prefix=['humidity'])\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then perform K-Means clustering with k=3 on numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['ecg_reading'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then handle missing values in 'LotArea' by imputing with median, then split the data into training and testing sets with an 80-20 split, then compute TF-IDF features for column 'SalePrice' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['LotArea'].fillna(df['LotArea'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['SalePrice'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'arrival_delay', then display feature importances from the Random Forest model, then create a new feature 'arrival_delay_ratio' as the ratio of 'arrival_delay' to 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['arrival_delay_ratio'] = df['arrival_delay'] / df['arrival_delay']"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then one-hot encode the categorical column 'Survived', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nprint(df.describe())\ndf = pd.get_dummies(df, columns=['Survived'], prefix=['Survived'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then compute TF-IDF features for column 'y' and display top 10 words, then one-hot encode the categorical column 'education'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['y'])\nprint(vect.get_feature_names_out())\ndf = pd.get_dummies(df, columns=['education'], prefix=['education'])"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display summary statistics of all numeric columns using df.describe(), then create a new feature 'recovered_ratio' as the ratio of 'recovered' to 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['country'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nprint(df.describe())\ndf['recovered_ratio'] = df['recovered'] / df['confirmed']"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'text', then train a Linear Regression model to predict 'text', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['post_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then handle missing values in 'TotalCharges' by imputing with median, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sensor_value', then compute TF-IDF features for column 'device_id' and display top 10 words, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['device_id'])\nprint(vect.get_feature_names_out())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['location'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then plot a histogram of 'sepal_length', then detect outliers in 'sepal_length' using the IQR method, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['sepal_length'].hist()\nplt.xlabel('sepal_length')\nplt.ylabel('Frequency')\nplt.show()\nQ1 = df['sepal_length'].quantile(0.25)\nQ3 = df['sepal_length'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sepal_length'] < (Q1 - 1.5*IQR)) | (df['sepal_length'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then handle missing values in 'flight' by imputing with median, then detect outliers in 'departure_delay' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['flight'].fillna(df['flight'].median(), inplace=True)\nQ1 = df['departure_delay'].quantile(0.25)\nQ3 = df['departure_delay'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['departure_delay'] < (Q1 - 1.5*IQR)) | (df['departure_delay'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then plot a histogram of 'date_time', then display summary statistics of all numeric columns using df.describe(), then train a Linear Regression model to predict 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['date_time'].hist()\nplt.xlabel('date_time')\nplt.ylabel('Frequency')\nplt.show()\nprint(df.describe())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'species', then plot a histogram of 'species', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['species'].hist()\nplt.xlabel('species')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'sepal_length', then detect outliers in 'petal_length' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['sepal_length'].hist()\nplt.xlabel('sepal_length')\nplt.ylabel('Frequency')\nplt.show()\nQ1 = df['petal_length'].quantile(0.25)\nQ3 = df['petal_length'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['petal_length'] < (Q1 - 1.5*IQR)) | (df['petal_length'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'traffic_volume', then compute TF-IDF features for column 'traffic_volume' and display top 10 words, then one-hot encode the categorical column 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['traffic_volume'])\nprint(vect.get_feature_names_out())\ndf = pd.get_dummies(df, columns=['traffic_volume'], prefix=['traffic_volume'])"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'SalePrice' column using min-max scaling, then compute TF-IDF features for column 'SalePrice' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Neighborhood'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['SalePrice_scaled'] = (df['SalePrice'] - df['SalePrice'].min()) / (df['SalePrice'].max() - df['SalePrice'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['SalePrice'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then create a new feature 'marital_ratio' as the ratio of 'marital' to 'y', then plot a histogram of 'job'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nprint(df.describe())\ndf['marital_ratio'] = df['marital'] / df['y']\ndf['job'].hist()\nplt.xlabel('job')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then clean text data in column 'rating' by removing punctuation and stopwords, then train a Linear Regression model to predict 'sentiment', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['rating_clean'] = df['rating'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['review'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'isFraud' using the IQR method, then plot a histogram of 'time'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['oldbalanceOrg'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['isFraud'].quantile(0.25)\nQ3 = df['isFraud'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['isFraud'] < (Q1 - 1.5*IQR)) | (df['isFraud'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['time'].hist()\nplt.xlabel('time')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then clean text data in column 'Churn' by removing punctuation and stopwords, then compute TF-IDF features for column 'TotalCharges' and display top 10 words, then train a Random Forest Classifier to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Churn_clean'] = df['Churn'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['TotalCharges'])\nprint(vect.get_feature_names_out())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['temperature'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Revenue', then normalize the 'Revenue' column using min-max scaling, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['Revenue_scaled'] = (df['Revenue'] - df['Revenue'].min()) / (df['Revenue'].max() - df['Revenue'].min())\nprint(df.describe())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then detect outliers in 'amount' using the IQR method, then compute TF-IDF features for column 'amount' and display top 10 words, then train a Random Forest Classifier to predict 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nQ1 = df['amount'].quantile(0.25)\nQ3 = df['amount'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['amount'] < (Q1 - 1.5*IQR)) | (df['amount'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['amount'])\nprint(vect.get_feature_names_out())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['species'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then create a new feature 'Survived_ratio' as the ratio of 'Survived' to 'Survived', then compute TF-IDF features for column 'Age' and display top 10 words, then one-hot encode the categorical column 'Pclass'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Survived_ratio'] = df['Survived'] / df['Survived']\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Age'])\nprint(vect.get_feature_names_out())\ndf = pd.get_dummies(df, columns=['Pclass'], prefix=['Pclass'])"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sentiment', then perform time-series forecasting using ARIMA to predict the next 12 periods, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['genre'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'SalePrice', then detect outliers in 'SalePrice' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nQ1 = df['SalePrice'].quantile(0.25)\nQ3 = df['SalePrice'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['SalePrice'] < (Q1 - 1.5*IQR)) | (df['SalePrice'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then clean text data in column 'date_time' by removing punctuation and stopwords, then normalize the 'date_time' column using min-max scaling, then plot a histogram of 'date_time'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_time_clean'] = df['date_time'].apply(clean)\ndf['date_time_scaled'] = (df['date_time'] - df['date_time'].min()) / (df['date_time'].max() - df['date_time'].min())\ndf['date_time'].hist()\nplt.xlabel('date_time')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split, then normalize the 'sepal_length' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['sepal_length_scaled'] = (df['sepal_length'] - df['sepal_length'].min()) / (df['sepal_length'].max() - df['sepal_length'].min())"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then detect outliers in 'Neighborhood' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score, then clean text data in column 'YearBuilt' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nQ1 = df['Neighborhood'].quantile(0.25)\nQ3 = df['Neighborhood'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Neighborhood'] < (Q1 - 1.5*IQR)) | (df['Neighborhood'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['YearBuilt_clean'] = df['YearBuilt'].apply(clean)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'flight', then normalize the 'arrival_delay' column using min-max scaling, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf = pd.get_dummies(df, columns=['flight'], prefix=['flight'])\ndf['arrival_delay_scaled'] = (df['arrival_delay'] - df['arrival_delay'].min()) / (df['arrival_delay'].max() - df['arrival_delay'].min())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then clean text data in column 'date_time' by removing punctuation and stopwords, then compute TF-IDF features for column 'temp' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_time_clean'] = df['date_time'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['temp'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'date' and display top 10 words, then train a Random Forest Classifier to predict 'confirmed', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date'])\nprint(vect.get_feature_names_out())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features, then detect outliers in 'total_amount' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)\nQ1 = df['total_amount'].quantile(0.25)\nQ3 = df['total_amount'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['total_amount'] < (Q1 - 1.5*IQR)) | (df['total_amount'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then one-hot encode the categorical column 'review', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['rating'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf = pd.get_dummies(df, columns=['review'], prefix=['review'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'timestamp' by imputing with median, then train a Linear Regression model to predict 'sensor_value', then clean text data in column 'location' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['timestamp'].fillna(df['timestamp'].median(), inplace=True)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['location_clean'] = df['location'].apply(clean)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then normalize the 'marital' column using min-max scaling, then detect outliers in 'marital' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['marital_scaled'] = (df['marital'] - df['marital'].min()) / (df['marital'].max() - df['marital'].min())\nQ1 = df['marital'].quantile(0.25)\nQ3 = df['marital'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['marital'] < (Q1 - 1.5*IQR)) | (df['marital'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then normalize the 'so2' column using min-max scaling, then one-hot encode the categorical column 'o3'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['so2_scaled'] = (df['so2'] - df['so2'].min()) / (df['so2'].max() - df['so2'].min())\ndf = pd.get_dummies(df, columns=['o3'], prefix=['o3'])"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'open' using the IQR method, then train a Linear Regression model to predict 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['open'].quantile(0.25)\nQ3 = df['open'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['open'] < (Q1 - 1.5*IQR)) | (df['open'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then detect outliers in 'date' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'petal_width', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['petal_width'], prefix=['petal_width'])\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then detect outliers in 'Neighborhood' using the IQR method, then split the data into training and testing sets with an 80-20 split, then one-hot encode the categorical column 'Neighborhood'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nQ1 = df['Neighborhood'].quantile(0.25)\nQ3 = df['Neighborhood'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Neighborhood'] < (Q1 - 1.5*IQR)) | (df['Neighborhood'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf = pd.get_dummies(df, columns=['Neighborhood'], prefix=['Neighborhood'])"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'deaths', then train a Linear Regression model to predict 'confirmed', then create a new feature 'recovered_ratio' as the ratio of 'recovered' to 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf = pd.get_dummies(df, columns=['deaths'], prefix=['deaths'])\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['recovered_ratio'] = df['recovered'] / df['confirmed']"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then calculate the correlation matrix for numeric features, then plot a histogram of 'recovered'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nprint(df.describe())\ncorr = df.corr()\nprint(corr)\ndf['recovered'].hist()\nplt.xlabel('recovered')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then normalize the 'heart_rate' column using min-max scaling, then create a new feature 'ecg_reading_ratio' as the ratio of 'ecg_reading' to 'ecg_reading', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['heart_rate_scaled'] = (df['heart_rate'] - df['heart_rate'].min()) / (df['heart_rate'].max() - df['heart_rate'].min())\ndf['ecg_reading_ratio'] = df['ecg_reading'] / df['ecg_reading']\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'SalePrice', then normalize the 'YearBuilt' column using min-max scaling, then handle missing values in 'SalePrice' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['YearBuilt_scaled'] = (df['YearBuilt'] - df['YearBuilt'].min()) / (df['YearBuilt'].max() - df['YearBuilt'].min())\ndf['SalePrice'].fillna(df['SalePrice'].median(), inplace=True)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then normalize the 'YearBuilt' column using min-max scaling, then display feature importances from the Random Forest model, then clean text data in column 'YearBuilt' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['YearBuilt_scaled'] = (df['YearBuilt'] - df['YearBuilt'].min()) / (df['YearBuilt'].max() - df['YearBuilt'].min())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['YearBuilt_clean'] = df['YearBuilt'].apply(clean)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then handle missing values in 'store' by imputing with median, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'store' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['store'].fillna(df['store'].median(), inplace=True)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sales'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['store_scaled'] = (df['store'] - df['store'].min()) / (df['store'].max() - df['store'].min())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'isFraud' and display top 10 words, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['isFraud'])\nprint(vect.get_feature_names_out())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nprint(df.describe())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then handle missing values in 'precipitation' by imputing with median, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['temperature'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['precipitation'].fillna(df['precipitation'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['precipitation'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features, then normalize the 'date' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)\ndf['date_scaled'] = (df['date'] - df['date'].min()) / (df['date'].max() - df['date'].min())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'customers', then normalize the 'day_of_week' column using min-max scaling, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf = pd.get_dummies(df, columns=['customers'], prefix=['customers'])\ndf['day_of_week_scaled'] = (df['day_of_week'] - df['day_of_week'].min()) / (df['day_of_week'].max() - df['day_of_week'].min())\nprint(df.describe())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'sepal_width' and display top 10 words, then plot a histogram of 'sepal_width'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sepal_width'])\nprint(vect.get_feature_names_out())\ndf['sepal_width'].hist()\nplt.xlabel('sepal_width')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['text'])\ny = df['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['shares'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then plot a histogram of 'store', then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['store'].hist()\nplt.xlabel('store')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'traffic_volume', then normalize the 'rain_1h' column using min-max scaling, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['rain_1h_scaled'] = (df['rain_1h'] - df['rain_1h'].min()) / (df['rain_1h'].max() - df['rain_1h'].min())\nprint(df.describe())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'confirmed' and display top 10 words, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['confirmed'])\nprint(vect.get_feature_names_out())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['confirmed'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nprint(df.describe())"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'total_amount', then split the data into training and testing sets with an 80-20 split, then handle missing values in 'customer_id' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['customer_id'].fillna(df['customer_id'].median(), inplace=True)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'wind_speed' using the IQR method, then compute TF-IDF features for column 'humidity' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['wind_speed'].quantile(0.25)\nQ3 = df['wind_speed'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['wind_speed'] < (Q1 - 1.5*IQR)) | (df['wind_speed'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['humidity'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'species', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then normalize the 'sentiment' column using min-max scaling, then handle missing values in 'sentiment' by imputing with median, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['sentiment_scaled'] = (df['sentiment'] - df['sentiment'].min()) / (df['sentiment'].max() - df['sentiment'].min())\ndf['sentiment'].fillna(df['sentiment'].median(), inplace=True)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sentiment', then handle missing values in 'genre' by imputing with median, then plot a histogram of 'review'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['genre'].fillna(df['genre'].median(), inplace=True)\ndf['review'].hist()\nplt.xlabel('review')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'carrier' by imputing with median, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nprint(df.describe())\ndf['carrier'].fillna(df['carrier'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then normalize the 'Low' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ncorr = df.corr()\nprint(corr)\ndf['Low_scaled'] = (df['Low'] - df['Low'].min()) / (df['Low'].max() - df['Low'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'Survived', then detect outliers in 'Survived' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Pclass'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nQ1 = df['Survived'].quantile(0.25)\nQ3 = df['Survived'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Survived'] < (Q1 - 1.5*IQR)) | (df['Survived'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then create a new feature 'Churn_ratio' as the ratio of 'Churn' to 'Churn', then train a Linear Regression model to predict 'Churn', then handle missing values in 'TotalCharges' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['Churn_ratio'] = df['Churn'] / df['Churn']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then plot a histogram of 'arrival_delay', then clean text data in column 'arrival_delay' by removing punctuation and stopwords, then train a Linear Regression model to predict 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['arrival_delay'].hist()\nplt.xlabel('arrival_delay')\nplt.ylabel('Frequency')\nplt.show()\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['arrival_delay_clean'] = df['arrival_delay'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'total_amount', then handle missing values in 'total_amount' by imputing with median, then train a Random Forest Classifier to predict 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf = pd.get_dummies(df, columns=['total_amount'], prefix=['total_amount'])\ndf['total_amount'].fillna(df['total_amount'].median(), inplace=True)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'wind_speed', then detect outliers in 'humidity' using the IQR method, then train a Random Forest Classifier to predict 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['wind_speed'].hist()\nplt.xlabel('wind_speed')\nplt.ylabel('Frequency')\nplt.show()\nQ1 = df['humidity'].quantile(0.25)\nQ3 = df['humidity'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['humidity'] < (Q1 - 1.5*IQR)) | (df['humidity'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'post_id', then create a new feature 'user_id_ratio' as the ratio of 'user_id' to 'text', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf = pd.get_dummies(df, columns=['post_id'], prefix=['post_id'])\ndf['user_id_ratio'] = df['user_id'] / df['text']\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['user_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'quantity' and display top 10 words, then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['quantity'])\nprint(vect.get_feature_names_out())\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then normalize the 'y' column using min-max scaling, then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'education' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['y_scaled'] = (df['y'] - df['y'].min()) / (df['y'].max() - df['y'].min())\nprint(df.describe())\ndf['education'].fillna(df['education'].median(), inplace=True)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then plot a histogram of 'SalePrice', then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['SalePrice'].hist()\nplt.xlabel('SalePrice')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split, then normalize the 'TotalCharges' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['TotalCharges_scaled'] = (df['TotalCharges'] - df['TotalCharges'].min()) / (df['TotalCharges'].max() - df['TotalCharges'].min())"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'quantity', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nprint(df.describe())\ndf['quantity'].hist()\nplt.xlabel('quantity')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['quantity'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'temp' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['temp'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['temp_scaled'] = (df['temp'] - df['temp'].min()) / (df['temp'].max() - df['temp'].min())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then plot a histogram of 'precipitation', then detect outliers in 'date' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['precipitation'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['precipitation'].hist()\nplt.xlabel('precipitation')\nplt.ylabel('Frequency')\nplt.show()\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then create a new feature 'open_ratio' as the ratio of 'open' to 'sales', then clean text data in column 'store' by removing punctuation and stopwords, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['open_ratio'] = df['open'] / df['sales']\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['store_clean'] = df['store'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then clean text data in column 'Fare' by removing punctuation and stopwords, then train a Linear Regression model to predict 'Survived', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Fare_clean'] = df['Fare'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Fare'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then plot a histogram of 'customers', then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'sales' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['customers'].hist()\nplt.xlabel('customers')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['sales_scaled'] = (df['sales'] - df['sales'].min()) / (df['sales'].max() - df['sales'].min())"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Survived', then perform K-Means clustering with k=3 on numeric features, then handle missing values in 'Sex' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['Sex'].fillna(df['Sex'].median(), inplace=True)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'precipitation', then create a new feature 'precipitation_ratio' as the ratio of 'precipitation' to 'temperature', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf = pd.get_dummies(df, columns=['precipitation'], prefix=['precipitation'])\ndf['precipitation_ratio'] = df['precipitation'] / df['temperature']\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['temperature'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'status' by imputing with median, then one-hot encode the categorical column 'timestamp', then plot a histogram of 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['status'].fillna(df['status'].median(), inplace=True)\ndf = pd.get_dummies(df, columns=['timestamp'], prefix=['timestamp'])\ndf['sensor_value'].hist()\nplt.xlabel('sensor_value')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then detect outliers in 'genre' using the IQR method, then plot a histogram of 'sentiment', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nQ1 = df['genre'].quantile(0.25)\nQ3 = df['genre'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['genre'] < (Q1 - 1.5*IQR)) | (df['genre'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['sentiment'].hist()\nplt.xlabel('sentiment')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then normalize the 'sepal_length' column using min-max scaling, then handle missing values in 'species' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nprint(df.describe())\ndf['sepal_length_scaled'] = (df['sepal_length'] - df['sepal_length'].min()) / (df['sepal_length'].max() - df['sepal_length'].min())\ndf['species'].fillna(df['species'].median(), inplace=True)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features, then plot a histogram of 'o3'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['o3'].hist()\nplt.xlabel('o3')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nprint(df.describe())\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sepal_length'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then evaluate the model performance using RMSE and R\u00b2 score, then clean text data in column 'Churn' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Churn_clean'] = df['Churn'].apply(clean)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'confirmed', then create a new feature 'recovered_ratio' as the ratio of 'recovered' to 'confirmed', then normalize the 'date' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf = pd.get_dummies(df, columns=['confirmed'], prefix=['confirmed'])\ndf['recovered_ratio'] = df['recovered'] / df['confirmed']\ndf['date_scaled'] = (df['date'] - df['date'].min()) / (df['date'].max() - df['date'].min())"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then create a new feature 'SalePrice_ratio' as the ratio of 'SalePrice' to 'SalePrice', then display summary statistics of all numeric columns using df.describe(), then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['SalePrice_ratio'] = df['SalePrice'] / df['SalePrice']\nprint(df.describe())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then clean text data in column 'date' by removing punctuation and stopwords, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['humidity'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'isFraud', then plot a histogram of 'oldbalanceOrg'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['time'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['oldbalanceOrg'].hist()\nplt.xlabel('oldbalanceOrg')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then evaluate the model performance using RMSE and R\u00b2 score, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Revenue'])\ny = df['Revenue']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nprint(df.describe())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'open' and display top 10 words, then plot a histogram of 'sales', then train a Linear Regression model to predict 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['open'])\nprint(vect.get_feature_names_out())\ndf['sales'].hist()\nplt.xlabel('sales')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then create a new feature 'departure_delay_ratio' as the ratio of 'departure_delay' to 'arrival_delay', then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['departure_delay_ratio'] = df['departure_delay'] / df['arrival_delay']\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then clean text data in column 'day_of_week' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe(), then train a Linear Regression model to predict 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['day_of_week_clean'] = df['day_of_week'].apply(clean)\nprint(df.describe())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nprint(df.describe())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then clean text data in column 'pressure' by removing punctuation and stopwords, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['pressure_clean'] = df['pressure'].apply(clean)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then clean text data in column 'deaths' by removing punctuation and stopwords, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['deaths_clean'] = df['deaths'].apply(clean)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['confirmed'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then display feature importances from the Random Forest model, then clean text data in column 'arrival_delay' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nprint(df.describe())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['arrival_delay_clean'] = df['arrival_delay'].apply(clean)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then clean text data in column 'petal_length' by removing punctuation and stopwords, then compute TF-IDF features for column 'sepal_width' and display top 10 words, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['petal_length_clean'] = df['petal_length'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sepal_width'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then normalize the 'open' column using min-max scaling, then calculate the correlation matrix for numeric features, then plot a histogram of 'open'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['open_scaled'] = (df['open'] - df['open'].min()) / (df['open'].max() - df['open'].min())\ncorr = df.corr()\nprint(corr)\ndf['open'].hist()\nplt.xlabel('open')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'Survived', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Pclass'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then plot a histogram of 'o3', then train a Linear Regression model to predict 'pm2_5', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['o3'].hist()\nplt.xlabel('o3')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then compute TF-IDF features for column 'quantity' and display top 10 words, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['quantity'])\nprint(vect.get_feature_names_out())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['transaction_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then compute TF-IDF features for column 'Survived' and display top 10 words, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Survived'])\nprint(vect.get_feature_names_out())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Age'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'text', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['likes'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then detect outliers in 'genre' using the IQR method, then normalize the 'length' column using min-max scaling, then clean text data in column 'review' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nQ1 = df['genre'].quantile(0.25)\nQ3 = df['genre'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['genre'] < (Q1 - 1.5*IQR)) | (df['genre'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['length_scaled'] = (df['length'] - df['length'].min()) / (df['length'].max() - df['length'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['review_clean'] = df['review'].apply(clean)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then detect outliers in 'marital' using the IQR method, then normalize the 'job' column using min-max scaling, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nQ1 = df['marital'].quantile(0.25)\nQ3 = df['marital'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['marital'] < (Q1 - 1.5*IQR)) | (df['marital'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['job_scaled'] = (df['job'] - df['job'].min()) / (df['job'].max() - df['job'].min())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display feature importances from the Random Forest model, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nprint(df.describe())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'Revenue' by imputing with median, then display feature importances from the Random Forest model, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Revenue'].fillna(df['Revenue'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nprint(df.describe())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['review'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Date', then detect outliers in 'Close' using the IQR method, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Date'].hist()\nplt.xlabel('Date')\nplt.ylabel('Frequency')\nplt.show()\nQ1 = df['Close'].quantile(0.25)\nQ3 = df['Close'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Close'] < (Q1 - 1.5*IQR)) | (df['Close'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Close'])\ny = df['Close']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'LotArea' and display top 10 words, then train a Linear Regression model to predict 'SalePrice', then clean text data in column 'YearBuilt' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['LotArea'])\nprint(vect.get_feature_names_out())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['YearBuilt_clean'] = df['YearBuilt'].apply(clean)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then detect outliers in 'Fare' using the IQR method, then clean text data in column 'Age' by removing punctuation and stopwords, then create a new feature 'Pclass_ratio' as the ratio of 'Pclass' to 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nQ1 = df['Fare'].quantile(0.25)\nQ3 = df['Fare'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Fare'] < (Q1 - 1.5*IQR)) | (df['Fare'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Age_clean'] = df['Age'].apply(clean)\ndf['Pclass_ratio'] = df['Pclass'] / df['Survived']"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'date' and display top 10 words, then handle missing values in 'wind_speed' by imputing with median, then create a new feature 'humidity_ratio' as the ratio of 'humidity' to 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date'])\nprint(vect.get_feature_names_out())\ndf['wind_speed'].fillna(df['wind_speed'].median(), inplace=True)\ndf['humidity_ratio'] = df['humidity'] / df['temperature']"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then plot a histogram of 'Open', then one-hot encode the categorical column 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ncorr = df.corr()\nprint(corr)\ndf['Open'].hist()\nplt.xlabel('Open')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['Close'], prefix=['Close'])"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then normalize the 'device_id' column using min-max scaling, then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['device_id_scaled'] = (df['device_id'] - df['device_id'].min()) / (df['device_id'].max() - df['device_id'].min())\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sensor_value'])\ny = df['sensor_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'post_id', then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf = pd.get_dummies(df, columns=['post_id'], prefix=['post_id'])\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['text'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then create a new feature 'pm10_ratio' as the ratio of 'pm10' to 'pm2_5', then split the data into training and testing sets with an 80-20 split, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['pm10_ratio'] = df['pm10'] / df['pm2_5']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sentiment', then split the data into training and testing sets with an 80-20 split, then create a new feature 'length_ratio' as the ratio of 'length' to 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['length_ratio'] = df['length'] / df['sentiment']"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then clean text data in column 'TotalCharges' by removing punctuation and stopwords, then detect outliers in 'Churn' using the IQR method, then compute TF-IDF features for column 'Churn' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['TotalCharges_clean'] = df['TotalCharges'].apply(clean)\nQ1 = df['Churn'].quantile(0.25)\nQ3 = df['Churn'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Churn'] < (Q1 - 1.5*IQR)) | (df['Churn'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Churn'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model, then create a new feature 'time_ratio' as the ratio of 'time' to 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['time_ratio'] = df['time'] / df['ecg_reading']"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'arrival_delay', then detect outliers in 'arrival_delay' using the IQR method, then plot a histogram of 'distance'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['arrival_delay'].quantile(0.25)\nQ3 = df['arrival_delay'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['arrival_delay'] < (Q1 - 1.5*IQR)) | (df['arrival_delay'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['distance'].hist()\nplt.xlabel('distance')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'location' by imputing with median, then calculate the correlation matrix for numeric features, then plot a histogram of 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['location'].fillna(df['location'].median(), inplace=True)\ncorr = df.corr()\nprint(corr)\ndf['sensor_value'].hist()\nplt.xlabel('sensor_value')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then clean text data in column 'customer_id' by removing punctuation and stopwords, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['customer_id_clean'] = df['customer_id'].apply(clean)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sensor_value', then train a Linear Regression model to predict 'sensor_value', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'pm2_5', then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['o3'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Revenue', then handle missing values in 'Product' by imputing with median, then normalize the 'UnitsSold' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['Product'].fillna(df['Product'].median(), inplace=True)\ndf['UnitsSold_scaled'] = (df['UnitsSold'] - df['UnitsSold'].min()) / (df['UnitsSold'].max() - df['UnitsSold'].min())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then clean text data in column 'Churn' by removing punctuation and stopwords, then evaluate the model performance using RMSE and R\u00b2 score, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Churn_clean'] = df['Churn'].apply(clean)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nprint(df.describe())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then plot a histogram of 'open', then display feature importances from the Random Forest model, then compute TF-IDF features for column 'store' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['open'].hist()\nplt.xlabel('open')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['store'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'SalePrice'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'SalePrice', then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then detect outliers in 'distance' using the IQR method, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['distance'].quantile(0.25)\nQ3 = df['distance'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['distance'] < (Q1 - 1.5*IQR)) | (df['distance'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)\nprint(df.describe())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then plot a histogram of 'humidity', then compute TF-IDF features for column 'temperature' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['pressure'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['humidity'].hist()\nplt.xlabel('humidity')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['temperature'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'sensor_value' and display top 10 words, then train a Linear Regression model to predict 'sensor_value', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sensor_value'])\nprint(vect.get_feature_names_out())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'location' using the IQR method, then create a new feature 'device_id_ratio' as the ratio of 'device_id' to 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nprint(df.describe())\nQ1 = df['location'].quantile(0.25)\nQ3 = df['location'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['location'] < (Q1 - 1.5*IQR)) | (df['location'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['device_id_ratio'] = df['device_id'] / df['sensor_value']"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then create a new feature 'YearBuilt_ratio' as the ratio of 'YearBuilt' to 'SalePrice', then normalize the 'LotArea' column using min-max scaling, then train a Random Forest Classifier to predict 'SalePrice'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['YearBuilt_ratio'] = df['YearBuilt'] / df['SalePrice']\ndf['LotArea_scaled'] = (df['LotArea'] - df['LotArea'].min()) / (df['LotArea'].max() - df['LotArea'].min())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'text', then one-hot encode the categorical column 'text', then clean text data in column 'likes' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['text'], prefix=['text'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['likes_clean'] = df['likes'].apply(clean)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then plot a histogram of 'genre', then normalize the 'sentiment' column using min-max scaling, then train a Random Forest Classifier to predict 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['genre'].hist()\nplt.xlabel('genre')\nplt.ylabel('Frequency')\nplt.show()\ndf['sentiment_scaled'] = (df['sentiment'] - df['sentiment'].min()) / (df['sentiment'].max() - df['sentiment'].min())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then display summary statistics of all numeric columns using df.describe(), then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ncorr = df.corr()\nprint(corr)\nprint(df.describe())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'SalePrice', then handle missing values in 'SalePrice' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['LotArea'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['SalePrice'].fillna(df['SalePrice'].median(), inplace=True)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then one-hot encode the categorical column 'o3', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf = pd.get_dummies(df, columns=['o3'], prefix=['o3'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then clean text data in column 'pressure' by removing punctuation and stopwords, then train a Random Forest Classifier to predict 'consumption', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['pressure_clean'] = df['pressure'].apply(clean)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['pressure'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then normalize the 'customer_id' column using min-max scaling, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['customer_id_scaled'] = (df['customer_id'] - df['customer_id'].min()) / (df['customer_id'].max() - df['customer_id'].min())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then detect outliers in 'pm10' using the IQR method, then train a Linear Regression model to predict 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['pm10'].quantile(0.25)\nQ3 = df['pm10'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['pm10'] < (Q1 - 1.5*IQR)) | (df['pm10'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'customers', then display summary statistics of all numeric columns using df.describe(), then clean text data in column 'day_of_week' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf = pd.get_dummies(df, columns=['customers'], prefix=['customers'])\nprint(df.describe())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['day_of_week_clean'] = df['day_of_week'].apply(clean)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then normalize the 'flight' column using min-max scaling, then handle missing values in 'flight' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nprint(df.describe())\ndf['flight_scaled'] = (df['flight'] - df['flight'].min()) / (df['flight'].max() - df['flight'].min())\ndf['flight'].fillna(df['flight'].median(), inplace=True)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then handle missing values in 'date_time' by imputing with median, then compute TF-IDF features for column 'snow_1h' and display top 10 words, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['date_time'].fillna(df['date_time'].median(), inplace=True)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['snow_1h'])\nprint(vect.get_feature_names_out())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then detect outliers in 'species' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods, then handle missing values in 'species' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nQ1 = df['species'].quantile(0.25)\nQ3 = df['species'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['species'] < (Q1 - 1.5*IQR)) | (df['species'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['petal_length'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['species'].fillna(df['species'].median(), inplace=True)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then clean text data in column 'date' by removing punctuation and stopwords, then compute TF-IDF features for column 'pressure' and display top 10 words, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['pressure'])\nprint(vect.get_feature_names_out())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'status' and display top 10 words, then create a new feature 'status_ratio' as the ratio of 'status' to 'sensor_value', then plot a histogram of 'status'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['status'])\nprint(vect.get_feature_names_out())\ndf['status_ratio'] = df['status'] / df['sensor_value']\ndf['status'].hist()\nplt.xlabel('status')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then detect outliers in 'length' using the IQR method, then train a Linear Regression model to predict 'sentiment', then normalize the 'sentiment' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nQ1 = df['length'].quantile(0.25)\nQ3 = df['length'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['length'] < (Q1 - 1.5*IQR)) | (df['length'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['sentiment_scaled'] = (df['sentiment'] - df['sentiment'].min()) / (df['sentiment'].max() - df['sentiment'].min())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'genre', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nprint(df.describe())\ndf['genre'].hist()\nplt.xlabel('genre')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'device_id', then detect outliers in 'device_id' using the IQR method, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf = pd.get_dummies(df, columns=['device_id'], prefix=['device_id'])\nQ1 = df['device_id'].quantile(0.25)\nQ3 = df['device_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['device_id'] < (Q1 - 1.5*IQR)) | (df['device_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nprint(df.describe())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Low', then handle missing values in 'Close' by imputing with median, then plot a histogram of 'Volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf = pd.get_dummies(df, columns=['Low'], prefix=['Low'])\ndf['Close'].fillna(df['Close'].median(), inplace=True)\ndf['Volume'].hist()\nplt.xlabel('Volume')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then one-hot encode the categorical column 'Sex', then detect outliers in 'Pclass' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nprint(df.describe())\ndf = pd.get_dummies(df, columns=['Sex'], prefix=['Sex'])\nQ1 = df['Pclass'].quantile(0.25)\nQ3 = df['Pclass'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Pclass'] < (Q1 - 1.5*IQR)) | (df['Pclass'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then clean text data in column 'Pclass' by removing punctuation and stopwords, then compute TF-IDF features for column 'Pclass' and display top 10 words, then normalize the 'Pclass' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Pclass_clean'] = df['Pclass'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Pclass'])\nprint(vect.get_feature_names_out())\ndf['Pclass_scaled'] = (df['Pclass'] - df['Pclass'].min()) / (df['Pclass'].max() - df['Pclass'].min())"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then detect outliers in 'Sex' using the IQR method, then calculate the correlation matrix for numeric features, then create a new feature 'Age_ratio' as the ratio of 'Age' to 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nQ1 = df['Sex'].quantile(0.25)\nQ3 = df['Sex'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Sex'] < (Q1 - 1.5*IQR)) | (df['Sex'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ncorr = df.corr()\nprint(corr)\ndf['Age_ratio'] = df['Age'] / df['Survived']"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Neighborhood', then display feature importances from the Random Forest model, then compute TF-IDF features for column 'SalePrice' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['Neighborhood'].hist()\nplt.xlabel('Neighborhood')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['SalePrice'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then handle missing values in 'time' by imputing with median, then split the data into training and testing sets with an 80-20 split, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['time'].fillna(df['time'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then perform K-Means clustering with k=3 on numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['age'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'temperature', then plot a histogram of 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['temperature'].hist()\nplt.xlabel('temperature')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then handle missing values in 'Date' by imputing with median, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Date'].fillna(df['Date'].median(), inplace=True)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['High'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Close'])\ny = df['Close']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then clean text data in column 'country' by removing punctuation and stopwords, then detect outliers in 'country' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['country_clean'] = df['country'].apply(clean)\nQ1 = df['country'].quantile(0.25)\nQ3 = df['country'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['country'] < (Q1 - 1.5*IQR)) | (df['country'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then detect outliers in 'temperature' using the IQR method, then perform K-Means clustering with k=3 on numeric features, then normalize the 'temperature' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nQ1 = df['temperature'].quantile(0.25)\nQ3 = df['temperature'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['temperature'] < (Q1 - 1.5*IQR)) | (df['temperature'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['temperature_scaled'] = (df['temperature'] - df['temperature'].min()) / (df['temperature'].max() - df['temperature'].min())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then clean text data in column 'MonthlyCharges' by removing punctuation and stopwords, then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'MonthlyCharges_ratio' as the ratio of 'MonthlyCharges' to 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['MonthlyCharges_clean'] = df['MonthlyCharges'].apply(clean)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['ContractType'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['MonthlyCharges_ratio'] = df['MonthlyCharges'] / df['Churn']"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'length', then create a new feature 'length_ratio' as the ratio of 'length' to 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['length'], prefix=['length'])\ndf['length_ratio'] = df['length'] / df['sentiment']"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'recovered', then train a Random Forest Classifier to predict 'confirmed', then compute TF-IDF features for column 'date' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf = pd.get_dummies(df, columns=['recovered'], prefix=['recovered'])\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'distance' and display top 10 words, then handle missing values in 'departure_delay' by imputing with median, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['distance'])\nprint(vect.get_feature_names_out())\ndf['departure_delay'].fillna(df['departure_delay'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then train a Linear Regression model to predict 'sales', then handle missing values in 'sales' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['sales'].fillna(df['sales'].median(), inplace=True)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then plot a histogram of 'sentiment', then normalize the 'length' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['sentiment'].hist()\nplt.xlabel('sentiment')\nplt.ylabel('Frequency')\nplt.show()\ndf['length_scaled'] = (df['length'] - df['length'].min()) / (df['length'].max() - df['length'].min())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then calculate the correlation matrix for numeric features, then plot a histogram of 'TotalCharges'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ncorr = df.corr()\nprint(corr)\ndf['TotalCharges'].hist()\nplt.xlabel('TotalCharges')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'quality' by removing punctuation and stopwords, then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'heart_rate' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['quality_clean'] = df['quality'].apply(clean)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['heart_rate'].quantile(0.25)\nQ3 = df['heart_rate'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['heart_rate'] < (Q1 - 1.5*IQR)) | (df['heart_rate'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'consumption', then train a Random Forest Classifier to predict 'consumption', then handle missing values in 'pressure' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['pressure'].fillna(df['pressure'].median(), inplace=True)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'date' using the IQR method, then clean text data in column 'temperature' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['temperature_clean'] = df['temperature'].apply(clean)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then normalize the 'Survived' column using min-max scaling, then compute TF-IDF features for column 'Fare' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ncorr = df.corr()\nprint(corr)\ndf['Survived_scaled'] = (df['Survived'] - df['Survived'].min()) / (df['Survived'].max() - df['Survived'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Fare'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sales'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'date' using the IQR method, then one-hot encode the categorical column 'date'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['date'], prefix=['date'])"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'precipitation', then split the data into training and testing sets with an 80-20 split, then normalize the 'wind_speed' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf = pd.get_dummies(df, columns=['precipitation'], prefix=['precipitation'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['wind_speed_scaled'] = (df['wind_speed'] - df['wind_speed'].min()) / (df['wind_speed'].max() - df['wind_speed'].min())"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then plot a histogram of 'total_amount', then train a Linear Regression model to predict 'total_amount', then train a Random Forest Classifier to predict 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['total_amount'].hist()\nplt.xlabel('total_amount')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'total_amount', then evaluate the model performance using RMSE and R\u00b2 score, then handle missing values in 'customer_id' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['customer_id'].fillna(df['customer_id'].median(), inplace=True)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'status' by imputing with median, then display summary statistics of all numeric columns using df.describe(), then compute TF-IDF features for column 'sensor_value' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['status'].fillna(df['status'].median(), inplace=True)\nprint(df.describe())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sensor_value'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model, then compute TF-IDF features for column 'petal_length' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['petal_length'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then train a Linear Regression model to predict 'Close', then detect outliers in 'High' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nQ1 = df['High'].quantile(0.25)\nQ3 = df['High'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['High'] < (Q1 - 1.5*IQR)) | (df['High'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then plot a histogram of 'job', then handle missing values in 'age' by imputing with median, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['job'].hist()\nplt.xlabel('job')\nplt.ylabel('Frequency')\nplt.show()\ndf['age'].fillna(df['age'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'sensor_value' by removing punctuation and stopwords, then detect outliers in 'location' using the IQR method, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sensor_value_clean'] = df['sensor_value'].apply(clean)\nQ1 = df['location'].quantile(0.25)\nQ3 = df['location'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['location'] < (Q1 - 1.5*IQR)) | (df['location'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sensor_value'])\ny = df['sensor_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then clean text data in column 'High' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'Volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['High_clean'] = df['High'].apply(clean)\nprint(df.describe())\ndf['Volume'].hist()\nplt.xlabel('Volume')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then clean text data in column 'Sex' by removing punctuation and stopwords, then calculate the correlation matrix for numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Sex_clean'] = df['Sex'].apply(clean)\ncorr = df.corr()\nprint(corr)\nprint(df.describe())"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'Pclass' and display top 10 words, then normalize the 'Sex' column using min-max scaling, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Pclass'])\nprint(vect.get_feature_names_out())\ndf['Sex_scaled'] = (df['Sex'] - df['Sex'].min()) / (df['Sex'].max() - df['Sex'].min())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then handle missing values in 'Close' by imputing with median, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Close'].fillna(df['Close'].median(), inplace=True)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Open'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nprint(df.describe())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'date_time' and display top 10 words, then detect outliers in 'rain_1h' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date_time'])\nprint(vect.get_feature_names_out())\nQ1 = df['rain_1h'].quantile(0.25)\nQ3 = df['rain_1h'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['rain_1h'] < (Q1 - 1.5*IQR)) | (df['rain_1h'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'time', then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf = pd.get_dummies(df, columns=['time'], prefix=['time'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then detect outliers in 'Pclass' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['Pclass'].quantile(0.25)\nQ3 = df['Pclass'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Pclass'] < (Q1 - 1.5*IQR)) | (df['Pclass'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Pclass'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then create a new feature 'device_id_ratio' as the ratio of 'device_id' to 'sensor_value', then normalize the 'location' column using min-max scaling, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['device_id_ratio'] = df['device_id'] / df['sensor_value']\ndf['location_scaled'] = (df['location'] - df['location'].min()) / (df['location'].max() - df['location'].min())\nprint(df.describe())"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'Age_ratio' as the ratio of 'Age' to 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Survived'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['Age_ratio'] = df['Age'] / df['Survived']"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then clean text data in column 'MonthlyCharges' by removing punctuation and stopwords, then plot a histogram of 'ContractType', then train a Linear Regression model to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['MonthlyCharges_clean'] = df['MonthlyCharges'].apply(clean)\ndf['ContractType'].hist()\nplt.xlabel('ContractType')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then plot a histogram of 'Date', then compute TF-IDF features for column 'Low' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['Date'].hist()\nplt.xlabel('Date')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Low'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then normalize the 'heart_rate' column using min-max scaling, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['ecg_reading'])\ny = df['ecg_reading']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['heart_rate_scaled'] = (df['heart_rate'] - df['heart_rate'].min()) / (df['heart_rate'].max() - df['heart_rate'].min())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'newbalanceOrig' and display top 10 words, then create a new feature 'amount_ratio' as the ratio of 'amount' to 'isFraud', then normalize the 'amount' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['newbalanceOrig'])\nprint(vect.get_feature_names_out())\ndf['amount_ratio'] = df['amount'] / df['isFraud']\ndf['amount_scaled'] = (df['amount'] - df['amount'].min()) / (df['amount'].max() - df['amount'].min())"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Fare', then normalize the 'Survived' column using min-max scaling, then clean text data in column 'Sex' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf = pd.get_dummies(df, columns=['Fare'], prefix=['Fare'])\ndf['Survived_scaled'] = (df['Survived'] - df['Survived'].min()) / (df['Survived'].max() - df['Survived'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Sex_clean'] = df['Sex'].apply(clean)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'patient_id', then normalize the 'time' column using min-max scaling, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf = pd.get_dummies(df, columns=['patient_id'], prefix=['patient_id'])\ndf['time_scaled'] = (df['time'] - df['time'].min()) / (df['time'].max() - df['time'].min())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then plot a histogram of 'Volume', then clean text data in column 'Open' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['Volume'].hist()\nplt.xlabel('Volume')\nplt.ylabel('Frequency')\nplt.show()\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Open_clean'] = df['Open'].apply(clean)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then plot a histogram of 'petal_width', then train a Linear Regression model to predict 'species', then create a new feature 'sepal_length_ratio' as the ratio of 'sepal_length' to 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['petal_width'].hist()\nplt.xlabel('petal_width')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['sepal_length_ratio'] = df['sepal_length'] / df['species']"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then evaluate the model performance using RMSE and R\u00b2 score, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['text'])\ny = df['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then normalize the 'time' column using min-max scaling, then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['time_scaled'] = (df['time'] - df['time'].min()) / (df['time'].max() - df['time'].min())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features, then normalize the 'patient_id' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)\ndf['patient_id_scaled'] = (df['patient_id'] - df['patient_id'].min()) / (df['patient_id'].max() - df['patient_id'].min())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then normalize the 'date' column using min-max scaling, then clean text data in column 'temperature' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ncorr = df.corr()\nprint(corr)\ndf['date_scaled'] = (df['date'] - df['date'].min()) / (df['date'].max() - df['date'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['temperature_clean'] = df['temperature'].apply(clean)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Revenue', then detect outliers in 'Revenue' using the IQR method, then create a new feature 'Revenue_ratio' as the ratio of 'Revenue' to 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['Revenue'].quantile(0.25)\nQ3 = df['Revenue'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Revenue'] < (Q1 - 1.5*IQR)) | (df['Revenue'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['Revenue_ratio'] = df['Revenue'] / df['Revenue']"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then create a new feature 'pressure_ratio' as the ratio of 'pressure' to 'consumption', then train a Random Forest Classifier to predict 'consumption', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['pressure_ratio'] = df['pressure'] / df['consumption']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'OverallQual', then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'LotArea' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf = pd.get_dummies(df, columns=['OverallQual'], prefix=['OverallQual'])\nprint(df.describe())\nQ1 = df['LotArea'].quantile(0.25)\nQ3 = df['LotArea'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['LotArea'] < (Q1 - 1.5*IQR)) | (df['LotArea'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'petal_length', then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'petal_width' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf = pd.get_dummies(df, columns=['petal_length'], prefix=['petal_length'])\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['petal_width'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then clean text data in column 'so2' by removing punctuation and stopwords, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['so2_clean'] = df['so2'].apply(clean)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'y' and display top 10 words, then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'education'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['y'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['education'].hist()\nplt.xlabel('education')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'pm2_5', then compute TF-IDF features for column 'pm10' and display top 10 words, then detect outliers in 'o3' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['pm10'])\nprint(vect.get_feature_names_out())\nQ1 = df['o3'].quantile(0.25)\nQ3 = df['o3'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['o3'] < (Q1 - 1.5*IQR)) | (df['o3'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then create a new feature 'humidity_ratio' as the ratio of 'humidity' to 'consumption', then plot a histogram of 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['humidity_ratio'] = df['humidity'] / df['consumption']\ndf['temperature'].hist()\nplt.xlabel('temperature')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then detect outliers in 'OverallQual' using the IQR method, then train a Random Forest Classifier to predict 'SalePrice', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nQ1 = df['OverallQual'].quantile(0.25)\nQ3 = df['OverallQual'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['OverallQual'] < (Q1 - 1.5*IQR)) | (df['OverallQual'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['temperature'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then clean text data in column 'date' by removing punctuation and stopwords, then evaluate the model performance using RMSE and R\u00b2 score, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then detect outliers in 'newbalanceOrig' using the IQR method, then train a Linear Regression model to predict 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nQ1 = df['newbalanceOrig'].quantile(0.25)\nQ3 = df['newbalanceOrig'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['newbalanceOrig'] < (Q1 - 1.5*IQR)) | (df['newbalanceOrig'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then clean text data in column 'date' by removing punctuation and stopwords, then evaluate the model performance using RMSE and R\u00b2 score, then detect outliers in 'temperature' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nQ1 = df['temperature'].quantile(0.25)\nQ3 = df['temperature'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['temperature'] < (Q1 - 1.5*IQR)) | (df['temperature'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Churn', then train a Random Forest Classifier to predict 'Churn', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nprint(df.describe())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then create a new feature 'education_ratio' as the ratio of 'education' to 'y', then plot a histogram of 'y', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['education_ratio'] = df['education'] / df['y']\ndf['y'].hist()\nplt.xlabel('y')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then clean text data in column 'o3' by removing punctuation and stopwords, then calculate the correlation matrix for numeric features, then plot a histogram of 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['o3_clean'] = df['o3'].apply(clean)\ncorr = df.corr()\nprint(corr)\ndf['pm2_5'].hist()\nplt.xlabel('pm2_5')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Low', then display feature importances from the Random Forest model, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Low'].hist()\nplt.xlabel('Low')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then detect outliers in 'rating' using the IQR method, then normalize the 'rating' column using min-max scaling, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nQ1 = df['rating'].quantile(0.25)\nQ3 = df['rating'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['rating'] < (Q1 - 1.5*IQR)) | (df['rating'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['rating_scaled'] = (df['rating'] - df['rating'].min()) / (df['rating'].max() - df['rating'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'text', then detect outliers in 'likes' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['likes'].quantile(0.25)\nQ3 = df['likes'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['likes'] < (Q1 - 1.5*IQR)) | (df['likes'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'total_amount', then train a Linear Regression model to predict 'total_amount', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['transaction_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then handle missing values in 'open' by imputing with median, then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['open'].fillna(df['open'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then clean text data in column 'pressure' by removing punctuation and stopwords, then evaluate the model performance using RMSE and R\u00b2 score, then handle missing values in 'date' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['pressure_clean'] = df['pressure'].apply(clean)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['date'].fillna(df['date'].median(), inplace=True)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'Pclass', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['Pclass'].hist()\nplt.xlabel('Pclass')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then normalize the 'sepal_width' column using min-max scaling, then plot a histogram of 'sepal_width'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['sepal_width_scaled'] = (df['sepal_width'] - df['sepal_width'].min()) / (df['sepal_width'].max() - df['sepal_width'].min())\ndf['sepal_width'].hist()\nplt.xlabel('sepal_width')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then plot a histogram of 'marital', then compute TF-IDF features for column 'job' and display top 10 words, then normalize the 'education' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['marital'].hist()\nplt.xlabel('marital')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['job'])\nprint(vect.get_feature_names_out())\ndf['education_scaled'] = (df['education'] - df['education'].min()) / (df['education'].max() - df['education'].min())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'user_id' and display top 10 words, then one-hot encode the categorical column 'user_id', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['user_id'])\nprint(vect.get_feature_names_out())\ndf = pd.get_dummies(df, columns=['user_id'], prefix=['user_id'])\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['post_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then evaluate the model performance using RMSE and R\u00b2 score, then create a new feature 'precipitation_ratio' as the ratio of 'precipitation' to 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['precipitation_ratio'] = df['precipitation'] / df['temperature']"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'OverallQual', then display feature importances from the Random Forest model, then plot a histogram of 'YearBuilt'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf = pd.get_dummies(df, columns=['OverallQual'], prefix=['OverallQual'])\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['YearBuilt'].hist()\nplt.xlabel('YearBuilt')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'patient_id' using the IQR method, then perform K-Means clustering with k=3 on numeric features, then handle missing values in 'quality' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nQ1 = df['patient_id'].quantile(0.25)\nQ3 = df['patient_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['patient_id'] < (Q1 - 1.5*IQR)) | (df['patient_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['quality'].fillna(df['quality'].median(), inplace=True)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then create a new feature 'Product_ratio' as the ratio of 'Product' to 'Revenue', then calculate the correlation matrix for numeric features, then one-hot encode the categorical column 'Product'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Product_ratio'] = df['Product'] / df['Revenue']\ncorr = df.corr()\nprint(corr)\ndf = pd.get_dummies(df, columns=['Product'], prefix=['Product'])"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then calculate the correlation matrix for numeric features, then plot a histogram of 'oldbalanceOrg'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ncorr = df.corr()\nprint(corr)\ndf['oldbalanceOrg'].hist()\nplt.xlabel('oldbalanceOrg')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'Date' column using min-max scaling, then clean text data in column 'Open' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Volume'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['Date_scaled'] = (df['Date'] - df['Date'].min()) / (df['Date'].max() - df['Date'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Open_clean'] = df['Open'].apply(clean)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'o3' column using min-max scaling, then one-hot encode the categorical column 'date'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['o3_scaled'] = (df['o3'] - df['o3'].min()) / (df['o3'].max() - df['o3'].min())\ndf = pd.get_dummies(df, columns=['date'], prefix=['date'])"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'location' by removing punctuation and stopwords, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['location_clean'] = df['location'].apply(clean)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['timestamp'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then plot a histogram of 'time', then one-hot encode the categorical column 'newbalanceOrig', then clean text data in column 'time' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['time'].hist()\nplt.xlabel('time')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['newbalanceOrig'], prefix=['newbalanceOrig'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['time_clean'] = df['time'].apply(clean)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then create a new feature 'temperature_ratio' as the ratio of 'temperature' to 'temperature', then plot a histogram of 'date', then one-hot encode the categorical column 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['temperature_ratio'] = df['temperature'] / df['temperature']\ndf['date'].hist()\nplt.xlabel('date')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['temperature'], prefix=['temperature'])"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then normalize the 'ContractType' column using min-max scaling, then detect outliers in 'Churn' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['ContractType_scaled'] = (df['ContractType'] - df['ContractType'].min()) / (df['ContractType'].max() - df['ContractType'].min())\nQ1 = df['Churn'].quantile(0.25)\nQ3 = df['Churn'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Churn'] < (Q1 - 1.5*IQR)) | (df['Churn'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['TotalCharges'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'pm2_5', then normalize the 'so2' column using min-max scaling, then handle missing values in 'so2' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['so2_scaled'] = (df['so2'] - df['so2'].min()) / (df['so2'].max() - df['so2'].min())\ndf['so2'].fillna(df['so2'].median(), inplace=True)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'YearBuilt', then display summary statistics of all numeric columns using df.describe(), then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf = pd.get_dummies(df, columns=['YearBuilt'], prefix=['YearBuilt'])\nprint(df.describe())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'SalePrice' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())\ndf['SalePrice'].fillna(df['SalePrice'].median(), inplace=True)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then clean text data in column 'petal_width' by removing punctuation and stopwords, then normalize the 'petal_width' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sepal_length'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['petal_width_clean'] = df['petal_width'].apply(clean)\ndf['petal_width_scaled'] = (df['petal_width'] - df['petal_width'].min()) / (df['petal_width'].max() - df['petal_width'].min())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'user_id', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['text'])\ny = df['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['user_id'].hist()\nplt.xlabel('user_id')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sensor_value', then compute TF-IDF features for column 'sensor_value' and display top 10 words, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sensor_value'])\nprint(vect.get_feature_names_out())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then clean text data in column 'Survived' by removing punctuation and stopwords, then plot a histogram of 'Pclass'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Survived_clean'] = df['Survived'].apply(clean)\ndf['Pclass'].hist()\nplt.xlabel('Pclass')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then plot a histogram of 'Age', then clean text data in column 'Sex' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['Age'].hist()\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Sex_clean'] = df['Sex'].apply(clean)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'petal_length' and display top 10 words, then detect outliers in 'sepal_length' using the IQR method, then normalize the 'petal_length' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['petal_length'])\nprint(vect.get_feature_names_out())\nQ1 = df['sepal_length'].quantile(0.25)\nQ3 = df['sepal_length'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sepal_length'] < (Q1 - 1.5*IQR)) | (df['sepal_length'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['petal_length_scaled'] = (df['petal_length'] - df['petal_length'].min()) / (df['petal_length'].max() - df['petal_length'].min())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'review', then train a Random Forest Classifier to predict 'sentiment', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf = pd.get_dummies(df, columns=['review'], prefix=['review'])\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'wind_speed' using the IQR method, then one-hot encode the categorical column 'temperature', then handle missing values in 'precipitation' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nQ1 = df['wind_speed'].quantile(0.25)\nQ3 = df['wind_speed'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['wind_speed'] < (Q1 - 1.5*IQR)) | (df['wind_speed'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['temperature'], prefix=['temperature'])\ndf['precipitation'].fillna(df['precipitation'].median(), inplace=True)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then handle missing values in 'review' by imputing with median, then one-hot encode the categorical column 'review', then plot a histogram of 'genre'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['review'].fillna(df['review'].median(), inplace=True)\ndf = pd.get_dummies(df, columns=['review'], prefix=['review'])\ndf['genre'].hist()\nplt.xlabel('genre')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['timestamp'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then plot a histogram of 'customer_id', then split the data into training and testing sets with an 80-20 split, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['customer_id'].hist()\nplt.xlabel('customer_id')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sales', then handle missing values in 'day_of_week' by imputing with median, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['day_of_week'].fillna(df['day_of_week'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then detect outliers in 'temperature' using the IQR method, then train a Linear Regression model to predict 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['temperature'].quantile(0.25)\nQ3 = df['temperature'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['temperature'] < (Q1 - 1.5*IQR)) | (df['temperature'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Pclass', then handle missing values in 'Sex' by imputing with median, then create a new feature 'Fare_ratio' as the ratio of 'Fare' to 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf = pd.get_dummies(df, columns=['Pclass'], prefix=['Pclass'])\ndf['Sex'].fillna(df['Sex'].median(), inplace=True)\ndf['Fare_ratio'] = df['Fare'] / df['Survived']"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then create a new feature 'Close_ratio' as the ratio of 'Close' to 'Close', then display summary statistics of all numeric columns using df.describe(), then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Close_ratio'] = df['Close'] / df['Close']\nprint(df.describe())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then normalize the 'so2' column using min-max scaling, then detect outliers in 'date' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nprint(df.describe())\ndf['so2_scaled'] = (df['so2'] - df['so2'].min()) / (df['so2'].max() - df['so2'].min())\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then clean text data in column 'Close' by removing punctuation and stopwords, then compute TF-IDF features for column 'Low' and display top 10 words, then one-hot encode the categorical column 'Volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Close_clean'] = df['Close'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Low'])\nprint(vect.get_feature_names_out())\ndf = pd.get_dummies(df, columns=['Volume'], prefix=['Volume'])"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sentiment', then perform time-series forecasting using ARIMA to predict the next 12 periods, then handle missing values in 'rating' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['rating'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['rating'].fillna(df['rating'].median(), inplace=True)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then normalize the 'sentiment' column using min-max scaling, then display feature importances from the Random Forest model, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['sentiment_scaled'] = (df['sentiment'] - df['sentiment'].min()) / (df['sentiment'].max() - df['sentiment'].min())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then one-hot encode the categorical column 'flight', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ncorr = df.corr()\nprint(corr)\ndf = pd.get_dummies(df, columns=['flight'], prefix=['flight'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then handle missing values in 'Pclass' by imputing with median, then plot a histogram of 'Pclass'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['Pclass'].fillna(df['Pclass'].median(), inplace=True)\ndf['Pclass'].hist()\nplt.xlabel('Pclass')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split, then create a new feature 'user_id_ratio' as the ratio of 'user_id' to 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['text'])\ny = df['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['user_id_ratio'] = df['user_id'] / df['text']"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then plot a histogram of 'review', then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'genre_ratio' as the ratio of 'genre' to 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['review'].hist()\nplt.xlabel('review')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['length'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['genre_ratio'] = df['genre'] / df['sentiment']"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then create a new feature 'sepal_length_ratio' as the ratio of 'sepal_length' to 'species', then detect outliers in 'species' using the IQR method, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['sepal_length_ratio'] = df['sepal_length'] / df['species']\nQ1 = df['species'].quantile(0.25)\nQ3 = df['species'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['species'] < (Q1 - 1.5*IQR)) | (df['species'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then normalize the 'consumption' column using min-max scaling, then plot a histogram of 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['consumption_scaled'] = (df['consumption'] - df['consumption'].min()) / (df['consumption'].max() - df['consumption'].min())\ndf['consumption'].hist()\nplt.xlabel('consumption')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then handle missing values in 'heart_rate' by imputing with median, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['heart_rate'].fillna(df['heart_rate'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'oldbalanceOrg'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nprint(df.describe())\ndf['oldbalanceOrg'].hist()\nplt.xlabel('oldbalanceOrg')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then detect outliers in 'time' using the IQR method, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nQ1 = df['time'].quantile(0.25)\nQ3 = df['time'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['time'] < (Q1 - 1.5*IQR)) | (df['time'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then detect outliers in 'pm10' using the IQR method, then compute TF-IDF features for column 'o3' and display top 10 words, then handle missing values in 'o3' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nQ1 = df['pm10'].quantile(0.25)\nQ3 = df['pm10'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['pm10'] < (Q1 - 1.5*IQR)) | (df['pm10'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['o3'])\nprint(vect.get_feature_names_out())\ndf['o3'].fillna(df['o3'].median(), inplace=True)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then detect outliers in 'product_id' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nQ1 = df['product_id'].quantile(0.25)\nQ3 = df['product_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['product_id'] < (Q1 - 1.5*IQR)) | (df['product_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'sepal_width', then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf = pd.get_dummies(df, columns=['sepal_width'], prefix=['sepal_width'])\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sepal_length'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then plot a histogram of 'total_amount', then one-hot encode the categorical column 'transaction_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ncorr = df.corr()\nprint(corr)\ndf['total_amount'].hist()\nplt.xlabel('total_amount')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['transaction_id'], prefix=['transaction_id'])"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then detect outliers in 'education' using the IQR method, then handle missing values in 'job' by imputing with median, then plot a histogram of 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nQ1 = df['education'].quantile(0.25)\nQ3 = df['education'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['education'] < (Q1 - 1.5*IQR)) | (df['education'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['job'].fillna(df['job'].median(), inplace=True)\ndf['y'].hist()\nplt.xlabel('y')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then detect outliers in 'consumption' using the IQR method, then handle missing values in 'date' by imputing with median, then train a Random Forest Classifier to predict 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nQ1 = df['consumption'].quantile(0.25)\nQ3 = df['consumption'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['consumption'] < (Q1 - 1.5*IQR)) | (df['consumption'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['date'].fillna(df['date'].median(), inplace=True)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'date' using the IQR method, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['country'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nprint(df.describe())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sentiment', then create a new feature 'review_ratio' as the ratio of 'review' to 'sentiment', then train a Linear Regression model to predict 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['review_ratio'] = df['review'] / df['sentiment']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then normalize the 'shares' column using min-max scaling, then train a Linear Regression model to predict 'text', then handle missing values in 'text' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['shares_scaled'] = (df['shares'] - df['shares'].min()) / (df['shares'].max() - df['shares'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['text'].fillna(df['text'].median(), inplace=True)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then normalize the 'deaths' column using min-max scaling, then plot a histogram of 'recovered', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['deaths_scaled'] = (df['deaths'] - df['deaths'].min()) / (df['deaths'].max() - df['deaths'].min())\ndf['recovered'].hist()\nplt.xlabel('recovered')\nplt.ylabel('Frequency')\nplt.show()\nprint(df.describe())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then detect outliers in 'recovered' using the IQR method, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['recovered'].quantile(0.25)\nQ3 = df['recovered'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['recovered'] < (Q1 - 1.5*IQR)) | (df['recovered'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['marital'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then handle missing values in 'so2' by imputing with median, then split the data into training and testing sets with an 80-20 split, then create a new feature 'date_ratio' as the ratio of 'date' to 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['so2'].fillna(df['so2'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['date_ratio'] = df['date'] / df['pm2_5']"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then detect outliers in 'Pclass' using the IQR method, then normalize the 'Survived' column using min-max scaling, then handle missing values in 'Sex' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nQ1 = df['Pclass'].quantile(0.25)\nQ3 = df['Pclass'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Pclass'] < (Q1 - 1.5*IQR)) | (df['Pclass'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['Survived_scaled'] = (df['Survived'] - df['Survived'].min()) / (df['Survived'].max() - df['Survived'].min())\ndf['Sex'].fillna(df['Sex'].median(), inplace=True)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'no2' and display top 10 words, then normalize the 'no2' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['no2'])\nprint(vect.get_feature_names_out())\ndf['no2_scaled'] = (df['no2'] - df['no2'].min()) / (df['no2'].max() - df['no2'].min())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then plot a histogram of 'petal_length', then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['petal_length'].hist()\nplt.xlabel('petal_length')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sepal_length'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'UnitsSold', then clean text data in column 'Date' by removing punctuation and stopwords, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf = pd.get_dummies(df, columns=['UnitsSold'], prefix=['UnitsSold'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Date_clean'] = df['Date'].apply(clean)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['patient_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['ecg_reading'])\ny = df['ecg_reading']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display feature importances from the Random Forest model, then compute TF-IDF features for column 'isFraud' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['isFraud'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Survived', then clean text data in column 'Sex' by removing punctuation and stopwords, then handle missing values in 'Pclass' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Sex_clean'] = df['Sex'].apply(clean)\ndf['Pclass'].fillna(df['Pclass'].median(), inplace=True)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then clean text data in column 'recovered' by removing punctuation and stopwords, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nprint(df.describe())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['recovered_clean'] = df['recovered'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'y', then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'y_ratio' as the ratio of 'y' to 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf = pd.get_dummies(df, columns=['y'], prefix=['y'])\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['age'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['y_ratio'] = df['y'] / df['y']"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'marital' using the IQR method, then train a Linear Regression model to predict 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nprint(df.describe())\nQ1 = df['marital'].quantile(0.25)\nQ3 = df['marital'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['marital'] < (Q1 - 1.5*IQR)) | (df['marital'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'store'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['store'], prefix=['store'])"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then evaluate the model performance using RMSE and R\u00b2 score, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['o3'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then display summary statistics of all numeric columns using df.describe(), then one-hot encode the categorical column 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nprint(df.describe())\ndf = pd.get_dummies(df, columns=['isFraud'], prefix=['isFraud'])"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'SalePrice', then normalize the 'SalePrice' column using min-max scaling, then plot a histogram of 'SalePrice'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['SalePrice_scaled'] = (df['SalePrice'] - df['SalePrice'].min()) / (df['SalePrice'].max() - df['SalePrice'].min())\ndf['SalePrice'].hist()\nplt.xlabel('SalePrice')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'ecg_reading' using the IQR method, then handle missing values in 'quality' by imputing with median, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nQ1 = df['ecg_reading'].quantile(0.25)\nQ3 = df['ecg_reading'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['ecg_reading'] < (Q1 - 1.5*IQR)) | (df['ecg_reading'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['quality'].fillna(df['quality'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then split the data into training and testing sets with an 80-20 split, then detect outliers in 'y' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['y'].quantile(0.25)\nQ3 = df['y'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['y'] < (Q1 - 1.5*IQR)) | (df['y'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe(), then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then perform K-Means clustering with k=3 on numeric features, then compute TF-IDF features for column 'MonthlyCharges' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['MonthlyCharges'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then plot a histogram of 'heart_rate'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['heart_rate'].hist()\nplt.xlabel('heart_rate')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then clean text data in column 'deaths' by removing punctuation and stopwords, then plot a histogram of 'date', then train a Linear Regression model to predict 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['deaths_clean'] = df['deaths'].apply(clean)\ndf['date'].hist()\nplt.xlabel('date')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'TotalCharges' by imputing with median, then train a Random Forest Classifier to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nprint(df.describe())\ndf['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then detect outliers in 'wind_speed' using the IQR method, then one-hot encode the categorical column 'humidity'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nQ1 = df['wind_speed'].quantile(0.25)\nQ3 = df['wind_speed'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['wind_speed'] < (Q1 - 1.5*IQR)) | (df['wind_speed'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['humidity'], prefix=['humidity'])"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods, then compute TF-IDF features for column 'user_id' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['text'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['user_id'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then compute TF-IDF features for column 'y' and display top 10 words, then clean text data in column 'marital' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['y'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['marital_clean'] = df['marital'].apply(clean)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'sales' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['day_of_week'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['sales_scaled'] = (df['sales'] - df['sales'].min()) / (df['sales'].max() - df['sales'].min())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then one-hot encode the categorical column 'job', then compute TF-IDF features for column 'job' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ncorr = df.corr()\nprint(corr)\ndf = pd.get_dummies(df, columns=['job'], prefix=['job'])\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['job'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'pm10' and display top 10 words, then normalize the 'so2' column using min-max scaling, then create a new feature 'so2_ratio' as the ratio of 'so2' to 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['pm10'])\nprint(vect.get_feature_names_out())\ndf['so2_scaled'] = (df['so2'] - df['so2'].min()) / (df['so2'].max() - df['so2'].min())\ndf['so2_ratio'] = df['so2'] / df['pm2_5']"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'text', then normalize the 'user_id' column using min-max scaling, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['user_id_scaled'] = (df['user_id'] - df['user_id'].min()) / (df['user_id'].max() - df['user_id'].min())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then handle missing values in 'petal_length' by imputing with median, then train a Linear Regression model to predict 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['petal_length'].fillna(df['petal_length'].median(), inplace=True)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Churn', then plot a histogram of 'TotalCharges', then train a Linear Regression model to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['TotalCharges'].hist()\nplt.xlabel('TotalCharges')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'pm2_5', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then clean text data in column 'quantity' by removing punctuation and stopwords, then evaluate the model performance using RMSE and R\u00b2 score, then plot a histogram of 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['quantity_clean'] = df['quantity'].apply(clean)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['total_amount'].hist()\nplt.xlabel('total_amount')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Close', then evaluate the model performance using RMSE and R\u00b2 score, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Open'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then split the data into training and testing sets with an 80-20 split, then handle missing values in 'date_time' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nprint(df.describe())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['date_time'].fillna(df['date_time'].median(), inplace=True)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'timestamp' using the IQR method, then normalize the 'status' column using min-max scaling, then create a new feature 'status_ratio' as the ratio of 'status' to 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nQ1 = df['timestamp'].quantile(0.25)\nQ3 = df['timestamp'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['timestamp'] < (Q1 - 1.5*IQR)) | (df['timestamp'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['status_scaled'] = (df['status'] - df['status'].min()) / (df['status'].max() - df['status'].min())\ndf['status_ratio'] = df['status'] / df['sensor_value']"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then normalize the 'heart_rate' column using min-max scaling, then compute TF-IDF features for column 'time' and display top 10 words, then train a Linear Regression model to predict 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['heart_rate_scaled'] = (df['heart_rate'] - df['heart_rate'].min()) / (df['heart_rate'].max() - df['heart_rate'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['time'])\nprint(vect.get_feature_names_out())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sentiment', then calculate the correlation matrix for numeric features, then normalize the 'rating' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)\ndf['rating_scaled'] = (df['rating'] - df['rating'].min()) / (df['rating'].max() - df['rating'].min())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Close', then split the data into training and testing sets with an 80-20 split, then compute TF-IDF features for column 'High' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Close'])\ny = df['Close']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['High'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then create a new feature 'education_ratio' as the ratio of 'education' to 'y', then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'education'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['education_ratio'] = df['education'] / df['y']\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['education'], prefix=['education'])"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features, then one-hot encode the categorical column 'genre'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)\ndf = pd.get_dummies(df, columns=['genre'], prefix=['genre'])"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'isFraud', then clean text data in column 'time' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['oldbalanceOrg'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['time_clean'] = df['time'].apply(clean)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Churn', then compute TF-IDF features for column 'ContractType' and display top 10 words, then handle missing values in 'TotalCharges' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['ContractType'])\nprint(vect.get_feature_names_out())\ndf['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Random Forest Classifier to predict 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['petal_length'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'precipitation' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score, then compute TF-IDF features for column 'wind_speed' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nQ1 = df['precipitation'].quantile(0.25)\nQ3 = df['precipitation'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['precipitation'] < (Q1 - 1.5*IQR)) | (df['precipitation'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['wind_speed'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then compute TF-IDF features for column 'distance' and display top 10 words, then normalize the 'distance' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['distance'])\nprint(vect.get_feature_names_out())\ndf['distance_scaled'] = (df['distance'] - df['distance'].min()) / (df['distance'].max() - df['distance'].min())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then plot a histogram of 'date', then normalize the 'pressure' column using min-max scaling, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['date'].hist()\nplt.xlabel('date')\nplt.ylabel('Frequency')\nplt.show()\ndf['pressure_scaled'] = (df['pressure'] - df['pressure'].min()) / (df['pressure'].max() - df['pressure'].min())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then create a new feature 'Survived_ratio' as the ratio of 'Survived' to 'Survived', then train a Random Forest Classifier to predict 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['Survived_ratio'] = df['Survived'] / df['Survived']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then detect outliers in 'temperature' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score, then clean text data in column 'consumption' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nQ1 = df['temperature'].quantile(0.25)\nQ3 = df['temperature'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['temperature'] < (Q1 - 1.5*IQR)) | (df['temperature'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['consumption_clean'] = df['consumption'].apply(clean)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then normalize the 'age' column using min-max scaling, then perform K-Means clustering with k=3 on numeric features, then train a Linear Regression model to predict 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['age_scaled'] = (df['age'] - df['age'].min()) / (df['age'].max() - df['age'].min())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then detect outliers in 'rain_1h' using the IQR method, then train a Random Forest Classifier to predict 'traffic_volume', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nQ1 = df['rain_1h'].quantile(0.25)\nQ3 = df['rain_1h'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['rain_1h'] < (Q1 - 1.5*IQR)) | (df['rain_1h'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'arrival_delay', then clean text data in column 'flight' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['flight_clean'] = df['flight'].apply(clean)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then plot a histogram of 'pm2_5', then display feature importances from the Random Forest model, then normalize the 'pm2_5' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['pm2_5'].hist()\nplt.xlabel('pm2_5')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['pm2_5_scaled'] = (df['pm2_5'] - df['pm2_5'].min()) / (df['pm2_5'].max() - df['pm2_5'].min())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Revenue', then display summary statistics of all numeric columns using df.describe(), then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nprint(df.describe())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then handle missing values in 'OverallQual' by imputing with median, then detect outliers in 'LotArea' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['OverallQual'].fillna(df['OverallQual'].median(), inplace=True)\nQ1 = df['LotArea'].quantile(0.25)\nQ3 = df['LotArea'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['LotArea'] < (Q1 - 1.5*IQR)) | (df['LotArea'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'temperature' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['temperature'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then plot a histogram of 'pressure', then train a Random Forest Classifier to predict 'consumption', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['pressure'].hist()\nplt.xlabel('pressure')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then plot a histogram of 'user_id', then one-hot encode the categorical column 'user_id', then compute TF-IDF features for column 'post_id' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['user_id'].hist()\nplt.xlabel('user_id')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['user_id'], prefix=['user_id'])\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['post_id'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'rain_1h' and display top 10 words, then create a new feature 'rain_1h_ratio' as the ratio of 'rain_1h' to 'traffic_volume', then detect outliers in 'traffic_volume' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['rain_1h'])\nprint(vect.get_feature_names_out())\ndf['rain_1h_ratio'] = df['rain_1h'] / df['traffic_volume']\nQ1 = df['traffic_volume'].quantile(0.25)\nQ3 = df['traffic_volume'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['traffic_volume'] < (Q1 - 1.5*IQR)) | (df['traffic_volume'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split, then handle missing values in 'date_time' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['date_time'].fillna(df['date_time'].median(), inplace=True)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'arrival_delay', then perform time-series forecasting using ARIMA to predict the next 12 periods, then plot a histogram of 'flight'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['carrier'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['flight'].hist()\nplt.xlabel('flight')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then split the data into training and testing sets with an 80-20 split, then create a new feature 'newbalanceOrig_ratio' as the ratio of 'newbalanceOrig' to 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['newbalanceOrig_ratio'] = df['newbalanceOrig'] / df['isFraud']"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then clean text data in column 'sentiment' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe(), then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sentiment_clean'] = df['sentiment'].apply(clean)\nprint(df.describe())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['review'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'device_id' column using min-max scaling, then train a Linear Regression model to predict 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['device_id_scaled'] = (df['device_id'] - df['device_id'].min()) / (df['device_id'].max() - df['device_id'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'UnitsSold', then handle missing values in 'Region' by imputing with median, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['UnitsSold'].hist()\nplt.xlabel('UnitsSold')\nplt.ylabel('Frequency')\nplt.show()\ndf['Region'].fillna(df['Region'].median(), inplace=True)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'confirmed', then plot a histogram of 'country', then normalize the 'country' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['country'].hist()\nplt.xlabel('country')\nplt.ylabel('Frequency')\nplt.show()\ndf['country_scaled'] = (df['country'] - df['country'].min()) / (df['country'].max() - df['country'].min())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Linear Regression model to predict 'Close', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nprint(df.describe())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'temperature', then detect outliers in 'precipitation' using the IQR method, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nQ1 = df['precipitation'].quantile(0.25)\nQ3 = df['precipitation'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['precipitation'] < (Q1 - 1.5*IQR)) | (df['precipitation'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then plot a histogram of 'temperature', then create a new feature 'pressure_ratio' as the ratio of 'pressure' to 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['humidity'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['temperature'].hist()\nplt.xlabel('temperature')\nplt.ylabel('Frequency')\nplt.show()\ndf['pressure_ratio'] = df['pressure'] / df['consumption']"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then detect outliers in 'genre' using the IQR method, then train a Linear Regression model to predict 'sentiment', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nQ1 = df['genre'].quantile(0.25)\nQ3 = df['genre'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['genre'] < (Q1 - 1.5*IQR)) | (df['genre'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sensor_value', then clean text data in column 'location' by removing punctuation and stopwords, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['location_clean'] = df['location'].apply(clean)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then handle missing values in 'deaths' by imputing with median, then detect outliers in 'date' using the IQR method, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['deaths'].fillna(df['deaths'].median(), inplace=True)\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then detect outliers in 'humidity' using the IQR method, then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nQ1 = df['humidity'].quantile(0.25)\nQ3 = df['humidity'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['humidity'] < (Q1 - 1.5*IQR)) | (df['humidity'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then detect outliers in 'rain_1h' using the IQR method, then handle missing values in 'date_time' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['rain_1h'].quantile(0.25)\nQ3 = df['rain_1h'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['rain_1h'] < (Q1 - 1.5*IQR)) | (df['rain_1h'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['date_time'].fillna(df['date_time'].median(), inplace=True)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then normalize the 'OverallQual' column using min-max scaling, then one-hot encode the categorical column 'YearBuilt', then create a new feature 'LotArea_ratio' as the ratio of 'LotArea' to 'SalePrice'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['OverallQual_scaled'] = (df['OverallQual'] - df['OverallQual'].min()) / (df['OverallQual'].max() - df['OverallQual'].min())\ndf = pd.get_dummies(df, columns=['YearBuilt'], prefix=['YearBuilt'])\ndf['LotArea_ratio'] = df['LotArea'] / df['SalePrice']"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then normalize the 'Date' column using min-max scaling, then split the data into training and testing sets with an 80-20 split, then clean text data in column 'UnitsSold' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Date_scaled'] = (df['Date'] - df['Date'].min()) / (df['Date'].max() - df['Date'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Revenue'])\ny = df['Revenue']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['UnitsSold_clean'] = df['UnitsSold'].apply(clean)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'SalePrice', then detect outliers in 'SalePrice' using the IQR method, then plot a histogram of 'LotArea'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nQ1 = df['SalePrice'].quantile(0.25)\nQ3 = df['SalePrice'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['SalePrice'] < (Q1 - 1.5*IQR)) | (df['SalePrice'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['LotArea'].hist()\nplt.xlabel('LotArea')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sentiment', then display feature importances from the Random Forest model, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'petal_length', then detect outliers in 'sepal_length' using the IQR method, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf = pd.get_dummies(df, columns=['petal_length'], prefix=['petal_length'])\nQ1 = df['sepal_length'].quantile(0.25)\nQ3 = df['sepal_length'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sepal_length'] < (Q1 - 1.5*IQR)) | (df['sepal_length'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'date_ratio' as the ratio of 'date' to 'temperature', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['temperature'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['date_ratio'] = df['date'] / df['temperature']\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then display summary statistics of all numeric columns using df.describe(), then one-hot encode the categorical column 'review'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nprint(df.describe())\ndf = pd.get_dummies(df, columns=['review'], prefix=['review'])"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then create a new feature 'ContractType_ratio' as the ratio of 'ContractType' to 'Churn', then evaluate the model performance using RMSE and R\u00b2 score, then detect outliers in 'MonthlyCharges' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['ContractType_ratio'] = df['ContractType'] / df['Churn']\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nQ1 = df['MonthlyCharges'].quantile(0.25)\nQ3 = df['MonthlyCharges'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['MonthlyCharges'] < (Q1 - 1.5*IQR)) | (df['MonthlyCharges'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then one-hot encode the categorical column 'job', then train a Linear Regression model to predict 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nprint(df.describe())\ndf = pd.get_dummies(df, columns=['job'], prefix=['job'])\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Churn', then split the data into training and testing sets with an 80-20 split, then create a new feature 'MonthlyCharges_ratio' as the ratio of 'MonthlyCharges' to 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['MonthlyCharges_ratio'] = df['MonthlyCharges'] / df['Churn']"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'distance' using the IQR method, then normalize the 'flight' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nprint(df.describe())\nQ1 = df['distance'].quantile(0.25)\nQ3 = df['distance'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['distance'] < (Q1 - 1.5*IQR)) | (df['distance'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['flight_scaled'] = (df['flight'] - df['flight'].min()) / (df['flight'].max() - df['flight'].min())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'petal_length' and display top 10 words, then split the data into training and testing sets with an 80-20 split, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['petal_length'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then create a new feature 'UnitsSold_ratio' as the ratio of 'UnitsSold' to 'Revenue', then plot a histogram of 'Region', then train a Random Forest Classifier to predict 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['UnitsSold_ratio'] = df['UnitsSold'] / df['Revenue']\ndf['Region'].hist()\nplt.xlabel('Region')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then plot a histogram of 'flight', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['flight'].hist()\nplt.xlabel('flight')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then handle missing values in 'length' by imputing with median, then display summary statistics of all numeric columns using df.describe(), then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['length'].fillna(df['length'].median(), inplace=True)\nprint(df.describe())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then normalize the 'sepal_length' column using min-max scaling, then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['sepal_length_scaled'] = (df['sepal_length'] - df['sepal_length'].min()) / (df['sepal_length'].max() - df['sepal_length'].min())\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'TotalCharges', then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf = pd.get_dummies(df, columns=['TotalCharges'], prefix=['TotalCharges'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe(), then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'SalePrice', then evaluate the model performance using RMSE and R\u00b2 score, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then normalize the 'Volume' column using min-max scaling, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Random Forest Classifier to predict 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Volume_scaled'] = (df['Volume'] - df['Volume'].min()) / (df['Volume'].max() - df['Volume'].min())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['High'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then calculate the correlation matrix for numeric features, then train a Random Forest Classifier to predict 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['education'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ncorr = df.corr()\nprint(corr)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then detect outliers in 'ecg_reading' using the IQR method, then plot a histogram of 'time'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['ecg_reading'].quantile(0.25)\nQ3 = df['ecg_reading'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['ecg_reading'] < (Q1 - 1.5*IQR)) | (df['ecg_reading'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['time'].hist()\nplt.xlabel('time')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then normalize the 'text' column using min-max scaling, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ncorr = df.corr()\nprint(corr)\ndf['text_scaled'] = (df['text'] - df['text'].min()) / (df['text'].max() - df['text'].min())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'carrier' using the IQR method, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['departure_delay'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['carrier'].quantile(0.25)\nQ3 = df['carrier'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['carrier'] < (Q1 - 1.5*IQR)) | (df['carrier'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then clean text data in column 'total_amount' by removing punctuation and stopwords, then train a Linear Regression model to predict 'total_amount', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['total_amount_clean'] = df['total_amount'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then plot a histogram of 'pressure', then compute TF-IDF features for column 'date' and display top 10 words, then one-hot encode the categorical column 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['pressure'].hist()\nplt.xlabel('pressure')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date'])\nprint(vect.get_feature_names_out())\ndf = pd.get_dummies(df, columns=['temperature'], prefix=['temperature'])"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then create a new feature 'MonthlyCharges_ratio' as the ratio of 'MonthlyCharges' to 'Churn', then compute TF-IDF features for column 'MonthlyCharges' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['MonthlyCharges_ratio'] = df['MonthlyCharges'] / df['Churn']\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['MonthlyCharges'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'arrival_delay', then perform time-series forecasting using ARIMA to predict the next 12 periods, then one-hot encode the categorical column 'flight'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['distance'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf = pd.get_dummies(df, columns=['flight'], prefix=['flight'])"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then detect outliers in 'temperature' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'date' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nQ1 = df['temperature'].quantile(0.25)\nQ3 = df['temperature'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['temperature'] < (Q1 - 1.5*IQR)) | (df['temperature'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['consumption'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['date_scaled'] = (df['date'] - df['date'].min()) / (df['date'].max() - df['date'].min())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then normalize the 'UnitsSold' column using min-max scaling, then create a new feature 'Date_ratio' as the ratio of 'Date' to 'Revenue', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['UnitsSold_scaled'] = (df['UnitsSold'] - df['UnitsSold'].min()) / (df['UnitsSold'].max() - df['UnitsSold'].min())\ndf['Date_ratio'] = df['Date'] / df['Revenue']\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then create a new feature 'Date_ratio' as the ratio of 'Date' to 'Close', then display summary statistics of all numeric columns using df.describe(), then compute TF-IDF features for column 'Open' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Date_ratio'] = df['Date'] / df['Close']\nprint(df.describe())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Open'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then clean text data in column 'time' by removing punctuation and stopwords, then split the data into training and testing sets with an 80-20 split, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['time_clean'] = df['time'].apply(clean)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe(), then compute TF-IDF features for column 'sepal_width' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sepal_width'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then create a new feature 'y_ratio' as the ratio of 'y' to 'y', then clean text data in column 'y' by removing punctuation and stopwords, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['y_ratio'] = df['y'] / df['y']\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['y_clean'] = df['y'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'arrival_delay', then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then plot a histogram of 'ContractType', then display summary statistics of all numeric columns using df.describe(), then clean text data in column 'Churn' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['ContractType'].hist()\nplt.xlabel('ContractType')\nplt.ylabel('Frequency')\nplt.show()\nprint(df.describe())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Churn_clean'] = df['Churn'].apply(clean)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then clean text data in column 'petal_length' by removing punctuation and stopwords, then plot a histogram of 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['petal_length_clean'] = df['petal_length'].apply(clean)\ndf['species'].hist()\nplt.xlabel('species')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'temperature', then one-hot encode the categorical column 'temperature', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['temperature'], prefix=['temperature'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model, then handle missing values in 'SalePrice' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['SalePrice'].fillna(df['SalePrice'].median(), inplace=True)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'pm2_5', then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'date', then plot a histogram of 'pm10', then compute TF-IDF features for column 'pm2_5' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf = pd.get_dummies(df, columns=['date'], prefix=['date'])\ndf['pm10'].hist()\nplt.xlabel('pm10')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['pm2_5'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'traffic_volume', then detect outliers in 'snow_1h' using the IQR method, then train a Random Forest Classifier to predict 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf = pd.get_dummies(df, columns=['traffic_volume'], prefix=['traffic_volume'])\nQ1 = df['snow_1h'].quantile(0.25)\nQ3 = df['snow_1h'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['snow_1h'] < (Q1 - 1.5*IQR)) | (df['snow_1h'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'oldbalanceOrg', then plot a histogram of 'newbalanceOrig', then train a Linear Regression model to predict 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf = pd.get_dummies(df, columns=['oldbalanceOrg'], prefix=['oldbalanceOrg'])\ndf['newbalanceOrig'].hist()\nplt.xlabel('newbalanceOrig')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then display feature importances from the Random Forest model, then train a Random Forest Classifier to predict 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'so2' using the IQR method, then one-hot encode the categorical column 'so2'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nprint(df.describe())\nQ1 = df['so2'].quantile(0.25)\nQ3 = df['so2'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['so2'] < (Q1 - 1.5*IQR)) | (df['so2'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['so2'], prefix=['so2'])"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'arrival_delay', then handle missing values in 'flight' by imputing with median, then train a Linear Regression model to predict 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['flight'].fillna(df['flight'].median(), inplace=True)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'location', then train a Linear Regression model to predict 'sensor_value', then create a new feature 'sensor_value_ratio' as the ratio of 'sensor_value' to 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['location'].hist()\nplt.xlabel('location')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['sensor_value_ratio'] = df['sensor_value'] / df['sensor_value']"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'date_time', then clean text data in column 'traffic_volume' by removing punctuation and stopwords, then plot a histogram of 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf = pd.get_dummies(df, columns=['date_time'], prefix=['date_time'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['traffic_volume_clean'] = df['traffic_volume'].apply(clean)\ndf['traffic_volume'].hist()\nplt.xlabel('traffic_volume')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'Date' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Date'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Random Forest Classifier to predict 'confirmed', then handle missing values in 'confirmed' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['confirmed'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['confirmed'].fillna(df['confirmed'].median(), inplace=True)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then clean text data in column 'Sex' by removing punctuation and stopwords, then perform time-series forecasting using ARIMA to predict the next 12 periods, then handle missing values in 'Fare' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Sex_clean'] = df['Sex'].apply(clean)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Survived'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['Fare'].fillna(df['Fare'].median(), inplace=True)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe(), then normalize the 'user_id' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['text'])\ny = df['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())\ndf['user_id_scaled'] = (df['user_id'] - df['user_id'].min()) / (df['user_id'].max() - df['user_id'].min())"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods, then plot a histogram of 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['status'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['sensor_value'].hist()\nplt.xlabel('sensor_value')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'sepal_width', then clean text data in column 'petal_width' by removing punctuation and stopwords, then train a Random Forest Classifier to predict 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf = pd.get_dummies(df, columns=['sepal_width'], prefix=['sepal_width'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['petal_width_clean'] = df['petal_width'].apply(clean)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display feature importances from the Random Forest model, then clean text data in column 'date' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then clean text data in column 'recovered' by removing punctuation and stopwords, then train a Linear Regression model to predict 'confirmed', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['recovered_clean'] = df['recovered'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then plot a histogram of 'date', then create a new feature 'humidity_ratio' as the ratio of 'humidity' to 'consumption', then normalize the 'pressure' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['date'].hist()\nplt.xlabel('date')\nplt.ylabel('Frequency')\nplt.show()\ndf['humidity_ratio'] = df['humidity'] / df['consumption']\ndf['pressure_scaled'] = (df['pressure'] - df['pressure'].min()) / (df['pressure'].max() - df['pressure'].min())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'traffic_volume' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['traffic_volume'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then perform time-series forecasting using ARIMA to predict the next 12 periods, then clean text data in column 'tenure' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['tenure'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['tenure_clean'] = df['tenure'].apply(clean)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'confirmed', then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'date_time', then clean text data in column 'traffic_volume' by removing punctuation and stopwords, then handle missing values in 'traffic_volume' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf = pd.get_dummies(df, columns=['date_time'], prefix=['date_time'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['traffic_volume_clean'] = df['traffic_volume'].apply(clean)\ndf['traffic_volume'].fillna(df['traffic_volume'].median(), inplace=True)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'pressure' column using min-max scaling, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['pressure_scaled'] = (df['pressure'] - df['pressure'].min()) / (df['pressure'].max() - df['pressure'].min())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then train a Linear Regression model to predict 'isFraud', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['time'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then normalize the 'Region' column using min-max scaling, then display feature importances from the Random Forest model, then one-hot encode the categorical column 'Region'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Region_scaled'] = (df['Region'] - df['Region'].min()) / (df['Region'].max() - df['Region'].min())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf = pd.get_dummies(df, columns=['Region'], prefix=['Region'])"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then plot a histogram of 'sepal_length', then one-hot encode the categorical column 'petal_width'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['sepal_length'].hist()\nplt.xlabel('sepal_length')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['petal_width'], prefix=['petal_width'])"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then detect outliers in 'sepal_length' using the IQR method, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nQ1 = df['sepal_length'].quantile(0.25)\nQ3 = df['sepal_length'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sepal_length'] < (Q1 - 1.5*IQR)) | (df['sepal_length'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then clean text data in column 'store' by removing punctuation and stopwords, then plot a histogram of 'store', then compute TF-IDF features for column 'day_of_week' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['store_clean'] = df['store'].apply(clean)\ndf['store'].hist()\nplt.xlabel('store')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['day_of_week'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then handle missing values in 'education' by imputing with median, then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['education'].fillna(df['education'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['y'].hist()\nplt.xlabel('y')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then handle missing values in 'MonthlyCharges' by imputing with median, then perform K-Means clustering with k=3 on numeric features, then clean text data in column 'ContractType' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['MonthlyCharges'].fillna(df['MonthlyCharges'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['ContractType_clean'] = df['ContractType'].apply(clean)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then clean text data in column 'petal_length' by removing punctuation and stopwords, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['petal_length_clean'] = df['petal_length'].apply(clean)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sepal_length'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then clean text data in column 'High' by removing punctuation and stopwords, then train a Random Forest Classifier to predict 'Close', then handle missing values in 'Date' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['High_clean'] = df['High'].apply(clean)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['Date'].fillna(df['Date'].median(), inplace=True)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then detect outliers in 'review' using the IQR method, then clean text data in column 'sentiment' by removing punctuation and stopwords, then train a Linear Regression model to predict 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nQ1 = df['review'].quantile(0.25)\nQ3 = df['review'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['review'] < (Q1 - 1.5*IQR)) | (df['review'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sentiment_clean'] = df['sentiment'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'timestamp' by removing punctuation and stopwords, then handle missing values in 'timestamp' by imputing with median, then create a new feature 'status_ratio' as the ratio of 'status' to 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['timestamp_clean'] = df['timestamp'].apply(clean)\ndf['timestamp'].fillna(df['timestamp'].median(), inplace=True)\ndf['status_ratio'] = df['status'] / df['sensor_value']"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Fare', then create a new feature 'Sex_ratio' as the ratio of 'Sex' to 'Survived', then train a Random Forest Classifier to predict 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf = pd.get_dummies(df, columns=['Fare'], prefix=['Fare'])\ndf['Sex_ratio'] = df['Sex'] / df['Survived']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then create a new feature 'departure_delay_ratio' as the ratio of 'departure_delay' to 'arrival_delay', then compute TF-IDF features for column 'carrier' and display top 10 words, then normalize the 'distance' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['departure_delay_ratio'] = df['departure_delay'] / df['arrival_delay']\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['carrier'])\nprint(vect.get_feature_names_out())\ndf['distance_scaled'] = (df['distance'] - df['distance'].min()) / (df['distance'].max() - df['distance'].min())"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then normalize the 'LotArea' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'SalePrice'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['LotArea_scaled'] = (df['LotArea'] - df['LotArea'].min()) / (df['LotArea'].max() - df['LotArea'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then normalize the 'YearBuilt' column using min-max scaling, then train a Linear Regression model to predict 'SalePrice', then train a Random Forest Classifier to predict 'SalePrice'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['YearBuilt_scaled'] = (df['YearBuilt'] - df['YearBuilt'].min()) / (df['YearBuilt'].max() - df['YearBuilt'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then handle missing values in 'review' by imputing with median, then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['review'].fillna(df['review'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'pm2_5', then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then handle missing values in 'Region' by imputing with median, then compute TF-IDF features for column 'Product' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Revenue'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['Region'].fillna(df['Region'].median(), inplace=True)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Product'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then evaluate the model performance using RMSE and R\u00b2 score, then detect outliers in 'petal_width' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sepal_width'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nQ1 = df['petal_width'].quantile(0.25)\nQ3 = df['petal_width'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['petal_width'] < (Q1 - 1.5*IQR)) | (df['petal_width'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then create a new feature 'Volume_ratio' as the ratio of 'Volume' to 'Close', then display feature importances from the Random Forest model, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Volume_ratio'] = df['Volume'] / df['Close']\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nprint(df.describe())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then one-hot encode the categorical column 'open', then compute TF-IDF features for column 'day_of_week' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf = pd.get_dummies(df, columns=['open'], prefix=['open'])\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['day_of_week'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then handle missing values in 'LotArea' by imputing with median, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['LotArea'].fillna(df['LotArea'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then handle missing values in 'Survived' by imputing with median, then display feature importances from the Random Forest model, then clean text data in column 'Fare' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Survived'].fillna(df['Survived'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Fare_clean'] = df['Fare'].apply(clean)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split, then handle missing values in 'total_amount' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['total_amount'].fillna(df['total_amount'].median(), inplace=True)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Random Forest Classifier to predict 'Revenue', then detect outliers in 'Product' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nprint(df.describe())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['Product'].quantile(0.25)\nQ3 = df['Product'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Product'] < (Q1 - 1.5*IQR)) | (df['Product'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then detect outliers in 'age' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods, then compute TF-IDF features for column 'y' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nQ1 = df['age'].quantile(0.25)\nQ3 = df['age'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['age'] < (Q1 - 1.5*IQR)) | (df['age'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['education'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['y'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then handle missing values in 'post_id' by imputing with median, then one-hot encode the categorical column 'likes', then train a Linear Regression model to predict 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['post_id'].fillna(df['post_id'].median(), inplace=True)\ndf = pd.get_dummies(df, columns=['likes'], prefix=['likes'])\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then compute TF-IDF features for column 'device_id' and display top 10 words, then train a Linear Regression model to predict 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['device_id'])\nprint(vect.get_feature_names_out())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then clean text data in column 'Churn' by removing punctuation and stopwords, then normalize the 'MonthlyCharges' column using min-max scaling, then handle missing values in 'TotalCharges' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Churn_clean'] = df['Churn'].apply(clean)\ndf['MonthlyCharges_scaled'] = (df['MonthlyCharges'] - df['MonthlyCharges'].min()) / (df['MonthlyCharges'].max() - df['MonthlyCharges'].min())\ndf['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'ecg_reading', then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['ecg_reading'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then handle missing values in 'pm2_5' by imputing with median, then display summary statistics of all numeric columns using df.describe(), then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['pm2_5'].fillna(df['pm2_5'].median(), inplace=True)\nprint(df.describe())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'isFraud', then compute TF-IDF features for column 'time' and display top 10 words, then detect outliers in 'time' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['time'])\nprint(vect.get_feature_names_out())\nQ1 = df['time'].quantile(0.25)\nQ3 = df['time'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['time'] < (Q1 - 1.5*IQR)) | (df['time'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['text'])\ny = df['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'traffic_volume', then clean text data in column 'date_time' by removing punctuation and stopwords, then detect outliers in 'snow_1h' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_time_clean'] = df['date_time'].apply(clean)\nQ1 = df['snow_1h'].quantile(0.25)\nQ3 = df['snow_1h'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['snow_1h'] < (Q1 - 1.5*IQR)) | (df['snow_1h'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'text', then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then clean text data in column 'TotalCharges' by removing punctuation and stopwords, then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'Churn_ratio' as the ratio of 'Churn' to 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['TotalCharges_clean'] = df['TotalCharges'].apply(clean)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['tenure'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['Churn_ratio'] = df['Churn'] / df['Churn']"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'Pclass' and display top 10 words, then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Pclass'])\nprint(vect.get_feature_names_out())\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Age'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then one-hot encode the categorical column 'Fare', then handle missing values in 'Fare' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf = pd.get_dummies(df, columns=['Fare'], prefix=['Fare'])\ndf['Fare'].fillna(df['Fare'].median(), inplace=True)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Linear Regression model to predict 'confirmed', then compute TF-IDF features for column 'country' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nprint(df.describe())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['country'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then split the data into training and testing sets with an 80-20 split, then one-hot encode the categorical column 'likes'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['text'])\ny = df['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf = pd.get_dummies(df, columns=['likes'], prefix=['likes'])"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then calculate the correlation matrix for numeric features, then clean text data in column 'date' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['temperature'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ncorr = df.corr()\nprint(corr)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Close', then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Close'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Close'])\ny = df['Close']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'ecg_reading', then normalize the 'ecg_reading' column using min-max scaling, then create a new feature 'ecg_reading_ratio' as the ratio of 'ecg_reading' to 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['ecg_reading_scaled'] = (df['ecg_reading'] - df['ecg_reading'].min()) / (df['ecg_reading'].max() - df['ecg_reading'].min())\ndf['ecg_reading_ratio'] = df['ecg_reading'] / df['ecg_reading']"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then create a new feature 'job_ratio' as the ratio of 'job' to 'y', then perform K-Means clustering with k=3 on numeric features, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['job_ratio'] = df['job'] / df['y']\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'review', then compute TF-IDF features for column 'rating' and display top 10 words, then plot a histogram of 'rating'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf = pd.get_dummies(df, columns=['review'], prefix=['review'])\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['rating'])\nprint(vect.get_feature_names_out())\ndf['rating'].hist()\nplt.xlabel('rating')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then handle missing values in 'customers' by imputing with median, then compute TF-IDF features for column 'store' and display top 10 words, then plot a histogram of 'day_of_week'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['customers'].fillna(df['customers'].median(), inplace=True)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['store'])\nprint(vect.get_feature_names_out())\ndf['day_of_week'].hist()\nplt.xlabel('day_of_week')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'species', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sepal_length'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then handle missing values in 'review' by imputing with median, then perform time-series forecasting using ARIMA to predict the next 12 periods, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['review'].fillna(df['review'].median(), inplace=True)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['genre'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'rating'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['length'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nprint(df.describe())\ndf['rating'].hist()\nplt.xlabel('rating')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'MonthlyCharges' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['tenure'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nprint(df.describe())\nQ1 = df['MonthlyCharges'].quantile(0.25)\nQ3 = df['MonthlyCharges'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['MonthlyCharges'] < (Q1 - 1.5*IQR)) | (df['MonthlyCharges'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then create a new feature 'time_ratio' as the ratio of 'time' to 'isFraud', then clean text data in column 'time' by removing punctuation and stopwords, then compute TF-IDF features for column 'oldbalanceOrg' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['time_ratio'] = df['time'] / df['isFraud']\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['time_clean'] = df['time'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['oldbalanceOrg'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then normalize the 'Low' column using min-max scaling, then perform time-series forecasting using ARIMA to predict the next 12 periods, then compute TF-IDF features for column 'Low' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Low_scaled'] = (df['Low'] - df['Low'].min()) / (df['Low'].max() - df['Low'].min())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['High'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Low'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then clean text data in column 'petal_length' by removing punctuation and stopwords, then normalize the 'petal_width' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['petal_length_clean'] = df['petal_length'].apply(clean)\ndf['petal_width_scaled'] = (df['petal_width'] - df['petal_width'].min()) / (df['petal_width'].max() - df['petal_width'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'arrival_delay', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['arrival_delay'].hist()\nplt.xlabel('arrival_delay')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sales', then handle missing values in 'open' by imputing with median, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['open'].fillna(df['open'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'review' and display top 10 words, then normalize the 'genre' column using min-max scaling, then train a Linear Regression model to predict 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['review'])\nprint(vect.get_feature_names_out())\ndf['genre_scaled'] = (df['genre'] - df['genre'].min()) / (df['genre'].max() - df['genre'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'quality', then evaluate the model performance using RMSE and R\u00b2 score, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['quality'].hist()\nplt.xlabel('quality')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'pm2_5', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['pm10'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'pressure' and display top 10 words, then train a Linear Regression model to predict 'consumption', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['pressure'])\nprint(vect.get_feature_names_out())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform time-series forecasting using ARIMA to predict the next 12 periods, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nprint(df.describe())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['quantity'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'departure_delay' by imputing with median, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nprint(df.describe())\ndf['departure_delay'].fillna(df['departure_delay'].median(), inplace=True)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'total_amount', then one-hot encode the categorical column 'customer_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['total_amount'].hist()\nplt.xlabel('total_amount')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['customer_id'], prefix=['customer_id'])"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then create a new feature 'ecg_reading_ratio' as the ratio of 'ecg_reading' to 'ecg_reading', then split the data into training and testing sets with an 80-20 split, then handle missing values in 'patient_id' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['ecg_reading_ratio'] = df['ecg_reading'] / df['ecg_reading']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['ecg_reading'])\ny = df['ecg_reading']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['patient_id'].fillna(df['patient_id'].median(), inplace=True)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'TotalCharges', then display feature importances from the Random Forest model, then clean text data in column 'ContractType' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf = pd.get_dummies(df, columns=['TotalCharges'], prefix=['TotalCharges'])\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['ContractType_clean'] = df['ContractType'].apply(clean)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'text', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then create a new feature 'sensor_value_ratio' as the ratio of 'sensor_value' to 'sensor_value', then normalize the 'sensor_value' column using min-max scaling, then one-hot encode the categorical column 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['sensor_value_ratio'] = df['sensor_value'] / df['sensor_value']\ndf['sensor_value_scaled'] = (df['sensor_value'] - df['sensor_value'].min()) / (df['sensor_value'].max() - df['sensor_value'].min())\ndf = pd.get_dummies(df, columns=['sensor_value'], prefix=['sensor_value'])"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then display feature importances from the Random Forest model, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nprint(df.describe())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then normalize the 'Product' column using min-max scaling, then plot a histogram of 'Product', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Product_scaled'] = (df['Product'] - df['Product'].min()) / (df['Product'].max() - df['Product'].min())\ndf['Product'].hist()\nplt.xlabel('Product')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Revenue'])\ny = df['Revenue']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then compute TF-IDF features for column 'Age' and display top 10 words, then create a new feature 'Sex_ratio' as the ratio of 'Sex' to 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Age'])\nprint(vect.get_feature_names_out())\ndf['Sex_ratio'] = df['Sex'] / df['Survived']"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then normalize the 'date' column using min-max scaling, then calculate the correlation matrix for numeric features, then clean text data in column 'date' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['date_scaled'] = (df['date'] - df['date'].min()) / (df['date'].max() - df['date'].min())\ncorr = df.corr()\nprint(corr)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'pm2_5', then handle missing values in 'no2' by imputing with median, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['no2'].fillna(df['no2'].median(), inplace=True)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'petal_width' and display top 10 words, then display feature importances from the Random Forest model, then handle missing values in 'petal_width' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['petal_width'])\nprint(vect.get_feature_names_out())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['petal_width'].fillna(df['petal_width'].median(), inplace=True)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then detect outliers in 'Age' using the IQR method, then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'Fare'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nQ1 = df['Age'].quantile(0.25)\nQ3 = df['Age'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Age'] < (Q1 - 1.5*IQR)) | (df['Age'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['Fare'], prefix=['Fare'])"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then clean text data in column 'sepal_length' by removing punctuation and stopwords, then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sepal_length_clean'] = df['sepal_length'].apply(clean)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'date', then create a new feature 'humidity_ratio' as the ratio of 'humidity' to 'temperature', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['date'].hist()\nplt.xlabel('date')\nplt.ylabel('Frequency')\nplt.show()\ndf['humidity_ratio'] = df['humidity'] / df['temperature']\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'SalePrice', then handle missing values in 'LotArea' by imputing with median, then one-hot encode the categorical column 'YearBuilt'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['LotArea'].fillna(df['LotArea'].median(), inplace=True)\ndf = pd.get_dummies(df, columns=['YearBuilt'], prefix=['YearBuilt'])"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then detect outliers in 'ContractType' using the IQR method, then clean text data in column 'MonthlyCharges' by removing punctuation and stopwords, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nQ1 = df['ContractType'].quantile(0.25)\nQ3 = df['ContractType'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['ContractType'] < (Q1 - 1.5*IQR)) | (df['ContractType'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['MonthlyCharges_clean'] = df['MonthlyCharges'].apply(clean)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'ecg_reading', then one-hot encode the categorical column 'patient_id', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['patient_id'], prefix=['patient_id'])\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then normalize the 'pressure' column using min-max scaling, then display summary statistics of all numeric columns using df.describe(), then one-hot encode the categorical column 'humidity'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['pressure_scaled'] = (df['pressure'] - df['pressure'].min()) / (df['pressure'].max() - df['pressure'].min())\nprint(df.describe())\ndf = pd.get_dummies(df, columns=['humidity'], prefix=['humidity'])"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then create a new feature 'amount_ratio' as the ratio of 'amount' to 'isFraud', then detect outliers in 'isFraud' using the IQR method, then clean text data in column 'oldbalanceOrg' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['amount_ratio'] = df['amount'] / df['isFraud']\nQ1 = df['isFraud'].quantile(0.25)\nQ3 = df['isFraud'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['isFraud'] < (Q1 - 1.5*IQR)) | (df['isFraud'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['oldbalanceOrg_clean'] = df['oldbalanceOrg'].apply(clean)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'heart_rate', then normalize the 'heart_rate' column using min-max scaling, then compute TF-IDF features for column 'time' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['heart_rate'].hist()\nplt.xlabel('heart_rate')\nplt.ylabel('Frequency')\nplt.show()\ndf['heart_rate_scaled'] = (df['heart_rate'] - df['heart_rate'].min()) / (df['heart_rate'].max() - df['heart_rate'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['time'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then create a new feature 'temp_ratio' as the ratio of 'temp' to 'traffic_volume', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['temp_ratio'] = df['temp'] / df['traffic_volume']\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['rain_1h'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then normalize the 'isFraud' column using min-max scaling, then display summary statistics of all numeric columns using df.describe(), then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['isFraud_scaled'] = (df['isFraud'] - df['isFraud'].min()) / (df['isFraud'].max() - df['isFraud'].min())\nprint(df.describe())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then create a new feature 'time_ratio' as the ratio of 'time' to 'ecg_reading', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['time_ratio'] = df['time'] / df['ecg_reading']\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['ecg_reading'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then normalize the 'review' column using min-max scaling, then display feature importances from the Random Forest model, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['review_scaled'] = (df['review'] - df['review'].min()) / (df['review'].max() - df['review'].min())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nprint(df.describe())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then normalize the 'so2' column using min-max scaling, then perform time-series forecasting using ARIMA to predict the next 12 periods, then handle missing values in 'date' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['so2_scaled'] = (df['so2'] - df['so2'].min()) / (df['so2'].max() - df['so2'].min())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['pm2_5'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['date'].fillna(df['date'].median(), inplace=True)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'sentiment', then plot a histogram of 'review'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['review'].hist()\nplt.xlabel('review')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then normalize the 'amount' column using min-max scaling, then clean text data in column 'newbalanceOrig' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['amount_scaled'] = (df['amount'] - df['amount'].min()) / (df['amount'].max() - df['amount'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['newbalanceOrig_clean'] = df['newbalanceOrig'].apply(clean)\nprint(df.describe())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods, then handle missing values in 'tenure' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Churn'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['tenure'].fillna(df['tenure'].median(), inplace=True)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then plot a histogram of 'sensor_value', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ncorr = df.corr()\nprint(corr)\ndf['sensor_value'].hist()\nplt.xlabel('sensor_value')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then normalize the 'Age' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Age_scaled'] = (df['Age'] - df['Age'].min()) / (df['Age'].max() - df['Age'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Age'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display summary statistics of all numeric columns using df.describe(), then clean text data in column 'tenure' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['ContractType'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nprint(df.describe())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['tenure_clean'] = df['tenure'].apply(clean)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'isFraud', then perform K-Means clustering with k=3 on numeric features, then plot a histogram of 'amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['amount'].hist()\nplt.xlabel('amount')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then create a new feature 'transaction_id_ratio' as the ratio of 'transaction_id' to 'total_amount', then display feature importances from the Random Forest model, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['transaction_id_ratio'] = df['transaction_id'] / df['total_amount']\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nprint(df.describe())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['wind_speed'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then handle missing values in 'recovered' by imputing with median, then display summary statistics of all numeric columns using df.describe(), then train a Random Forest Classifier to predict 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['recovered'].fillna(df['recovered'].median(), inplace=True)\nprint(df.describe())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then create a new feature 'Sex_ratio' as the ratio of 'Sex' to 'Survived', then handle missing values in 'Sex' by imputing with median, then detect outliers in 'Pclass' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Sex_ratio'] = df['Sex'] / df['Survived']\ndf['Sex'].fillna(df['Sex'].median(), inplace=True)\nQ1 = df['Pclass'].quantile(0.25)\nQ3 = df['Pclass'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Pclass'] < (Q1 - 1.5*IQR)) | (df['Pclass'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then clean text data in column 'rating' by removing punctuation and stopwords, then compute TF-IDF features for column 'rating' and display top 10 words, then train a Random Forest Classifier to predict 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['rating_clean'] = df['rating'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['rating'])\nprint(vect.get_feature_names_out())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Close', then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'text', then handle missing values in 'post_id' by imputing with median, then train a Linear Regression model to predict 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['post_id'].fillna(df['post_id'].median(), inplace=True)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'genre' column using min-max scaling, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['genre_scaled'] = (df['genre'] - df['genre'].min()) / (df['genre'].max() - df['genre'].min())\nprint(df.describe())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'temperature', then calculate the correlation matrix for numeric features, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then create a new feature 'text_ratio' as the ratio of 'text' to 'text', then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['text_ratio'] = df['text'] / df['text']\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then split the data into training and testing sets with an 80-20 split, then create a new feature 'product_id_ratio' as the ratio of 'product_id' to 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['product_id_ratio'] = df['product_id'] / df['total_amount']"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then create a new feature 'length_ratio' as the ratio of 'length' to 'sentiment', then perform time-series forecasting using ARIMA to predict the next 12 periods, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['length_ratio'] = df['length'] / df['sentiment']\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['review'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'device_id' using the IQR method, then train a Random Forest Classifier to predict 'sensor_value', then compute TF-IDF features for column 'timestamp' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nQ1 = df['device_id'].quantile(0.25)\nQ3 = df['device_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['device_id'] < (Q1 - 1.5*IQR)) | (df['device_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['timestamp'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'temperature', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then plot a histogram of 'date', then one-hot encode the categorical column 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['wind_speed'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['date'].hist()\nplt.xlabel('date')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['temperature'], prefix=['temperature'])"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then detect outliers in 'traffic_volume' using the IQR method, then train a Linear Regression model to predict 'traffic_volume', then one-hot encode the categorical column 'rain_1h'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nQ1 = df['traffic_volume'].quantile(0.25)\nQ3 = df['traffic_volume'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['traffic_volume'] < (Q1 - 1.5*IQR)) | (df['traffic_volume'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['rain_1h'], prefix=['rain_1h'])"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then clean text data in column 'carrier' by removing punctuation and stopwords, then display feature importances from the Random Forest model, then detect outliers in 'departure_delay' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['carrier_clean'] = df['carrier'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['departure_delay'].quantile(0.25)\nQ3 = df['departure_delay'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['departure_delay'] < (Q1 - 1.5*IQR)) | (df['departure_delay'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features, then normalize the 'status' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)\ndf['status_scaled'] = (df['status'] - df['status'].min()) / (df['status'].max() - df['status'].min())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'MonthlyCharges', then clean text data in column 'tenure' by removing punctuation and stopwords, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf = pd.get_dummies(df, columns=['MonthlyCharges'], prefix=['MonthlyCharges'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['tenure_clean'] = df['tenure'].apply(clean)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe(), then one-hot encode the categorical column 'date'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())\ndf = pd.get_dummies(df, columns=['date'], prefix=['date'])"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then detect outliers in 'ContractType' using the IQR method, then compute TF-IDF features for column 'MonthlyCharges' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['ContractType'].quantile(0.25)\nQ3 = df['ContractType'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['ContractType'] < (Q1 - 1.5*IQR)) | (df['ContractType'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['MonthlyCharges'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'amount' and display top 10 words, then clean text data in column 'newbalanceOrig' by removing punctuation and stopwords, then train a Random Forest Classifier to predict 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['amount'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['newbalanceOrig_clean'] = df['newbalanceOrig'].apply(clean)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then normalize the 'rating' column using min-max scaling, then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'genre_ratio' as the ratio of 'genre' to 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['rating_scaled'] = (df['rating'] - df['rating'].min()) / (df['rating'].max() - df['rating'].min())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['review'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['genre_ratio'] = df['genre'] / df['sentiment']"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then detect outliers in 'traffic_volume' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'rain_1h' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nQ1 = df['traffic_volume'].quantile(0.25)\nQ3 = df['traffic_volume'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['traffic_volume'] < (Q1 - 1.5*IQR)) | (df['traffic_volume'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['snow_1h'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['rain_1h_scaled'] = (df['rain_1h'] - df['rain_1h'].min()) / (df['rain_1h'].max() - df['rain_1h'].min())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'Volume_ratio' as the ratio of 'Volume' to 'Close', then one-hot encode the categorical column 'Low'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Volume'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['Volume_ratio'] = df['Volume'] / df['Close']\ndf = pd.get_dummies(df, columns=['Low'], prefix=['Low'])"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'text', then display summary statistics of all numeric columns using df.describe(), then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nprint(df.describe())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['likes'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['wind_speed'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then plot a histogram of 'sentiment', then calculate the correlation matrix for numeric features, then create a new feature 'genre_ratio' as the ratio of 'genre' to 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['sentiment'].hist()\nplt.xlabel('sentiment')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)\ndf['genre_ratio'] = df['genre'] / df['sentiment']"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then normalize the 'rain_1h' column using min-max scaling, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ncorr = df.corr()\nprint(corr)\ndf['rain_1h_scaled'] = (df['rain_1h'] - df['rain_1h'].min()) / (df['rain_1h'].max() - df['rain_1h'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then clean text data in column 'pm10' by removing punctuation and stopwords, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ncorr = df.corr()\nprint(corr)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['pm10_clean'] = df['pm10'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'date' column using min-max scaling, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['date_scaled'] = (df['date'] - df['date'].min()) / (df['date'].max() - df['date'].min())\nprint(df.describe())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'customers' by imputing with median, then one-hot encode the categorical column 'customers'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nprint(df.describe())\ndf['customers'].fillna(df['customers'].median(), inplace=True)\ndf = pd.get_dummies(df, columns=['customers'], prefix=['customers'])"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Revenue', then clean text data in column 'Product' by removing punctuation and stopwords, then handle missing values in 'Product' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Product_clean'] = df['Product'].apply(clean)\ndf['Product'].fillna(df['Product'].median(), inplace=True)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then detect outliers in 'departure_delay' using the IQR method, then train a Linear Regression model to predict 'arrival_delay', then train a Random Forest Classifier to predict 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nQ1 = df['departure_delay'].quantile(0.25)\nQ3 = df['departure_delay'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['departure_delay'] < (Q1 - 1.5*IQR)) | (df['departure_delay'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'deaths' and display top 10 words, then display summary statistics of all numeric columns using df.describe(), then create a new feature 'confirmed_ratio' as the ratio of 'confirmed' to 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['deaths'])\nprint(vect.get_feature_names_out())\nprint(df.describe())\ndf['confirmed_ratio'] = df['confirmed'] / df['confirmed']"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then train a Linear Regression model to predict 'species', then clean text data in column 'petal_length' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['petal_length_clean'] = df['petal_length'].apply(clean)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then clean text data in column 'Low' by removing punctuation and stopwords, then plot a histogram of 'Close', then compute TF-IDF features for column 'Low' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Low_clean'] = df['Low'].apply(clean)\ndf['Close'].hist()\nplt.xlabel('Close')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Low'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then clean text data in column 'tenure' by removing punctuation and stopwords, then plot a histogram of 'ContractType', then handle missing values in 'Churn' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['tenure_clean'] = df['tenure'].apply(clean)\ndf['ContractType'].hist()\nplt.xlabel('ContractType')\nplt.ylabel('Frequency')\nplt.show()\ndf['Churn'].fillna(df['Churn'].median(), inplace=True)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'newbalanceOrig', then normalize the 'oldbalanceOrg' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf = pd.get_dummies(df, columns=['newbalanceOrig'], prefix=['newbalanceOrig'])\ndf['oldbalanceOrg_scaled'] = (df['oldbalanceOrg'] - df['oldbalanceOrg'].min()) / (df['oldbalanceOrg'].max() - df['oldbalanceOrg'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then clean text data in column 'Neighborhood' by removing punctuation and stopwords, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nprint(df.describe())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Neighborhood_clean'] = df['Neighborhood'].apply(clean)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Revenue'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then normalize the 'petal_width' column using min-max scaling, then compute TF-IDF features for column 'sepal_length' and display top 10 words, then create a new feature 'species_ratio' as the ratio of 'species' to 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['petal_width_scaled'] = (df['petal_width'] - df['petal_width'].min()) / (df['petal_width'].max() - df['petal_width'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sepal_length'])\nprint(vect.get_feature_names_out())\ndf['species_ratio'] = df['species'] / df['species']"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then handle missing values in 'SalePrice' by imputing with median, then detect outliers in 'YearBuilt' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['SalePrice'].fillna(df['SalePrice'].median(), inplace=True)\nQ1 = df['YearBuilt'].quantile(0.25)\nQ3 = df['YearBuilt'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['YearBuilt'] < (Q1 - 1.5*IQR)) | (df['YearBuilt'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then create a new feature 'precipitation_ratio' as the ratio of 'precipitation' to 'temperature', then train a Random Forest Classifier to predict 'temperature', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['precipitation_ratio'] = df['precipitation'] / df['temperature']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then normalize the 'YearBuilt' column using min-max scaling, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ncorr = df.corr()\nprint(corr)\ndf['YearBuilt_scaled'] = (df['YearBuilt'] - df['YearBuilt'].min()) / (df['YearBuilt'].max() - df['YearBuilt'].min())\nprint(df.describe())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then calculate the correlation matrix for numeric features, then clean text data in column 'country' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nprint(df.describe())\ncorr = df.corr()\nprint(corr)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['country_clean'] = df['country'].apply(clean)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sentiment', then clean text data in column 'genre' by removing punctuation and stopwords, then create a new feature 'rating_ratio' as the ratio of 'rating' to 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['genre_clean'] = df['genre'].apply(clean)\ndf['rating_ratio'] = df['rating'] / df['sentiment']"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then split the data into training and testing sets with an 80-20 split, then create a new feature 'Sex_ratio' as the ratio of 'Sex' to 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['Sex_ratio'] = df['Sex'] / df['Survived']"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'sensor_value', then compute TF-IDF features for column 'timestamp' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['timestamp'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['timestamp'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'isFraud', then one-hot encode the categorical column 'newbalanceOrig', then plot a histogram of 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['newbalanceOrig'], prefix=['newbalanceOrig'])\ndf['isFraud'].hist()\nplt.xlabel('isFraud')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['temperature'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then clean text data in column 'isFraud' by removing punctuation and stopwords, then perform time-series forecasting using ARIMA to predict the next 12 periods, then one-hot encode the categorical column 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['isFraud_clean'] = df['isFraud'].apply(clean)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['amount'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf = pd.get_dummies(df, columns=['isFraud'], prefix=['isFraud'])"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'date' and display top 10 words, then normalize the 'consumption' column using min-max scaling, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date'])\nprint(vect.get_feature_names_out())\ndf['consumption_scaled'] = (df['consumption'] - df['consumption'].min()) / (df['consumption'].max() - df['consumption'].min())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'deaths', then display summary statistics of all numeric columns using df.describe(), then create a new feature 'recovered_ratio' as the ratio of 'recovered' to 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf = pd.get_dummies(df, columns=['deaths'], prefix=['deaths'])\nprint(df.describe())\ndf['recovered_ratio'] = df['recovered'] / df['confirmed']"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then create a new feature 'genre_ratio' as the ratio of 'genre' to 'sentiment', then display feature importances from the Random Forest model, then detect outliers in 'sentiment' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['genre_ratio'] = df['genre'] / df['sentiment']\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['sentiment'].quantile(0.25)\nQ3 = df['sentiment'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sentiment'] < (Q1 - 1.5*IQR)) | (df['sentiment'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'distance' and display top 10 words, then perform K-Means clustering with k=3 on numeric features, then train a Linear Regression model to predict 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['distance'])\nprint(vect.get_feature_names_out())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display summary statistics of all numeric columns using df.describe(), then one-hot encode the categorical column 'time'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['time'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nprint(df.describe())\ndf = pd.get_dummies(df, columns=['time'], prefix=['time'])"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then handle missing values in 'open' by imputing with median, then normalize the 'day_of_week' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['open'].fillna(df['open'].median(), inplace=True)\ndf['day_of_week_scaled'] = (df['day_of_week'] - df['day_of_week'].min()) / (df['day_of_week'].max() - df['day_of_week'].min())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'oldbalanceOrg' using the IQR method, then compute TF-IDF features for column 'newbalanceOrig' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nprint(df.describe())\nQ1 = df['oldbalanceOrg'].quantile(0.25)\nQ3 = df['oldbalanceOrg'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['oldbalanceOrg'] < (Q1 - 1.5*IQR)) | (df['oldbalanceOrg'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['newbalanceOrig'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then create a new feature 'petal_length_ratio' as the ratio of 'petal_length' to 'species', then train a Linear Regression model to predict 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nprint(df.describe())\ndf['petal_length_ratio'] = df['petal_length'] / df['species']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Sex', then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Random Forest Classifier to predict 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Sex'].hist()\nplt.xlabel('Sex')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Sex'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'SalePrice', then one-hot encode the categorical column 'OverallQual', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['OverallQual'], prefix=['OverallQual'])\nprint(df.describe())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'pressure' and display top 10 words, then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'pressure_ratio' as the ratio of 'pressure' to 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['pressure'])\nprint(vect.get_feature_names_out())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['pressure'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['pressure_ratio'] = df['pressure'] / df['consumption']"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Fare', then display feature importances from the Random Forest model, then create a new feature 'Age_ratio' as the ratio of 'Age' to 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Fare'].hist()\nplt.xlabel('Fare')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['Age_ratio'] = df['Age'] / df['Survived']"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then clean text data in column 'timestamp' by removing punctuation and stopwords, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nprint(df.describe())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['timestamp_clean'] = df['timestamp'].apply(clean)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'age', then evaluate the model performance using RMSE and R\u00b2 score, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf = pd.get_dummies(df, columns=['age'], prefix=['age'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['education'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then normalize the 'sepal_length' column using min-max scaling, then train a Random Forest Classifier to predict 'species', then handle missing values in 'petal_width' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['sepal_length_scaled'] = (df['sepal_length'] - df['sepal_length'].min()) / (df['sepal_length'].max() - df['sepal_length'].min())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['petal_width'].fillna(df['petal_width'].median(), inplace=True)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then plot a histogram of 'SalePrice', then compute TF-IDF features for column 'OverallQual' and display top 10 words, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['SalePrice'].hist()\nplt.xlabel('SalePrice')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['OverallQual'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then perform time-series forecasting using ARIMA to predict the next 12 periods, then plot a histogram of 'TotalCharges'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['tenure'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['TotalCharges'].hist()\nplt.xlabel('TotalCharges')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then create a new feature 'consumption_ratio' as the ratio of 'consumption' to 'consumption', then compute TF-IDF features for column 'date' and display top 10 words, then handle missing values in 'temperature' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['consumption_ratio'] = df['consumption'] / df['consumption']\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date'])\nprint(vect.get_feature_names_out())\ndf['temperature'].fillna(df['temperature'].median(), inplace=True)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'total_amount', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['product_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then clean text data in column 'country' by removing punctuation and stopwords, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nprint(df.describe())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['country_clean'] = df['country'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'LotArea' and display top 10 words, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'Neighborhood' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['LotArea'])\nprint(vect.get_feature_names_out())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['SalePrice'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['Neighborhood_scaled'] = (df['Neighborhood'] - df['Neighborhood'].min()) / (df['Neighborhood'].max() - df['Neighborhood'].min())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'Region' and display top 10 words, then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Region'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Revenue'])\ny = df['Revenue']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Close', then plot a histogram of 'Close', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['Close'].hist()\nplt.xlabel('Close')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then handle missing values in 'arrival_delay' by imputing with median, then normalize the 'arrival_delay' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ncorr = df.corr()\nprint(corr)\ndf['arrival_delay'].fillna(df['arrival_delay'].median(), inplace=True)\ndf['arrival_delay_scaled'] = (df['arrival_delay'] - df['arrival_delay'].min()) / (df['arrival_delay'].max() - df['arrival_delay'].min())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then handle missing values in 'customers' by imputing with median, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['customers'].fillna(df['customers'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe(), then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then one-hot encode the categorical column 'OverallQual', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf = pd.get_dummies(df, columns=['OverallQual'], prefix=['OverallQual'])\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then display feature importances from the Random Forest model, then clean text data in column 'Churn' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Churn_clean'] = df['Churn'].apply(clean)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'Survived', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then create a new feature 'day_of_week_ratio' as the ratio of 'day_of_week' to 'sales', then detect outliers in 'customers' using the IQR method, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['day_of_week_ratio'] = df['day_of_week'] / df['sales']\nQ1 = df['customers'].quantile(0.25)\nQ3 = df['customers'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['customers'] < (Q1 - 1.5*IQR)) | (df['customers'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then clean text data in column 'Date' by removing punctuation and stopwords, then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'Date'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Date_clean'] = df['Date'].apply(clean)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['Date'], prefix=['Date'])"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then normalize the 'tenure' column using min-max scaling, then plot a histogram of 'ContractType', then train a Random Forest Classifier to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['tenure_scaled'] = (df['tenure'] - df['tenure'].min()) / (df['tenure'].max() - df['tenure'].min())\ndf['ContractType'].hist()\nplt.xlabel('ContractType')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then create a new feature 'UnitsSold_ratio' as the ratio of 'UnitsSold' to 'Revenue', then one-hot encode the categorical column 'Date', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['UnitsSold_ratio'] = df['UnitsSold'] / df['Revenue']\ndf = pd.get_dummies(df, columns=['Date'], prefix=['Date'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'arrival_delay', then plot a histogram of 'departure_delay', then normalize the 'flight' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['departure_delay'].hist()\nplt.xlabel('departure_delay')\nplt.ylabel('Frequency')\nplt.show()\ndf['flight_scaled'] = (df['flight'] - df['flight'].min()) / (df['flight'].max() - df['flight'].min())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then plot a histogram of 'day_of_week', then one-hot encode the categorical column 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['day_of_week'].hist()\nplt.xlabel('day_of_week')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['sales'], prefix=['sales'])"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'customers' and display top 10 words, then display summary statistics of all numeric columns using df.describe(), then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['customers'])\nprint(vect.get_feature_names_out())\nprint(df.describe())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Churn', then create a new feature 'TotalCharges_ratio' as the ratio of 'TotalCharges' to 'Churn', then plot a histogram of 'tenure'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf = pd.get_dummies(df, columns=['Churn'], prefix=['Churn'])\ndf['TotalCharges_ratio'] = df['TotalCharges'] / df['Churn']\ndf['tenure'].hist()\nplt.xlabel('tenure')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sensor_value', then perform time-series forecasting using ARIMA to predict the next 12 periods, then clean text data in column 'location' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['location'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['location_clean'] = df['location'].apply(clean)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sensor_value', then perform K-Means clustering with k=3 on numeric features, then clean text data in column 'sensor_value' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sensor_value_clean'] = df['sensor_value'].apply(clean)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then display summary statistics of all numeric columns using df.describe(), then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nprint(df.describe())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then detect outliers in 'oldbalanceOrg' using the IQR method, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nQ1 = df['oldbalanceOrg'].quantile(0.25)\nQ3 = df['oldbalanceOrg'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['oldbalanceOrg'] < (Q1 - 1.5*IQR)) | (df['oldbalanceOrg'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then perform time-series forecasting using ARIMA to predict the next 12 periods, then compute TF-IDF features for column 'isFraud' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['time'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['isFraud'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'confirmed', then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then handle missing values in 'consumption' by imputing with median, then normalize the 'consumption' column using min-max scaling, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['consumption'].fillna(df['consumption'].median(), inplace=True)\ndf['consumption_scaled'] = (df['consumption'] - df['consumption'].min()) / (df['consumption'].max() - df['consumption'].min())\nprint(df.describe())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sales', then train a Random Forest Classifier to predict 'sales', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'timestamp' by imputing with median, then normalize the 'status' column using min-max scaling, then create a new feature 'timestamp_ratio' as the ratio of 'timestamp' to 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['timestamp'].fillna(df['timestamp'].median(), inplace=True)\ndf['status_scaled'] = (df['status'] - df['status'].min()) / (df['status'].max() - df['status'].min())\ndf['timestamp_ratio'] = df['timestamp'] / df['sensor_value']"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then compute TF-IDF features for column 'pm10' and display top 10 words, then clean text data in column 'o3' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['pm10'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['o3_clean'] = df['o3'].apply(clean)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Churn', then evaluate the model performance using RMSE and R\u00b2 score, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'petal_length' by imputing with median, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nprint(df.describe())\ndf['petal_length'].fillna(df['petal_length'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'total_amount', then create a new feature 'customer_id_ratio' as the ratio of 'customer_id' to 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['customer_id_ratio'] = df['customer_id'] / df['total_amount']"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'education' and display top 10 words, then plot a histogram of 'age', then normalize the 'education' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['education'])\nprint(vect.get_feature_names_out())\ndf['age'].hist()\nplt.xlabel('age')\nplt.ylabel('Frequency')\nplt.show()\ndf['education_scaled'] = (df['education'] - df['education'].min()) / (df['education'].max() - df['education'].min())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'deaths', then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf = pd.get_dummies(df, columns=['deaths'], prefix=['deaths'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Pclass'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model, then handle missing values in 'LotArea' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['LotArea'].fillna(df['LotArea'].median(), inplace=True)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then handle missing values in 'age' by imputing with median, then train a Random Forest Classifier to predict 'y', then create a new feature 'y_ratio' as the ratio of 'y' to 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['age'].fillna(df['age'].median(), inplace=True)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['y_ratio'] = df['y'] / df['y']"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'patient_id', then normalize the 'ecg_reading' column using min-max scaling, then detect outliers in 'quality' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf = pd.get_dummies(df, columns=['patient_id'], prefix=['patient_id'])\ndf['ecg_reading_scaled'] = (df['ecg_reading'] - df['ecg_reading'].min()) / (df['ecg_reading'].max() - df['ecg_reading'].min())\nQ1 = df['quality'].quantile(0.25)\nQ3 = df['quality'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['quality'] < (Q1 - 1.5*IQR)) | (df['quality'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then clean text data in column 'quantity' by removing punctuation and stopwords, then train a Linear Regression model to predict 'total_amount', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['quantity_clean'] = df['quantity'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then plot a histogram of 'no2', then train a Random Forest Classifier to predict 'pm2_5', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['no2'].hist()\nplt.xlabel('no2')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'date' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Close', then compute TF-IDF features for column 'Low' and display top 10 words, then one-hot encode the categorical column 'High'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Low'])\nprint(vect.get_feature_names_out())\ndf = pd.get_dummies(df, columns=['High'], prefix=['High'])"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then normalize the 'Churn' column using min-max scaling, then split the data into training and testing sets with an 80-20 split, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['Churn_scaled'] = (df['Churn'] - df['Churn'].min()) / (df['Churn'].max() - df['Churn'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then normalize the 'species' column using min-max scaling, then detect outliers in 'petal_width' using the IQR method, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['species_scaled'] = (df['species'] - df['species'].min()) / (df['species'].max() - df['species'].min())\nQ1 = df['petal_width'].quantile(0.25)\nQ3 = df['petal_width'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['petal_width'] < (Q1 - 1.5*IQR)) | (df['petal_width'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nprint(df.describe())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Volume', then compute TF-IDF features for column 'Open' and display top 10 words, then handle missing values in 'High' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf = pd.get_dummies(df, columns=['Volume'], prefix=['Volume'])\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Open'])\nprint(vect.get_feature_names_out())\ndf['High'].fillna(df['High'].median(), inplace=True)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then normalize the 'MonthlyCharges' column using min-max scaling, then one-hot encode the categorical column 'Churn', then detect outliers in 'Churn' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['MonthlyCharges_scaled'] = (df['MonthlyCharges'] - df['MonthlyCharges'].min()) / (df['MonthlyCharges'].max() - df['MonthlyCharges'].min())\ndf = pd.get_dummies(df, columns=['Churn'], prefix=['Churn'])\nQ1 = df['Churn'].quantile(0.25)\nQ3 = df['Churn'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Churn'] < (Q1 - 1.5*IQR)) | (df['Churn'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then normalize the 'temperature' column using min-max scaling, then clean text data in column 'temperature' by removing punctuation and stopwords, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['temperature_scaled'] = (df['temperature'] - df['temperature'].min()) / (df['temperature'].max() - df['temperature'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['temperature_clean'] = df['temperature'].apply(clean)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then normalize the 'Sex' column using min-max scaling, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['Sex_scaled'] = (df['Sex'] - df['Sex'].min()) / (df['Sex'].max() - df['Sex'].min())\nprint(df.describe())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then create a new feature 'day_of_week_ratio' as the ratio of 'day_of_week' to 'sales', then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['day_of_week_ratio'] = df['day_of_week'] / df['sales']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then one-hot encode the categorical column 'education', then clean text data in column 'marital' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ncorr = df.corr()\nprint(corr)\ndf = pd.get_dummies(df, columns=['education'], prefix=['education'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['marital_clean'] = df['marital'].apply(clean)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'text', then train a Random Forest Classifier to predict 'text', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then handle missing values in 'High' by imputing with median, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Close'])\ny = df['Close']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['High'].fillna(df['High'].median(), inplace=True)\nprint(df.describe())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then one-hot encode the categorical column 'rating', then clean text data in column 'rating' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sentiment'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf = pd.get_dummies(df, columns=['rating'], prefix=['rating'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['rating_clean'] = df['rating'].apply(clean)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods, then clean text data in column 'High' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Low'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['High_clean'] = df['High'].apply(clean)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then clean text data in column 'Neighborhood' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'SalePrice' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Neighborhood_clean'] = df['Neighborhood'].apply(clean)\nprint(df.describe())\ndf['SalePrice'].fillna(df['SalePrice'].median(), inplace=True)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'text', then normalize the 'user_id' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['user_id_scaled'] = (df['user_id'] - df['user_id'].min()) / (df['user_id'].max() - df['user_id'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then plot a histogram of 'age', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['age'].hist()\nplt.xlabel('age')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then normalize the 'rain_1h' column using min-max scaling, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['rain_1h_scaled'] = (df['rain_1h'] - df['rain_1h'].min()) / (df['rain_1h'].max() - df['rain_1h'].min())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['temp'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then plot a histogram of 'SalePrice', then split the data into training and testing sets with an 80-20 split, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['SalePrice'].hist()\nplt.xlabel('SalePrice')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then compute TF-IDF features for column 'recovered' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['recovered'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then detect outliers in 'species' using the IQR method, then create a new feature 'species_ratio' as the ratio of 'species' to 'species', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nQ1 = df['species'].quantile(0.25)\nQ3 = df['species'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['species'] < (Q1 - 1.5*IQR)) | (df['species'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['species_ratio'] = df['species'] / df['species']\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'precipitation' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nQ1 = df['precipitation'].quantile(0.25)\nQ3 = df['precipitation'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['precipitation'] < (Q1 - 1.5*IQR)) | (df['precipitation'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['humidity'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nprint(df.describe())"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'Pclass' using the IQR method, then one-hot encode the categorical column 'Fare'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Pclass'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['Pclass'].quantile(0.25)\nQ3 = df['Pclass'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Pclass'] < (Q1 - 1.5*IQR)) | (df['Pclass'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['Fare'], prefix=['Fare'])"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then handle missing values in 'amount' by imputing with median, then one-hot encode the categorical column 'time', then plot a histogram of 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['amount'].fillna(df['amount'].median(), inplace=True)\ndf = pd.get_dummies(df, columns=['time'], prefix=['time'])\ndf['isFraud'].hist()\nplt.xlabel('isFraud')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'arrival_delay', then split the data into training and testing sets with an 80-20 split, then one-hot encode the categorical column 'carrier'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf = pd.get_dummies(df, columns=['carrier'], prefix=['carrier'])"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'genre', then create a new feature 'rating_ratio' as the ratio of 'rating' to 'sentiment', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf = pd.get_dummies(df, columns=['genre'], prefix=['genre'])\ndf['rating_ratio'] = df['rating'] / df['sentiment']\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sentiment'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then clean text data in column 'post_id' by removing punctuation and stopwords, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['post_id_clean'] = df['post_id'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then detect outliers in 'pm10' using the IQR method, then compute TF-IDF features for column 'date' and display top 10 words, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nQ1 = df['pm10'].quantile(0.25)\nQ3 = df['pm10'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['pm10'] < (Q1 - 1.5*IQR)) | (df['pm10'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date'])\nprint(vect.get_feature_names_out())\nprint(df.describe())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then create a new feature 'newbalanceOrig_ratio' as the ratio of 'newbalanceOrig' to 'isFraud', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ncorr = df.corr()\nprint(corr)\ndf['newbalanceOrig_ratio'] = df['newbalanceOrig'] / df['isFraud']\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Random Forest Classifier to predict 'isFraud', then train a Linear Regression model to predict 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nprint(df.describe())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe(), then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Close'])\ny = df['Close']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Low'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then plot a histogram of 'tenure', then one-hot encode the categorical column 'Churn', then create a new feature 'tenure_ratio' as the ratio of 'tenure' to 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['tenure'].hist()\nplt.xlabel('tenure')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['Churn'], prefix=['Churn'])\ndf['tenure_ratio'] = df['tenure'] / df['Churn']"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'confirmed', then display feature importances from the Random Forest model, then clean text data in column 'confirmed' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['confirmed_clean'] = df['confirmed'].apply(clean)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then detect outliers in 'marital' using the IQR method, then perform K-Means clustering with k=3 on numeric features, then normalize the 'age' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nQ1 = df['marital'].quantile(0.25)\nQ3 = df['marital'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['marital'] < (Q1 - 1.5*IQR)) | (df['marital'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['age_scaled'] = (df['age'] - df['age'].min()) / (df['age'].max() - df['age'].min())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'date' and display top 10 words, then perform time-series forecasting using ARIMA to predict the next 12 periods, then one-hot encode the categorical column 'precipitation'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date'])\nprint(vect.get_feature_names_out())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf = pd.get_dummies(df, columns=['precipitation'], prefix=['precipitation'])"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then detect outliers in 'shares' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nQ1 = df['shares'].quantile(0.25)\nQ3 = df['shares'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['shares'] < (Q1 - 1.5*IQR)) | (df['shares'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['text'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display feature importances from the Random Forest model, then one-hot encode the categorical column 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf = pd.get_dummies(df, columns=['confirmed'], prefix=['confirmed'])"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'temperature' and display top 10 words, then clean text data in column 'temperature' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['temperature'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['temperature_clean'] = df['temperature'].apply(clean)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display summary statistics of all numeric columns using df.describe(), then train a Linear Regression model to predict 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['departure_delay'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nprint(df.describe())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then perform time-series forecasting using ARIMA to predict the next 12 periods, then handle missing values in 'OverallQual' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['YearBuilt'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['OverallQual'].fillna(df['OverallQual'].median(), inplace=True)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then normalize the 'TotalCharges' column using min-max scaling, then plot a histogram of 'ContractType', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['TotalCharges_scaled'] = (df['TotalCharges'] - df['TotalCharges'].min()) / (df['TotalCharges'].max() - df['TotalCharges'].min())\ndf['ContractType'].hist()\nplt.xlabel('ContractType')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then create a new feature 'Revenue_ratio' as the ratio of 'Revenue' to 'Revenue', then train a Linear Regression model to predict 'Revenue', then train a Random Forest Classifier to predict 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Revenue_ratio'] = df['Revenue'] / df['Revenue']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'sales' and display top 10 words, then detect outliers in 'day_of_week' using the IQR method, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sales'])\nprint(vect.get_feature_names_out())\nQ1 = df['day_of_week'].quantile(0.25)\nQ3 = df['day_of_week'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['day_of_week'] < (Q1 - 1.5*IQR)) | (df['day_of_week'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then train a Linear Regression model to predict 'ecg_reading', then compute TF-IDF features for column 'heart_rate' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['heart_rate'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then compute TF-IDF features for column 'quality' and display top 10 words, then one-hot encode the categorical column 'heart_rate'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['quality'])\nprint(vect.get_feature_names_out())\ndf = pd.get_dummies(df, columns=['heart_rate'], prefix=['heart_rate'])"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'consumption', then display summary statistics of all numeric columns using df.describe(), then clean text data in column 'date' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sales', then detect outliers in 'store' using the IQR method, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nQ1 = df['store'].quantile(0.25)\nQ3 = df['store'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['store'] < (Q1 - 1.5*IQR)) | (df['store'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then detect outliers in 'post_id' using the IQR method, then train a Linear Regression model to predict 'text', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nQ1 = df['post_id'].quantile(0.25)\nQ3 = df['post_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['post_id'] < (Q1 - 1.5*IQR)) | (df['post_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'location' by imputing with median, then clean text data in column 'device_id' by removing punctuation and stopwords, then train a Random Forest Classifier to predict 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['location'].fillna(df['location'].median(), inplace=True)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['device_id_clean'] = df['device_id'].apply(clean)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then handle missing values in 'open' by imputing with median, then train a Random Forest Classifier to predict 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['open'].fillna(df['open'].median(), inplace=True)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then one-hot encode the categorical column 'Volume', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nprint(df.describe())\ndf = pd.get_dummies(df, columns=['Volume'], prefix=['Volume'])\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Volume'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'petal_width' column using min-max scaling, then handle missing values in 'petal_length' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sepal_width'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['petal_width_scaled'] = (df['petal_width'] - df['petal_width'].min()) / (df['petal_width'].max() - df['petal_width'].min())\ndf['petal_length'].fillna(df['petal_length'].median(), inplace=True)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sales', then normalize the 'sales' column using min-max scaling, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['sales_scaled'] = (df['sales'] - df['sales'].min()) / (df['sales'].max() - df['sales'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform K-Means clustering with k=3 on numeric features, then create a new feature 'High_ratio' as the ratio of 'High' to 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nprint(df.describe())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['High_ratio'] = df['High'] / df['Close']"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then clean text data in column 'temp' by removing punctuation and stopwords, then train a Linear Regression model to predict 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ncorr = df.corr()\nprint(corr)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['temp_clean'] = df['temp'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'user_id' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['likes'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['user_id'].quantile(0.25)\nQ3 = df['user_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['user_id'] < (Q1 - 1.5*IQR)) | (df['user_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'no2', then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf = pd.get_dummies(df, columns=['no2'], prefix=['no2'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'location' and display top 10 words, then plot a histogram of 'device_id', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['location'])\nprint(vect.get_feature_names_out())\ndf['device_id'].hist()\nplt.xlabel('device_id')\nplt.ylabel('Frequency')\nplt.show()\nprint(df.describe())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then clean text data in column 'pm10' by removing punctuation and stopwords, then train a Random Forest Classifier to predict 'pm2_5', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['pm10_clean'] = df['pm10'].apply(clean)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'text', then create a new feature 'shares_ratio' as the ratio of 'shares' to 'text', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['shares_ratio'] = df['shares'] / df['text']\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then detect outliers in 'so2' using the IQR method, then calculate the correlation matrix for numeric features, then normalize the 'so2' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nQ1 = df['so2'].quantile(0.25)\nQ3 = df['so2'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['so2'] < (Q1 - 1.5*IQR)) | (df['so2'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ncorr = df.corr()\nprint(corr)\ndf['so2_scaled'] = (df['so2'] - df['so2'].min()) / (df['so2'].max() - df['so2'].min())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then display feature importances from the Random Forest model, then train a Linear Regression model to predict 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'traffic_volume', then clean text data in column 'traffic_volume' by removing punctuation and stopwords, then create a new feature 'rain_1h_ratio' as the ratio of 'rain_1h' to 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['traffic_volume_clean'] = df['traffic_volume'].apply(clean)\ndf['rain_1h_ratio'] = df['rain_1h'] / df['traffic_volume']"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'UnitsSold', then split the data into training and testing sets with an 80-20 split, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf = pd.get_dummies(df, columns=['UnitsSold'], prefix=['UnitsSold'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Revenue'])\ny = df['Revenue']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then handle missing values in 'post_id' by imputing with median, then detect outliers in 'post_id' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ncorr = df.corr()\nprint(corr)\ndf['post_id'].fillna(df['post_id'].median(), inplace=True)\nQ1 = df['post_id'].quantile(0.25)\nQ3 = df['post_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['post_id'] < (Q1 - 1.5*IQR)) | (df['post_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'LotArea' using the IQR method, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['LotArea'].quantile(0.25)\nQ3 = df['LotArea'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['LotArea'] < (Q1 - 1.5*IQR)) | (df['LotArea'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'status' and display top 10 words, then one-hot encode the categorical column 'location', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['status'])\nprint(vect.get_feature_names_out())\ndf = pd.get_dummies(df, columns=['location'], prefix=['location'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then evaluate the model performance using RMSE and R\u00b2 score, then compute TF-IDF features for column 'date_time' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['rain_1h'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date_time'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'y', then clean text data in column 'y' by removing punctuation and stopwords, then train a Linear Regression model to predict 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['y_clean'] = df['y'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'heart_rate' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe(), then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['heart_rate_clean'] = df['heart_rate'].apply(clean)\nprint(df.describe())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'species', then detect outliers in 'petal_width' using the IQR method, then normalize the 'petal_length' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['petal_width'].quantile(0.25)\nQ3 = df['petal_width'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['petal_width'] < (Q1 - 1.5*IQR)) | (df['petal_width'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['petal_length_scaled'] = (df['petal_length'] - df['petal_length'].min()) / (df['petal_length'].max() - df['petal_length'].min())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'Revenue' using the IQR method, then compute TF-IDF features for column 'Revenue' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nQ1 = df['Revenue'].quantile(0.25)\nQ3 = df['Revenue'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Revenue'] < (Q1 - 1.5*IQR)) | (df['Revenue'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Revenue'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then display feature importances from the Random Forest model, then train a Linear Regression model to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'transaction_id' and display top 10 words, then plot a histogram of 'customer_id', then train a Random Forest Classifier to predict 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['transaction_id'])\nprint(vect.get_feature_names_out())\ndf['customer_id'].hist()\nplt.xlabel('customer_id')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then clean text data in column 'newbalanceOrig' by removing punctuation and stopwords, then handle missing values in 'time' by imputing with median, then normalize the 'newbalanceOrig' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['newbalanceOrig_clean'] = df['newbalanceOrig'].apply(clean)\ndf['time'].fillna(df['time'].median(), inplace=True)\ndf['newbalanceOrig_scaled'] = (df['newbalanceOrig'] - df['newbalanceOrig'].min()) / (df['newbalanceOrig'].max() - df['newbalanceOrig'].min())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then plot a histogram of 'arrival_delay', then one-hot encode the categorical column 'arrival_delay', then handle missing values in 'distance' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['arrival_delay'].hist()\nplt.xlabel('arrival_delay')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['arrival_delay'], prefix=['arrival_delay'])\ndf['distance'].fillna(df['distance'].median(), inplace=True)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then compute TF-IDF features for column 'temperature' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['humidity'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['temperature'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods, then handle missing values in 'SalePrice' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['SalePrice'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['SalePrice'].fillna(df['SalePrice'].median(), inplace=True)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then compute TF-IDF features for column 'species' and display top 10 words, then normalize the 'petal_width' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['species'])\nprint(vect.get_feature_names_out())\ndf['petal_width_scaled'] = (df['petal_width'] - df['petal_width'].min()) / (df['petal_width'].max() - df['petal_width'].min())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then detect outliers in 'sepal_width' using the IQR method, then perform K-Means clustering with k=3 on numeric features, then train a Linear Regression model to predict 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nQ1 = df['sepal_width'].quantile(0.25)\nQ3 = df['sepal_width'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sepal_width'] < (Q1 - 1.5*IQR)) | (df['sepal_width'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then normalize the 'customer_id' column using min-max scaling, then split the data into training and testing sets with an 80-20 split, then handle missing values in 'product_id' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['customer_id_scaled'] = (df['customer_id'] - df['customer_id'].min()) / (df['customer_id'].max() - df['customer_id'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['product_id'].fillna(df['product_id'].median(), inplace=True)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then train a Linear Regression model to predict 'arrival_delay', then compute TF-IDF features for column 'carrier' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['carrier'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then perform K-Means clustering with k=3 on numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['transaction_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Random Forest Classifier to predict 'pm2_5', then detect outliers in 'no2' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nprint(df.describe())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['no2'].quantile(0.25)\nQ3 = df['no2'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['no2'] < (Q1 - 1.5*IQR)) | (df['no2'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then handle missing values in 'product_id' by imputing with median, then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['product_id'].fillna(df['product_id'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then create a new feature 'carrier_ratio' as the ratio of 'carrier' to 'arrival_delay', then compute TF-IDF features for column 'flight' and display top 10 words, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['carrier_ratio'] = df['carrier'] / df['arrival_delay']\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['flight'])\nprint(vect.get_feature_names_out())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then normalize the 'SalePrice' column using min-max scaling, then plot a histogram of 'SalePrice'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['SalePrice_scaled'] = (df['SalePrice'] - df['SalePrice'].min()) / (df['SalePrice'].max() - df['SalePrice'].min())\ndf['SalePrice'].hist()\nplt.xlabel('SalePrice')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'petal_length_ratio' as the ratio of 'petal_length' to 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sepal_length'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['petal_length_ratio'] = df['petal_length'] / df['species']"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sensor_value', then normalize the 'sensor_value' column using min-max scaling, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['sensor_value_scaled'] = (df['sensor_value'] - df['sensor_value'].min()) / (df['sensor_value'].max() - df['sensor_value'].min())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then plot a histogram of 'day_of_week', then calculate the correlation matrix for numeric features, then clean text data in column 'customers' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['day_of_week'].hist()\nplt.xlabel('day_of_week')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['customers_clean'] = df['customers'].apply(clean)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'patient_id' by imputing with median, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nprint(df.describe())\ndf['patient_id'].fillna(df['patient_id'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['ecg_reading'])\ny = df['ecg_reading']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then handle missing values in 'time' by imputing with median, then train a Linear Regression model to predict 'isFraud', then one-hot encode the categorical column 'oldbalanceOrg'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['time'].fillna(df['time'].median(), inplace=True)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['oldbalanceOrg'], prefix=['oldbalanceOrg'])"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then perform K-Means clustering with k=3 on numeric features, then clean text data in column 'no2' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['no2_clean'] = df['no2'].apply(clean)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split, then compute TF-IDF features for column 'species' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['species'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'isFraud', then display feature importances from the Random Forest model, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then plot a histogram of 'Date', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['Date'].hist()\nplt.xlabel('Date')\nplt.ylabel('Frequency')\nplt.show()\nprint(df.describe())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then create a new feature 'Product_ratio' as the ratio of 'Product' to 'Revenue', then train a Linear Regression model to predict 'Revenue', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Product_ratio'] = df['Product'] / df['Revenue']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe(), then one-hot encode the categorical column 'length'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())\ndf = pd.get_dummies(df, columns=['length'], prefix=['length'])"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features, then normalize the 'temperature' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)\ndf['temperature_scaled'] = (df['temperature'] - df['temperature'].min()) / (df['temperature'].max() - df['temperature'].min())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'Revenue' by imputing with median, then train a Linear Regression model to predict 'Revenue', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Revenue'].fillna(df['Revenue'].median(), inplace=True)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then one-hot encode the categorical column 'High', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ncorr = df.corr()\nprint(corr)\ndf = pd.get_dummies(df, columns=['High'], prefix=['High'])\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Close'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then clean text data in column 'transaction_id' by removing punctuation and stopwords, then detect outliers in 'transaction_id' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['transaction_id_clean'] = df['transaction_id'].apply(clean)\nQ1 = df['transaction_id'].quantile(0.25)\nQ3 = df['transaction_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['transaction_id'] < (Q1 - 1.5*IQR)) | (df['transaction_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then plot a histogram of 'snow_1h', then detect outliers in 'traffic_volume' using the IQR method, then compute TF-IDF features for column 'temp' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['snow_1h'].hist()\nplt.xlabel('snow_1h')\nplt.ylabel('Frequency')\nplt.show()\nQ1 = df['traffic_volume'].quantile(0.25)\nQ3 = df['traffic_volume'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['traffic_volume'] < (Q1 - 1.5*IQR)) | (df['traffic_volume'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['temp'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Random Forest Classifier to predict 'consumption', then plot a histogram of 'humidity'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['humidity'].hist()\nplt.xlabel('humidity')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then handle missing values in 'genre' by imputing with median, then display summary statistics of all numeric columns using df.describe(), then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['genre'].fillna(df['genre'].median(), inplace=True)\nprint(df.describe())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'sales', then create a new feature 'store_ratio' as the ratio of 'store' to 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['store_ratio'] = df['store'] / df['sales']"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then one-hot encode the categorical column 'departure_delay', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nprint(df.describe())\ndf = pd.get_dummies(df, columns=['departure_delay'], prefix=['departure_delay'])\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['arrival_delay'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then create a new feature 'review_ratio' as the ratio of 'review' to 'sentiment', then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'review' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['review_ratio'] = df['review'] / df['sentiment']\nprint(df.describe())\nQ1 = df['review'].quantile(0.25)\nQ3 = df['review'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['review'] < (Q1 - 1.5*IQR)) | (df['review'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Random Forest Classifier to predict 'traffic_volume', then compute TF-IDF features for column 'traffic_volume' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['temp'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['traffic_volume'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then handle missing values in 'sepal_width' by imputing with median, then evaluate the model performance using RMSE and R\u00b2 score, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['sepal_width'].fillna(df['sepal_width'].median(), inplace=True)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'species', then calculate the correlation matrix for numeric features, then one-hot encode the categorical column 'petal_width'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)\ndf = pd.get_dummies(df, columns=['petal_width'], prefix=['petal_width'])"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then create a new feature 'sensor_value_ratio' as the ratio of 'sensor_value' to 'sensor_value', then one-hot encode the categorical column 'device_id', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['sensor_value_ratio'] = df['sensor_value'] / df['sensor_value']\ndf = pd.get_dummies(df, columns=['device_id'], prefix=['device_id'])\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'sensor_value', then normalize the 'device_id' column using min-max scaling, then train a Random Forest Classifier to predict 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf = pd.get_dummies(df, columns=['sensor_value'], prefix=['sensor_value'])\ndf['device_id_scaled'] = (df['device_id'] - df['device_id'].min()) / (df['device_id'].max() - df['device_id'].min())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then plot a histogram of 'sepal_width', then calculate the correlation matrix for numeric features, then detect outliers in 'sepal_width' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['sepal_width'].hist()\nplt.xlabel('sepal_width')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)\nQ1 = df['sepal_width'].quantile(0.25)\nQ3 = df['sepal_width'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sepal_width'] < (Q1 - 1.5*IQR)) | (df['sepal_width'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then handle missing values in 'consumption' by imputing with median, then create a new feature 'consumption_ratio' as the ratio of 'consumption' to 'consumption', then detect outliers in 'date' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['consumption'].fillna(df['consumption'].median(), inplace=True)\ndf['consumption_ratio'] = df['consumption'] / df['consumption']\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'sentiment', then normalize the 'sentiment' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['sentiment_scaled'] = (df['sentiment'] - df['sentiment'].min()) / (df['sentiment'].max() - df['sentiment'].min())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then detect outliers in 'length' using the IQR method, then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nQ1 = df['length'].quantile(0.25)\nQ3 = df['length'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['length'] < (Q1 - 1.5*IQR)) | (df['length'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['sentiment'], prefix=['sentiment'])"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then detect outliers in 'consumption' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'pressure_ratio' as the ratio of 'pressure' to 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nQ1 = df['consumption'].quantile(0.25)\nQ3 = df['consumption'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['consumption'] < (Q1 - 1.5*IQR)) | (df['consumption'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['consumption'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['pressure_ratio'] = df['pressure'] / df['consumption']"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then clean text data in column 'device_id' by removing punctuation and stopwords, then train a Random Forest Classifier to predict 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['location'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['device_id_clean'] = df['device_id'].apply(clean)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then perform K-Means clustering with k=3 on numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['location'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'text', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['text'])\ny = df['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'customers' using the IQR method, then one-hot encode the categorical column 'day_of_week'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['store'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['customers'].quantile(0.25)\nQ3 = df['customers'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['customers'] < (Q1 - 1.5*IQR)) | (df['customers'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['day_of_week'], prefix=['day_of_week'])"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then normalize the 'newbalanceOrig' column using min-max scaling, then one-hot encode the categorical column 'oldbalanceOrg'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['newbalanceOrig_scaled'] = (df['newbalanceOrig'] - df['newbalanceOrig'].min()) / (df['newbalanceOrig'].max() - df['newbalanceOrig'].min())\ndf = pd.get_dummies(df, columns=['oldbalanceOrg'], prefix=['oldbalanceOrg'])"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then create a new feature 'shares_ratio' as the ratio of 'shares' to 'text', then perform time-series forecasting using ARIMA to predict the next 12 periods, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['shares_ratio'] = df['shares'] / df['text']\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['user_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'time' and display top 10 words, then one-hot encode the categorical column 'newbalanceOrig', then detect outliers in 'amount' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['time'])\nprint(vect.get_feature_names_out())\ndf = pd.get_dummies(df, columns=['newbalanceOrig'], prefix=['newbalanceOrig'])\nQ1 = df['amount'].quantile(0.25)\nQ3 = df['amount'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['amount'] < (Q1 - 1.5*IQR)) | (df['amount'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then create a new feature 'confirmed_ratio' as the ratio of 'confirmed' to 'confirmed', then split the data into training and testing sets with an 80-20 split, then normalize the 'recovered' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['confirmed_ratio'] = df['confirmed'] / df['confirmed']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['recovered_scaled'] = (df['recovered'] - df['recovered'].min()) / (df['recovered'].max() - df['recovered'].min())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['snow_1h'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'total_amount' and display top 10 words, then split the data into training and testing sets with an 80-20 split, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['total_amount'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then handle missing values in 'rain_1h' by imputing with median, then create a new feature 'date_time_ratio' as the ratio of 'date_time' to 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ncorr = df.corr()\nprint(corr)\ndf['rain_1h'].fillna(df['rain_1h'].median(), inplace=True)\ndf['date_time_ratio'] = df['date_time'] / df['traffic_volume']"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'OverallQual' and display top 10 words, then create a new feature 'LotArea_ratio' as the ratio of 'LotArea' to 'SalePrice', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['OverallQual'])\nprint(vect.get_feature_names_out())\ndf['LotArea_ratio'] = df['LotArea'] / df['SalePrice']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'species', then perform time-series forecasting using ARIMA to predict the next 12 periods, then compute TF-IDF features for column 'petal_width' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['species'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['petal_width'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then create a new feature 'date_ratio' as the ratio of 'date' to 'consumption', then handle missing values in 'humidity' by imputing with median, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['date_ratio'] = df['date'] / df['consumption']\ndf['humidity'].fillna(df['humidity'].median(), inplace=True)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then normalize the 'pm2_5' column using min-max scaling, then compute TF-IDF features for column 'no2' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['pm2_5_scaled'] = (df['pm2_5'] - df['pm2_5'].min()) / (df['pm2_5'].max() - df['pm2_5'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['no2'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then create a new feature 'heart_rate_ratio' as the ratio of 'heart_rate' to 'ecg_reading', then clean text data in column 'quality' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['heart_rate_ratio'] = df['heart_rate'] / df['ecg_reading']\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['quality_clean'] = df['quality'].apply(clean)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then create a new feature 'Neighborhood_ratio' as the ratio of 'Neighborhood' to 'SalePrice', then perform K-Means clustering with k=3 on numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['Neighborhood_ratio'] = df['Neighborhood'] / df['SalePrice']\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['LotArea'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then compute TF-IDF features for column 'deaths' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['deaths'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then create a new feature 'date_ratio' as the ratio of 'date' to 'confirmed', then normalize the 'deaths' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['date_ratio'] = df['date'] / df['confirmed']\ndf['deaths_scaled'] = (df['deaths'] - df['deaths'].min()) / (df['deaths'].max() - df['deaths'].min())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'genre'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nprint(df.describe())\ndf['genre'].hist()\nplt.xlabel('genre')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then create a new feature 'Product_ratio' as the ratio of 'Product' to 'Revenue', then split the data into training and testing sets with an 80-20 split, then handle missing values in 'UnitsSold' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Product_ratio'] = df['Product'] / df['Revenue']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Revenue'])\ny = df['Revenue']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['UnitsSold'].fillna(df['UnitsSold'].median(), inplace=True)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Close', then handle missing values in 'Date' by imputing with median, then one-hot encode the categorical column 'Date'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['Date'].fillna(df['Date'].median(), inplace=True)\ndf = pd.get_dummies(df, columns=['Date'], prefix=['Date'])"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then create a new feature 'marital_ratio' as the ratio of 'marital' to 'y', then plot a histogram of 'age', then compute TF-IDF features for column 'age' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['marital_ratio'] = df['marital'] / df['y']\ndf['age'].hist()\nplt.xlabel('age')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['age'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then display summary statistics of all numeric columns using df.describe(), then compute TF-IDF features for column 'pressure' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ncorr = df.corr()\nprint(corr)\nprint(df.describe())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['pressure'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then handle missing values in 'recovered' by imputing with median, then perform K-Means clustering with k=3 on numeric features, then clean text data in column 'country' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['recovered'].fillna(df['recovered'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['country_clean'] = df['country'].apply(clean)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then handle missing values in 'Fare' by imputing with median, then create a new feature 'Sex_ratio' as the ratio of 'Sex' to 'Survived', then one-hot encode the categorical column 'Pclass'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Fare'].fillna(df['Fare'].median(), inplace=True)\ndf['Sex_ratio'] = df['Sex'] / df['Survived']\ndf = pd.get_dummies(df, columns=['Pclass'], prefix=['Pclass'])"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Close', then create a new feature 'Volume_ratio' as the ratio of 'Volume' to 'Close', then plot a histogram of 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['Volume_ratio'] = df['Volume'] / df['Close']\ndf['Close'].hist()\nplt.xlabel('Close')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then handle missing values in 'y' by imputing with median, then create a new feature 'age_ratio' as the ratio of 'age' to 'y', then clean text data in column 'marital' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['y'].fillna(df['y'].median(), inplace=True)\ndf['age_ratio'] = df['age'] / df['y']\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['marital_clean'] = df['marital'].apply(clean)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'sepal_length' and display top 10 words, then plot a histogram of 'sepal_length'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sepal_length'])\nprint(vect.get_feature_names_out())\ndf['sepal_length'].hist()\nplt.xlabel('sepal_length')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then clean text data in column 'total_amount' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe(), then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['total_amount_clean'] = df['total_amount'].apply(clean)\nprint(df.describe())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then clean text data in column 'departure_delay' by removing punctuation and stopwords, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'carrier' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['departure_delay_clean'] = df['departure_delay'].apply(clean)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['carrier_scaled'] = (df['carrier'] - df['carrier'].min()) / (df['carrier'].max() - df['carrier'].min())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then compute TF-IDF features for column 'traffic_volume' and display top 10 words, then create a new feature 'traffic_volume_ratio' as the ratio of 'traffic_volume' to 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['traffic_volume'])\nprint(vect.get_feature_names_out())\ndf['traffic_volume_ratio'] = df['traffic_volume'] / df['traffic_volume']"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'tenure' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score, then detect outliers in 'MonthlyCharges' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['tenure'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nQ1 = df['MonthlyCharges'].quantile(0.25)\nQ3 = df['MonthlyCharges'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['MonthlyCharges'] < (Q1 - 1.5*IQR)) | (df['MonthlyCharges'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then handle missing values in 'Survived' by imputing with median, then display feature importances from the Random Forest model, then one-hot encode the categorical column 'Sex'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Survived'].fillna(df['Survived'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf = pd.get_dummies(df, columns=['Sex'], prefix=['Sex'])"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then plot a histogram of 'likes', then perform time-series forecasting using ARIMA to predict the next 12 periods, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['likes'].hist()\nplt.xlabel('likes')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['likes'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'ecg_reading', then normalize the 'patient_id' column using min-max scaling, then compute TF-IDF features for column 'time' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['patient_id_scaled'] = (df['patient_id'] - df['patient_id'].min()) / (df['patient_id'].max() - df['patient_id'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['time'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then compute TF-IDF features for column 'quality' and display top 10 words, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['quality'])\nprint(vect.get_feature_names_out())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'consumption', then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['consumption'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then normalize the 'Survived' column using min-max scaling, then clean text data in column 'Pclass' by removing punctuation and stopwords, then train a Random Forest Classifier to predict 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Survived_scaled'] = (df['Survived'] - df['Survived'].min()) / (df['Survived'].max() - df['Survived'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Pclass_clean'] = df['Pclass'].apply(clean)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display feature importances from the Random Forest model, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nprint(df.describe())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'Revenue', then plot a histogram of 'Region'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['Region'].hist()\nplt.xlabel('Region')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then detect outliers in 'tenure' using the IQR method, then clean text data in column 'tenure' by removing punctuation and stopwords, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nQ1 = df['tenure'].quantile(0.25)\nQ3 = df['tenure'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['tenure'] < (Q1 - 1.5*IQR)) | (df['tenure'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['tenure_clean'] = df['tenure'].apply(clean)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'precipitation' by imputing with median, then compute TF-IDF features for column 'date' and display top 10 words, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['precipitation'].fillna(df['precipitation'].median(), inplace=True)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date'])\nprint(vect.get_feature_names_out())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then detect outliers in 'High' using the IQR method, then handle missing values in 'Open' by imputing with median, then plot a histogram of 'Open'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nQ1 = df['High'].quantile(0.25)\nQ3 = df['High'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['High'] < (Q1 - 1.5*IQR)) | (df['High'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['Open'].fillna(df['Open'].median(), inplace=True)\ndf['Open'].hist()\nplt.xlabel('Open')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then normalize the 'review' column using min-max scaling, then one-hot encode the categorical column 'rating', then compute TF-IDF features for column 'sentiment' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['review_scaled'] = (df['review'] - df['review'].min()) / (df['review'].max() - df['review'].min())\ndf = pd.get_dummies(df, columns=['rating'], prefix=['rating'])\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sentiment'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then plot a histogram of 'time', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['time'].hist()\nplt.xlabel('time')\nplt.ylabel('Frequency')\nplt.show()\nprint(df.describe())"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then handle missing values in 'device_id' by imputing with median, then detect outliers in 'device_id' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['device_id'].fillna(df['device_id'].median(), inplace=True)\nQ1 = df['device_id'].quantile(0.25)\nQ3 = df['device_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['device_id'] < (Q1 - 1.5*IQR)) | (df['device_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then clean text data in column 'shares' by removing punctuation and stopwords, then one-hot encode the categorical column 'post_id', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['shares_clean'] = df['shares'].apply(clean)\ndf = pd.get_dummies(df, columns=['post_id'], prefix=['post_id'])\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'heart_rate' and display top 10 words, then plot a histogram of 'time', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['heart_rate'])\nprint(vect.get_feature_names_out())\ndf['time'].hist()\nplt.xlabel('time')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score, then plot a histogram of 'Product'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['Product'].hist()\nplt.xlabel('Product')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sales', then one-hot encode the categorical column 'sales', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['sales'], prefix=['sales'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'date', then train a Linear Regression model to predict 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['date'], prefix=['date'])\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then compute TF-IDF features for column 'sensor_value' and display top 10 words, then clean text data in column 'status' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sensor_value'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['status_clean'] = df['status'].apply(clean)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then handle missing values in 'Fare' by imputing with median, then split the data into training and testing sets with an 80-20 split, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Fare'].fillna(df['Fare'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Age'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'text', then detect outliers in 'shares' using the IQR method, then normalize the 'post_id' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['shares'].quantile(0.25)\nQ3 = df['shares'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['shares'] < (Q1 - 1.5*IQR)) | (df['shares'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['post_id_scaled'] = (df['post_id'] - df['post_id'].min()) / (df['post_id'].max() - df['post_id'].min())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform time-series forecasting using ARIMA to predict the next 12 periods, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nprint(df.describe())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['length'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'location' by removing punctuation and stopwords, then train a Linear Regression model to predict 'sensor_value', then normalize the 'device_id' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['location_clean'] = df['location'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['device_id_scaled'] = (df['device_id'] - df['device_id'].min()) / (df['device_id'].max() - df['device_id'].min())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then calculate the correlation matrix for numeric features, then clean text data in column 'pm10' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['pm2_5'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ncorr = df.corr()\nprint(corr)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['pm10_clean'] = df['pm10'].apply(clean)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['sales'], prefix=['sales'])"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'departure_delay' column using min-max scaling, then handle missing values in 'arrival_delay' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['departure_delay_scaled'] = (df['departure_delay'] - df['departure_delay'].min()) / (df['departure_delay'].max() - df['departure_delay'].min())\ndf['arrival_delay'].fillna(df['arrival_delay'].median(), inplace=True)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Churn', then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then handle missing values in 'oldbalanceOrg' by imputing with median, then train a Random Forest Classifier to predict 'isFraud', then train a Linear Regression model to predict 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['oldbalanceOrg'].fillna(df['oldbalanceOrg'].median(), inplace=True)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then normalize the 'newbalanceOrig' column using min-max scaling, then compute TF-IDF features for column 'amount' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['newbalanceOrig_scaled'] = (df['newbalanceOrig'] - df['newbalanceOrig'].min()) / (df['newbalanceOrig'].max() - df['newbalanceOrig'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['amount'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then plot a histogram of 'pm2_5', then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['pm2_5'].hist()\nplt.xlabel('pm2_5')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'sensor_value', then train a Linear Regression model to predict 'sensor_value', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['sensor_value'].hist()\nplt.xlabel('sensor_value')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then normalize the 'pm10' column using min-max scaling, then handle missing values in 'so2' by imputing with median, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['pm10_scaled'] = (df['pm10'] - df['pm10'].min()) / (df['pm10'].max() - df['pm10'].min())\ndf['so2'].fillna(df['so2'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['transaction_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then create a new feature 'amount_ratio' as the ratio of 'amount' to 'isFraud', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['amount_ratio'] = df['amount'] / df['isFraud']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then handle missing values in 'Open' by imputing with median, then split the data into training and testing sets with an 80-20 split, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Open'].fillna(df['Open'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Close'])\ny = df['Close']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Fare', then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'Pclass' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Fare'].hist()\nplt.xlabel('Fare')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Survived'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['Pclass'].quantile(0.25)\nQ3 = df['Pclass'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Pclass'] < (Q1 - 1.5*IQR)) | (df['Pclass'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then compute TF-IDF features for column 'quality' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['ecg_reading'])\ny = df['ecg_reading']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['quality'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then normalize the 'LotArea' column using min-max scaling, then train a Random Forest Classifier to predict 'SalePrice', then create a new feature 'YearBuilt_ratio' as the ratio of 'YearBuilt' to 'SalePrice'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['LotArea_scaled'] = (df['LotArea'] - df['LotArea'].min()) / (df['LotArea'].max() - df['LotArea'].min())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['YearBuilt_ratio'] = df['YearBuilt'] / df['SalePrice']"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'humidity' and display top 10 words, then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['humidity'])\nprint(vect.get_feature_names_out())\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'temperature', then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf = pd.get_dummies(df, columns=['temperature'], prefix=['temperature'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then clean text data in column 'isFraud' by removing punctuation and stopwords, then detect outliers in 'oldbalanceOrg' using the IQR method, then compute TF-IDF features for column 'newbalanceOrig' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['isFraud_clean'] = df['isFraud'].apply(clean)\nQ1 = df['oldbalanceOrg'].quantile(0.25)\nQ3 = df['oldbalanceOrg'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['oldbalanceOrg'] < (Q1 - 1.5*IQR)) | (df['oldbalanceOrg'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['newbalanceOrig'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then split the data into training and testing sets with an 80-20 split, then compute TF-IDF features for column 'pm10' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['pm10'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then handle missing values in 'customers' by imputing with median, then calculate the correlation matrix for numeric features, then train a Random Forest Classifier to predict 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['customers'].fillna(df['customers'].median(), inplace=True)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then normalize the 'tenure' column using min-max scaling, then train a Random Forest Classifier to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['tenure_scaled'] = (df['tenure'] - df['tenure'].min()) / (df['tenure'].max() - df['tenure'].min())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Random Forest Classifier to predict 'arrival_delay', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Survived', then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf = pd.get_dummies(df, columns=['Survived'], prefix=['Survived'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['Survived'].hist()\nplt.xlabel('Survived')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'job_ratio' as the ratio of 'job' to 'y', then normalize the 'education' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['job'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['job_ratio'] = df['job'] / df['y']\ndf['education_scaled'] = (df['education'] - df['education'].min()) / (df['education'].max() - df['education'].min())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then one-hot encode the categorical column 'distance', then plot a histogram of 'distance'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf = pd.get_dummies(df, columns=['distance'], prefix=['distance'])\ndf['distance'].hist()\nplt.xlabel('distance')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then compute TF-IDF features for column 'country' and display top 10 words, then plot a histogram of 'country'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['country'])\nprint(vect.get_feature_names_out())\ndf['country'].hist()\nplt.xlabel('country')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then split the data into training and testing sets with an 80-20 split, then compute TF-IDF features for column 'MonthlyCharges' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nprint(df.describe())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['MonthlyCharges'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'High', then train a Linear Regression model to predict 'Close', then create a new feature 'Open_ratio' as the ratio of 'Open' to 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf = pd.get_dummies(df, columns=['High'], prefix=['High'])\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['Open_ratio'] = df['Open'] / df['Close']"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'arrival_delay', then handle missing values in 'carrier' by imputing with median, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['carrier'].fillna(df['carrier'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then create a new feature 'post_id_ratio' as the ratio of 'post_id' to 'text', then one-hot encode the categorical column 'text', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['post_id_ratio'] = df['post_id'] / df['text']\ndf = pd.get_dummies(df, columns=['text'], prefix=['text'])\nprint(df.describe())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'length'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sentiment'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['length'], prefix=['length'])"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'Sex' and display top 10 words, then normalize the 'Pclass' column using min-max scaling, then one-hot encode the categorical column 'Age'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Sex'])\nprint(vect.get_feature_names_out())\ndf['Pclass_scaled'] = (df['Pclass'] - df['Pclass'].min()) / (df['Pclass'].max() - df['Pclass'].min())\ndf = pd.get_dummies(df, columns=['Age'], prefix=['Age'])"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'isFraud', then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'temperature' column using min-max scaling, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['temperature'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['temperature_scaled'] = (df['temperature'] - df['temperature'].min()) / (df['temperature'].max() - df['temperature'].min())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'deaths', then train a Random Forest Classifier to predict 'confirmed', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf = pd.get_dummies(df, columns=['deaths'], prefix=['deaths'])\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nprint(df.describe())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'arrival_delay', then handle missing values in 'departure_delay' by imputing with median, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['departure_delay'].fillna(df['departure_delay'].median(), inplace=True)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'sentiment', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then one-hot encode the categorical column 'Age', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ncorr = df.corr()\nprint(corr)\ndf = pd.get_dummies(df, columns=['Age'], prefix=['Age'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then create a new feature 'Age_ratio' as the ratio of 'Age' to 'Survived', then evaluate the model performance using RMSE and R\u00b2 score, then clean text data in column 'Sex' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Age_ratio'] = df['Age'] / df['Survived']\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Sex_clean'] = df['Sex'].apply(clean)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then clean text data in column 'age' by removing punctuation and stopwords, then one-hot encode the categorical column 'age', then normalize the 'age' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['age_clean'] = df['age'].apply(clean)\ndf = pd.get_dummies(df, columns=['age'], prefix=['age'])\ndf['age_scaled'] = (df['age'] - df['age'].min()) / (df['age'].max() - df['age'].min())"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'device_id' using the IQR method, then one-hot encode the categorical column 'status', then plot a histogram of 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nQ1 = df['device_id'].quantile(0.25)\nQ3 = df['device_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['device_id'] < (Q1 - 1.5*IQR)) | (df['device_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['status'], prefix=['status'])\ndf['sensor_value'].hist()\nplt.xlabel('sensor_value')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'so2', then plot a histogram of 'pm10', then clean text data in column 'o3' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf = pd.get_dummies(df, columns=['so2'], prefix=['so2'])\ndf['pm10'].hist()\nplt.xlabel('pm10')\nplt.ylabel('Frequency')\nplt.show()\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['o3_clean'] = df['o3'].apply(clean)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then clean text data in column 'education' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['education_clean'] = df['education'].apply(clean)\nprint(df.describe())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'Date' by imputing with median, then plot a histogram of 'Volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nprint(df.describe())\ndf['Date'].fillna(df['Date'].median(), inplace=True)\ndf['Volume'].hist()\nplt.xlabel('Volume')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'traffic_volume', then one-hot encode the categorical column 'traffic_volume', then create a new feature 'rain_1h_ratio' as the ratio of 'rain_1h' to 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['traffic_volume'], prefix=['traffic_volume'])\ndf['rain_1h_ratio'] = df['rain_1h'] / df['traffic_volume']"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'humidity' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())\nQ1 = df['humidity'].quantile(0.25)\nQ3 = df['humidity'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['humidity'] < (Q1 - 1.5*IQR)) | (df['humidity'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe(), then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Low'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods, then one-hot encode the categorical column 'OverallQual'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['LotArea'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf = pd.get_dummies(df, columns=['OverallQual'], prefix=['OverallQual'])"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features, then clean text data in column 'store' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['store_clean'] = df['store'].apply(clean)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'species', then evaluate the model performance using RMSE and R\u00b2 score, then create a new feature 'species_ratio' as the ratio of 'species' to 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['species_ratio'] = df['species'] / df['species']"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then detect outliers in 'High' using the IQR method, then plot a histogram of 'Date', then one-hot encode the categorical column 'Date'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nQ1 = df['High'].quantile(0.25)\nQ3 = df['High'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['High'] < (Q1 - 1.5*IQR)) | (df['High'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['Date'].hist()\nplt.xlabel('Date')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['Date'], prefix=['Date'])"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model, then create a new feature 'device_id_ratio' as the ratio of 'device_id' to 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['device_id_ratio'] = df['device_id'] / df['sensor_value']"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then handle missing values in 'traffic_volume' by imputing with median, then train a Linear Regression model to predict 'traffic_volume', then compute TF-IDF features for column 'temp' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['traffic_volume'].fillna(df['traffic_volume'].median(), inplace=True)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['temp'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then plot a histogram of 'snow_1h', then create a new feature 'date_time_ratio' as the ratio of 'date_time' to 'traffic_volume', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['snow_1h'].hist()\nplt.xlabel('snow_1h')\nplt.ylabel('Frequency')\nplt.show()\ndf['date_time_ratio'] = df['date_time'] / df['traffic_volume']\nprint(df.describe())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then clean text data in column 'isFraud' by removing punctuation and stopwords, then plot a histogram of 'time'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['isFraud_clean'] = df['isFraud'].apply(clean)\ndf['time'].hist()\nplt.xlabel('time')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then plot a histogram of 'customers', then one-hot encode the categorical column 'customers', then clean text data in column 'customers' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['customers'].hist()\nplt.xlabel('customers')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['customers'], prefix=['customers'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['customers_clean'] = df['customers'].apply(clean)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then handle missing values in 'deaths' by imputing with median, then normalize the 'date' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ncorr = df.corr()\nprint(corr)\ndf['deaths'].fillna(df['deaths'].median(), inplace=True)\ndf['date_scaled'] = (df['date'] - df['date'].min()) / (df['date'].max() - df['date'].min())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'temperature' and display top 10 words, then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['temperature'])\nprint(vect.get_feature_names_out())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then plot a histogram of 'age', then train a Random Forest Classifier to predict 'y', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['age'].hist()\nplt.xlabel('age')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['marital'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then split the data into training and testing sets with an 80-20 split, then handle missing values in 'location' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sensor_value'])\ny = df['sensor_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['location'].fillna(df['location'].median(), inplace=True)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then handle missing values in 'Age' by imputing with median, then clean text data in column 'Fare' by removing punctuation and stopwords, then compute TF-IDF features for column 'Survived' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Age'].fillna(df['Age'].median(), inplace=True)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Fare_clean'] = df['Fare'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Survived'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'arrival_delay', then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'flight_ratio' as the ratio of 'flight' to 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['distance'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['flight_ratio'] = df['flight'] / df['arrival_delay']"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'OverallQual', then train a Random Forest Classifier to predict 'SalePrice', then handle missing values in 'SalePrice' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf = pd.get_dummies(df, columns=['OverallQual'], prefix=['OverallQual'])\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['SalePrice'].fillna(df['SalePrice'].median(), inplace=True)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'consumption', then normalize the 'temperature' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['temperature_scaled'] = (df['temperature'] - df['temperature'].min()) / (df['temperature'].max() - df['temperature'].min())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then evaluate the model performance using RMSE and R\u00b2 score, then create a new feature 'traffic_volume_ratio' as the ratio of 'traffic_volume' to 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['traffic_volume_ratio'] = df['traffic_volume'] / df['traffic_volume']"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'flight' and display top 10 words, then one-hot encode the categorical column 'departure_delay', then clean text data in column 'carrier' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['flight'])\nprint(vect.get_feature_names_out())\ndf = pd.get_dummies(df, columns=['departure_delay'], prefix=['departure_delay'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['carrier_clean'] = df['carrier'].apply(clean)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then clean text data in column 'y' by removing punctuation and stopwords, then handle missing values in 'job' by imputing with median, then plot a histogram of 'job'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['y_clean'] = df['y'].apply(clean)\ndf['job'].fillna(df['job'].median(), inplace=True)\ndf['job'].hist()\nplt.xlabel('job')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then create a new feature 'date_time_ratio' as the ratio of 'date_time' to 'traffic_volume', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['date_time_ratio'] = df['date_time'] / df['traffic_volume']\nprint(df.describe())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Product', then one-hot encode the categorical column 'Region', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Product'].hist()\nplt.xlabel('Product')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['Region'], prefix=['Region'])\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Volume', then display feature importances from the Random Forest model, then compute TF-IDF features for column 'Low' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Volume'].hist()\nplt.xlabel('Volume')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Low'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then handle missing values in 'store' by imputing with median, then train a Random Forest Classifier to predict 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['open'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['store'].fillna(df['store'].median(), inplace=True)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'patient_id' using the IQR method, then compute TF-IDF features for column 'quality' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['patient_id'].quantile(0.25)\nQ3 = df['patient_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['patient_id'] < (Q1 - 1.5*IQR)) | (df['patient_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['quality'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then create a new feature 'date_time_ratio' as the ratio of 'date_time' to 'traffic_volume', then one-hot encode the categorical column 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['date_time_ratio'] = df['date_time'] / df['traffic_volume']\ndf = pd.get_dummies(df, columns=['traffic_volume'], prefix=['traffic_volume'])"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'product_id', then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf = pd.get_dummies(df, columns=['product_id'], prefix=['product_id'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'Revenue', then handle missing values in 'Product' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['Product'].fillna(df['Product'].median(), inplace=True)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['education'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nprint(df.describe())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then handle missing values in 'Churn' by imputing with median, then plot a histogram of 'Churn', then compute TF-IDF features for column 'tenure' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['Churn'].fillna(df['Churn'].median(), inplace=True)\ndf['Churn'].hist()\nplt.xlabel('Churn')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['tenure'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display feature importances from the Random Forest model, then train a Linear Regression model to predict 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then create a new feature 'arrival_delay_ratio' as the ratio of 'arrival_delay' to 'arrival_delay', then split the data into training and testing sets with an 80-20 split, then normalize the 'flight' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['arrival_delay_ratio'] = df['arrival_delay'] / df['arrival_delay']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['flight_scaled'] = (df['flight'] - df['flight'].min()) / (df['flight'].max() - df['flight'].min())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['recovered'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then normalize the 'temperature' column using min-max scaling, then compute TF-IDF features for column 'date' and display top 10 words, then train a Random Forest Classifier to predict 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['temperature_scaled'] = (df['temperature'] - df['temperature'].min()) / (df['temperature'].max() - df['temperature'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date'])\nprint(vect.get_feature_names_out())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then plot a histogram of 'LotArea', then compute TF-IDF features for column 'LotArea' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['LotArea'].hist()\nplt.xlabel('LotArea')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['LotArea'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'arrival_delay', then train a Linear Regression model to predict 'arrival_delay', then handle missing values in 'departure_delay' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['departure_delay'].fillna(df['departure_delay'].median(), inplace=True)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'Close', then clean text data in column 'Date' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Close'])\ny = df['Close']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Date_clean'] = df['Date'].apply(clean)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then plot a histogram of 'tenure', then detect outliers in 'tenure' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['tenure'].hist()\nplt.xlabel('tenure')\nplt.ylabel('Frequency')\nplt.show()\nQ1 = df['tenure'].quantile(0.25)\nQ3 = df['tenure'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['tenure'] < (Q1 - 1.5*IQR)) | (df['tenure'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then plot a histogram of 'YearBuilt', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['YearBuilt'].hist()\nplt.xlabel('YearBuilt')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then normalize the 'departure_delay' column using min-max scaling, then plot a histogram of 'arrival_delay', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['departure_delay_scaled'] = (df['departure_delay'] - df['departure_delay'].min()) / (df['departure_delay'].max() - df['departure_delay'].min())\ndf['arrival_delay'].hist()\nplt.xlabel('arrival_delay')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Survived', then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf = pd.get_dummies(df, columns=['Survived'], prefix=['Survived'])\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Fare'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then display feature importances from the Random Forest model, then clean text data in column 'date' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nprint(df.describe())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then create a new feature 'distance_ratio' as the ratio of 'distance' to 'arrival_delay', then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['distance_ratio'] = df['distance'] / df['arrival_delay']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then detect outliers in 'store' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nQ1 = df['store'].quantile(0.25)\nQ3 = df['store'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['store'] < (Q1 - 1.5*IQR)) | (df['store'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display feature importances from the Random Forest model, then train a Random Forest Classifier to predict 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['age'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then detect outliers in 'petal_width' using the IQR method, then compute TF-IDF features for column 'species' and display top 10 words, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nQ1 = df['petal_width'].quantile(0.25)\nQ3 = df['petal_width'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['petal_width'] < (Q1 - 1.5*IQR)) | (df['petal_width'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['species'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then detect outliers in 'date' using the IQR method, then display feature importances from the Random Forest model, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nprint(df.describe())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then normalize the 'humidity' column using min-max scaling, then detect outliers in 'humidity' using the IQR method, then create a new feature 'pressure_ratio' as the ratio of 'pressure' to 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['humidity_scaled'] = (df['humidity'] - df['humidity'].min()) / (df['humidity'].max() - df['humidity'].min())\nQ1 = df['humidity'].quantile(0.25)\nQ3 = df['humidity'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['humidity'] < (Q1 - 1.5*IQR)) | (df['humidity'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['pressure_ratio'] = df['pressure'] / df['consumption']"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features, then create a new feature 'no2_ratio' as the ratio of 'no2' to 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)\ndf['no2_ratio'] = df['no2'] / df['pm2_5']"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'consumption', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then clean text data in column 'marital' by removing punctuation and stopwords, then one-hot encode the categorical column 'education', then create a new feature 'age_ratio' as the ratio of 'age' to 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['marital_clean'] = df['marital'].apply(clean)\ndf = pd.get_dummies(df, columns=['education'], prefix=['education'])\ndf['age_ratio'] = df['age'] / df['y']"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then display summary statistics of all numeric columns using df.describe(), then normalize the 'carrier' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nprint(df.describe())\ndf['carrier_scaled'] = (df['carrier'] - df['carrier'].min()) / (df['carrier'].max() - df['carrier'].min())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'isFraud', then compute TF-IDF features for column 'isFraud' and display top 10 words, then train a Linear Regression model to predict 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['isFraud'])\nprint(vect.get_feature_names_out())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then plot a histogram of 'tenure', then display feature importances from the Random Forest model, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['tenure'].hist()\nplt.xlabel('tenure')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then normalize the 'customers' column using min-max scaling, then clean text data in column 'customers' by removing punctuation and stopwords, then train a Linear Regression model to predict 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['customers_scaled'] = (df['customers'] - df['customers'].min()) / (df['customers'].max() - df['customers'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['customers_clean'] = df['customers'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then create a new feature 'Survived_ratio' as the ratio of 'Survived' to 'Survived', then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Survived_ratio'] = df['Survived'] / df['Survived']\nprint(df.describe())\ndf['Survived'].hist()\nplt.xlabel('Survived')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'ContractType' and display top 10 words, then train a Linear Regression model to predict 'Churn', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['ContractType'])\nprint(vect.get_feature_names_out())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then create a new feature 'review_ratio' as the ratio of 'review' to 'sentiment', then compute TF-IDF features for column 'rating' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['review_ratio'] = df['review'] / df['sentiment']\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['rating'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then compute TF-IDF features for column 'MonthlyCharges' and display top 10 words, then train a Random Forest Classifier to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nprint(df.describe())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['MonthlyCharges'])\nprint(vect.get_feature_names_out())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'recovered', then train a Linear Regression model to predict 'confirmed', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf = pd.get_dummies(df, columns=['recovered'], prefix=['recovered'])\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'SalePrice', then compute TF-IDF features for column 'YearBuilt' and display top 10 words, then clean text data in column 'Neighborhood' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['YearBuilt'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Neighborhood_clean'] = df['Neighborhood'].apply(clean)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'deaths' and display top 10 words, then train a Linear Regression model to predict 'confirmed', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['deaths'])\nprint(vect.get_feature_names_out())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'ecg_reading', then normalize the 'quality' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['ecg_reading'])\ny = df['ecg_reading']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['quality_scaled'] = (df['quality'] - df['quality'].min()) / (df['quality'].max() - df['quality'].min())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'patient_id' using the IQR method, then create a new feature 'patient_id_ratio' as the ratio of 'patient_id' to 'ecg_reading', then train a Random Forest Classifier to predict 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nQ1 = df['patient_id'].quantile(0.25)\nQ3 = df['patient_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['patient_id'] < (Q1 - 1.5*IQR)) | (df['patient_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['patient_id_ratio'] = df['patient_id'] / df['ecg_reading']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then compute TF-IDF features for column 'length' and display top 10 words, then handle missing values in 'sentiment' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['rating'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['length'])\nprint(vect.get_feature_names_out())\ndf['sentiment'].fillna(df['sentiment'].median(), inplace=True)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then clean text data in column 'Neighborhood' by removing punctuation and stopwords, then compute TF-IDF features for column 'LotArea' and display top 10 words, then normalize the 'OverallQual' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Neighborhood_clean'] = df['Neighborhood'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['LotArea'])\nprint(vect.get_feature_names_out())\ndf['OverallQual_scaled'] = (df['OverallQual'] - df['OverallQual'].min()) / (df['OverallQual'].max() - df['OverallQual'].min())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then create a new feature 'country_ratio' as the ratio of 'country' to 'confirmed', then train a Linear Regression model to predict 'confirmed', then one-hot encode the categorical column 'recovered'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['country_ratio'] = df['country'] / df['confirmed']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['recovered'], prefix=['recovered'])"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'quality' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['quality_scaled'] = (df['quality'] - df['quality'].min()) / (df['quality'].max() - df['quality'].min())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then clean text data in column 'time' by removing punctuation and stopwords, then normalize the 'newbalanceOrig' column using min-max scaling, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['time_clean'] = df['time'].apply(clean)\ndf['newbalanceOrig_scaled'] = (df['newbalanceOrig'] - df['newbalanceOrig'].min()) / (df['newbalanceOrig'].max() - df['newbalanceOrig'].min())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then create a new feature 'customers_ratio' as the ratio of 'customers' to 'sales', then plot a histogram of 'day_of_week', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['customers_ratio'] = df['customers'] / df['sales']\ndf['day_of_week'].hist()\nplt.xlabel('day_of_week')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'temperature' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nprint(df.describe())\ndf['temperature'].fillna(df['temperature'].median(), inplace=True)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then handle missing values in 'flight' by imputing with median, then normalize the 'arrival_delay' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['flight'].fillna(df['flight'].median(), inplace=True)\ndf['arrival_delay_scaled'] = (df['arrival_delay'] - df['arrival_delay'].min()) / (df['arrival_delay'].max() - df['arrival_delay'].min())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then handle missing values in 'length' by imputing with median, then evaluate the model performance using RMSE and R\u00b2 score, then detect outliers in 'rating' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['length'].fillna(df['length'].median(), inplace=True)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nQ1 = df['rating'].quantile(0.25)\nQ3 = df['rating'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['rating'] < (Q1 - 1.5*IQR)) | (df['rating'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then compute TF-IDF features for column 'timestamp' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sensor_value'])\ny = df['sensor_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['timestamp'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then display summary statistics of all numeric columns using df.describe(), then one-hot encode the categorical column 'location'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nprint(df.describe())\ndf = pd.get_dummies(df, columns=['location'], prefix=['location'])"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features, then clean text data in column 'Fare' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Pclass'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Fare_clean'] = df['Fare'].apply(clean)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then create a new feature 'marital_ratio' as the ratio of 'marital' to 'y', then train a Linear Regression model to predict 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['marital_ratio'] = df['marital'] / df['y']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then plot a histogram of 'country', then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['country'].hist()\nplt.xlabel('country')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then train a Random Forest Classifier to predict 'ecg_reading', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then handle missing values in 'sentiment' by imputing with median, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['sentiment'].fillna(df['sentiment'].median(), inplace=True)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['rating'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Random Forest Classifier to predict 'Survived', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Age'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then normalize the 'species' column using min-max scaling, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['species_scaled'] = (df['species'] - df['species'].min()) / (df['species'].max() - df['species'].min())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['species'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nprint(df.describe())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then normalize the 'isFraud' column using min-max scaling, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['isFraud_scaled'] = (df['isFraud'] - df['isFraud'].min()) / (df['isFraud'].max() - df['isFraud'].min())\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then split the data into training and testing sets with an 80-20 split, then clean text data in column 'location' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nprint(df.describe())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sensor_value'])\ny = df['sensor_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['location_clean'] = df['location'].apply(clean)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'total_amount', then plot a histogram of 'customer_id', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['customer_id'].hist()\nplt.xlabel('customer_id')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then normalize the 'sepal_width' column using min-max scaling, then create a new feature 'petal_length_ratio' as the ratio of 'petal_length' to 'species', then one-hot encode the categorical column 'sepal_length'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['sepal_width_scaled'] = (df['sepal_width'] - df['sepal_width'].min()) / (df['sepal_width'].max() - df['sepal_width'].min())\ndf['petal_length_ratio'] = df['petal_length'] / df['species']\ndf = pd.get_dummies(df, columns=['sepal_length'], prefix=['sepal_length'])"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Fare', then one-hot encode the categorical column 'Age', then normalize the 'Sex' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Fare'].hist()\nplt.xlabel('Fare')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['Age'], prefix=['Age'])\ndf['Sex_scaled'] = (df['Sex'] - df['Sex'].min()) / (df['Sex'].max() - df['Sex'].min())"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then normalize the 'Pclass' column using min-max scaling, then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'Sex'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Pclass_scaled'] = (df['Pclass'] - df['Pclass'].min()) / (df['Pclass'].max() - df['Pclass'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['Sex'].hist()\nplt.xlabel('Sex')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then clean text data in column 'date' by removing punctuation and stopwords, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'total_amount', then create a new feature 'product_id_ratio' as the ratio of 'product_id' to 'total_amount', then clean text data in column 'quantity' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['product_id_ratio'] = df['product_id'] / df['total_amount']\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['quantity_clean'] = df['quantity'].apply(clean)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then plot a histogram of 'location', then train a Random Forest Classifier to predict 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ncorr = df.corr()\nprint(corr)\ndf['location'].hist()\nplt.xlabel('location')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then one-hot encode the categorical column 'SalePrice', then train a Random Forest Classifier to predict 'SalePrice'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Neighborhood'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf = pd.get_dummies(df, columns=['SalePrice'], prefix=['SalePrice'])\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'Region' by removing punctuation and stopwords, then create a new feature 'UnitsSold_ratio' as the ratio of 'UnitsSold' to 'Revenue', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Region_clean'] = df['Region'].apply(clean)\ndf['UnitsSold_ratio'] = df['UnitsSold'] / df['Revenue']\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Revenue'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then detect outliers in 'ContractType' using the IQR method, then compute TF-IDF features for column 'MonthlyCharges' and display top 10 words, then handle missing values in 'ContractType' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nQ1 = df['ContractType'].quantile(0.25)\nQ3 = df['ContractType'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['ContractType'] < (Q1 - 1.5*IQR)) | (df['ContractType'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['MonthlyCharges'])\nprint(vect.get_feature_names_out())\ndf['ContractType'].fillna(df['ContractType'].median(), inplace=True)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Random Forest Classifier to predict 'isFraud', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'time', then clean text data in column 'patient_id' by removing punctuation and stopwords, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf = pd.get_dummies(df, columns=['time'], prefix=['time'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['patient_id_clean'] = df['patient_id'].apply(clean)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['ecg_reading'])\ny = df['ecg_reading']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then create a new feature 'patient_id_ratio' as the ratio of 'patient_id' to 'ecg_reading', then plot a histogram of 'time', then one-hot encode the categorical column 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['patient_id_ratio'] = df['patient_id'] / df['ecg_reading']\ndf['time'].hist()\nplt.xlabel('time')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['ecg_reading'], prefix=['ecg_reading'])"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sentiment', then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then create a new feature 'customers_ratio' as the ratio of 'customers' to 'sales', then train a Linear Regression model to predict 'sales', then detect outliers in 'open' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['customers_ratio'] = df['customers'] / df['sales']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nQ1 = df['open'].quantile(0.25)\nQ3 = df['open'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['open'] < (Q1 - 1.5*IQR)) | (df['open'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then handle missing values in 'status' by imputing with median, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['timestamp'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['status'].fillna(df['status'].median(), inplace=True)\nprint(df.describe())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Region', then train a Random Forest Classifier to predict 'Revenue', then detect outliers in 'Date' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Region'].hist()\nplt.xlabel('Region')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['Date'].quantile(0.25)\nQ3 = df['Date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Date'] < (Q1 - 1.5*IQR)) | (df['Date'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'departure_delay' column using min-max scaling, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['departure_delay_scaled'] = (df['departure_delay'] - df['departure_delay'].min()) / (df['departure_delay'].max() - df['departure_delay'].min())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then plot a histogram of 'review', then create a new feature 'genre_ratio' as the ratio of 'genre' to 'sentiment', then train a Random Forest Classifier to predict 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['review'].hist()\nplt.xlabel('review')\nplt.ylabel('Frequency')\nplt.show()\ndf['genre_ratio'] = df['genre'] / df['sentiment']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then detect outliers in 'no2' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nQ1 = df['no2'].quantile(0.25)\nQ3 = df['no2'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['no2'] < (Q1 - 1.5*IQR)) | (df['no2'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then one-hot encode the categorical column 'LotArea', then plot a histogram of 'LotArea'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf = pd.get_dummies(df, columns=['LotArea'], prefix=['LotArea'])\ndf['LotArea'].hist()\nplt.xlabel('LotArea')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split, then one-hot encode the categorical column 'sepal_length'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['species'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf = pd.get_dummies(df, columns=['sepal_length'], prefix=['sepal_length'])"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then clean text data in column 'High' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe(), then train a Linear Regression model to predict 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['High_clean'] = df['High'].apply(clean)\nprint(df.describe())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then train a Linear Regression model to predict 'total_amount', then clean text data in column 'customer_id' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['customer_id_clean'] = df['customer_id'].apply(clean)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'ecg_reading', then perform K-Means clustering with k=3 on numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['heart_rate'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'timestamp' by removing punctuation and stopwords, then detect outliers in 'sensor_value' using the IQR method, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['timestamp_clean'] = df['timestamp'].apply(clean)\nQ1 = df['sensor_value'].quantile(0.25)\nQ3 = df['sensor_value'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sensor_value'] < (Q1 - 1.5*IQR)) | (df['sensor_value'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then handle missing values in 'newbalanceOrig' by imputing with median, then one-hot encode the categorical column 'newbalanceOrig', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['newbalanceOrig'].fillna(df['newbalanceOrig'].median(), inplace=True)\ndf = pd.get_dummies(df, columns=['newbalanceOrig'], prefix=['newbalanceOrig'])\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'time' and display top 10 words, then create a new feature 'quality_ratio' as the ratio of 'quality' to 'ecg_reading', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['time'])\nprint(vect.get_feature_names_out())\ndf['quality_ratio'] = df['quality'] / df['ecg_reading']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['ecg_reading'])\ny = df['ecg_reading']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'length', then create a new feature 'review_ratio' as the ratio of 'review' to 'sentiment', then train a Random Forest Classifier to predict 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf = pd.get_dummies(df, columns=['length'], prefix=['length'])\ndf['review_ratio'] = df['review'] / df['sentiment']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then normalize the 'amount' column using min-max scaling, then compute TF-IDF features for column 'time' and display top 10 words, then create a new feature 'time_ratio' as the ratio of 'time' to 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['amount_scaled'] = (df['amount'] - df['amount'].min()) / (df['amount'].max() - df['amount'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['time'])\nprint(vect.get_feature_names_out())\ndf['time_ratio'] = df['time'] / df['isFraud']"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then handle missing values in 'o3' by imputing with median, then train a Random Forest Classifier to predict 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['pm10'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['o3'].fillna(df['o3'].median(), inplace=True)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then detect outliers in 'total_amount' using the IQR method, then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'customer_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nQ1 = df['total_amount'].quantile(0.25)\nQ3 = df['total_amount'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['total_amount'] < (Q1 - 1.5*IQR)) | (df['total_amount'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['customer_id'].hist()\nplt.xlabel('customer_id')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'humidity' and display top 10 words, then display summary statistics of all numeric columns using df.describe(), then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['humidity'])\nprint(vect.get_feature_names_out())\nprint(df.describe())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['precipitation'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then detect outliers in 'date' using the IQR method, then compute TF-IDF features for column 'humidity' and display top 10 words, then clean text data in column 'date' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['humidity'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then compute TF-IDF features for column 'wind_speed' and display top 10 words, then train a Random Forest Classifier to predict 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['wind_speed'])\nprint(vect.get_feature_names_out())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then normalize the 'pressure' column using min-max scaling, then one-hot encode the categorical column 'consumption', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['pressure_scaled'] = (df['pressure'] - df['pressure'].min()) / (df['pressure'].max() - df['pressure'].min())\ndf = pd.get_dummies(df, columns=['consumption'], prefix=['consumption'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'text', then plot a histogram of 'user_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['text'])\ny = df['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['user_id'].hist()\nplt.xlabel('user_id')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'Revenue' by removing punctuation and stopwords, then normalize the 'Product' column using min-max scaling, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Revenue_clean'] = df['Revenue'].apply(clean)\ndf['Product_scaled'] = (df['Product'] - df['Product'].min()) / (df['Product'].max() - df['Product'].min())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then handle missing values in 'LotArea' by imputing with median, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ncorr = df.corr()\nprint(corr)\ndf['LotArea'].fillna(df['LotArea'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then detect outliers in 'snow_1h' using the IQR method, then train a Linear Regression model to predict 'traffic_volume', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nQ1 = df['snow_1h'].quantile(0.25)\nQ3 = df['snow_1h'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['snow_1h'] < (Q1 - 1.5*IQR)) | (df['snow_1h'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['snow_1h'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then normalize the 'likes' column using min-max scaling, then one-hot encode the categorical column 'post_id', then plot a histogram of 'shares'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['likes_scaled'] = (df['likes'] - df['likes'].min()) / (df['likes'].max() - df['likes'].min())\ndf = pd.get_dummies(df, columns=['post_id'], prefix=['post_id'])\ndf['shares'].hist()\nplt.xlabel('shares')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display feature importances from the Random Forest model, then train a Linear Regression model to predict 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['post_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Close', then perform time-series forecasting using ARIMA to predict the next 12 periods, then compute TF-IDF features for column 'Date' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Low'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Date'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then clean text data in column 'sales' by removing punctuation and stopwords, then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sales_clean'] = df['sales'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['store'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then perform time-series forecasting using ARIMA to predict the next 12 periods, then handle missing values in 'open' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['store'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['open'].fillna(df['open'].median(), inplace=True)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then detect outliers in 'time' using the IQR method, then normalize the 'amount' column using min-max scaling, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nQ1 = df['time'].quantile(0.25)\nQ3 = df['time'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['time'] < (Q1 - 1.5*IQR)) | (df['time'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['amount_scaled'] = (df['amount'] - df['amount'].min()) / (df['amount'].max() - df['amount'].min())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'country' and display top 10 words, then display feature importances from the Random Forest model, then handle missing values in 'country' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['country'])\nprint(vect.get_feature_names_out())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['country'].fillna(df['country'].median(), inplace=True)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then calculate the correlation matrix for numeric features, then detect outliers in 'sepal_length' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ncorr = df.corr()\nprint(corr)\nQ1 = df['sepal_length'].quantile(0.25)\nQ3 = df['sepal_length'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sepal_length'] < (Q1 - 1.5*IQR)) | (df['sepal_length'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then plot a histogram of 'day_of_week', then split the data into training and testing sets with an 80-20 split, then compute TF-IDF features for column 'open' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['day_of_week'].hist()\nplt.xlabel('day_of_week')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['open'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'traffic_volume', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then create a new feature 'humidity_ratio' as the ratio of 'humidity' to 'temperature', then one-hot encode the categorical column 'date', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['humidity_ratio'] = df['humidity'] / df['temperature']\ndf = pd.get_dummies(df, columns=['date'], prefix=['date'])\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then normalize the 'store' column using min-max scaling, then train a Random Forest Classifier to predict 'sales', then train a Linear Regression model to predict 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['store_scaled'] = (df['store'] - df['store'].min()) / (df['store'].max() - df['store'].min())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then create a new feature 'review_ratio' as the ratio of 'review' to 'sentiment', then perform K-Means clustering with k=3 on numeric features, then compute TF-IDF features for column 'sentiment' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['review_ratio'] = df['review'] / df['sentiment']\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sentiment'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'ecg_reading', then handle missing values in 'patient_id' by imputing with median, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['patient_id'].fillna(df['patient_id'].median(), inplace=True)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['time'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then create a new feature 'amount_ratio' as the ratio of 'amount' to 'isFraud', then plot a histogram of 'newbalanceOrig', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['amount_ratio'] = df['amount'] / df['isFraud']\ndf['newbalanceOrig'].hist()\nplt.xlabel('newbalanceOrig')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then plot a histogram of 'sales', then perform time-series forecasting using ARIMA to predict the next 12 periods, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['sales'].hist()\nplt.xlabel('sales')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['open'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then normalize the 'Churn' column using min-max scaling, then handle missing values in 'TotalCharges' by imputing with median, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['Churn_scaled'] = (df['Churn'] - df['Churn'].min()) / (df['Churn'].max() - df['Churn'].min())\ndf['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then detect outliers in 'snow_1h' using the IQR method, then clean text data in column 'traffic_volume' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ncorr = df.corr()\nprint(corr)\nQ1 = df['snow_1h'].quantile(0.25)\nQ3 = df['snow_1h'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['snow_1h'] < (Q1 - 1.5*IQR)) | (df['snow_1h'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['traffic_volume_clean'] = df['traffic_volume'].apply(clean)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then compute TF-IDF features for column 'no2' and display top 10 words, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nprint(df.describe())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['no2'])\nprint(vect.get_feature_names_out())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then normalize the 'OverallQual' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['OverallQual_scaled'] = (df['OverallQual'] - df['OverallQual'].min()) / (df['OverallQual'].max() - df['OverallQual'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['MonthlyCharges'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then handle missing values in 'arrival_delay' by imputing with median, then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'departure_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['arrival_delay'].fillna(df['arrival_delay'].median(), inplace=True)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['departure_delay'], prefix=['departure_delay'])"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sentiment', then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then handle missing values in 'time' by imputing with median, then train a Random Forest Classifier to predict 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ncorr = df.corr()\nprint(corr)\ndf['time'].fillna(df['time'].median(), inplace=True)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then handle missing values in 'OverallQual' by imputing with median, then one-hot encode the categorical column 'OverallQual'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['OverallQual'].fillna(df['OverallQual'].median(), inplace=True)\ndf = pd.get_dummies(df, columns=['OverallQual'], prefix=['OverallQual'])"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Churn', then evaluate the model performance using RMSE and R\u00b2 score, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['ContractType'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then normalize the 'date_time' column using min-max scaling, then perform K-Means clustering with k=3 on numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['date_time_scaled'] = (df['date_time'] - df['date_time'].min()) / (df['date_time'].max() - df['date_time'].min())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['temp'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'text', then normalize the 'post_id' column using min-max scaling, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['post_id_scaled'] = (df['post_id'] - df['post_id'].min()) / (df['post_id'].max() - df['post_id'].min())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['amount'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then detect outliers in 'rating' using the IQR method, then create a new feature 'sentiment_ratio' as the ratio of 'sentiment' to 'sentiment', then one-hot encode the categorical column 'genre'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nQ1 = df['rating'].quantile(0.25)\nQ3 = df['rating'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['rating'] < (Q1 - 1.5*IQR)) | (df['rating'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['sentiment_ratio'] = df['sentiment'] / df['sentiment']\ndf = pd.get_dummies(df, columns=['genre'], prefix=['genre'])"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sensor_value', then create a new feature 'status_ratio' as the ratio of 'status' to 'sensor_value', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['status_ratio'] = df['status'] / df['sensor_value']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sensor_value'])\ny = df['sensor_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then detect outliers in 'TotalCharges' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nQ1 = df['TotalCharges'].quantile(0.25)\nQ3 = df['TotalCharges'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['TotalCharges'] < (Q1 - 1.5*IQR)) | (df['TotalCharges'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['MonthlyCharges'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'time', then train a Random Forest Classifier to predict 'isFraud', then normalize the 'oldbalanceOrg' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf = pd.get_dummies(df, columns=['time'], prefix=['time'])\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['oldbalanceOrg_scaled'] = (df['oldbalanceOrg'] - df['oldbalanceOrg'].min()) / (df['oldbalanceOrg'].max() - df['oldbalanceOrg'].min())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then normalize the 'pressure' column using min-max scaling, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['pressure_scaled'] = (df['pressure'] - df['pressure'].min()) / (df['pressure'].max() - df['pressure'].min())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'device_id' using the IQR method, then one-hot encode the categorical column 'location'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['device_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['device_id'].quantile(0.25)\nQ3 = df['device_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['device_id'] < (Q1 - 1.5*IQR)) | (df['device_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['location'], prefix=['location'])"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then plot a histogram of 'post_id', then train a Linear Regression model to predict 'text', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['post_id'].hist()\nplt.xlabel('post_id')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['text'])\ny = df['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'humidity', then clean text data in column 'precipitation' by removing punctuation and stopwords, then handle missing values in 'wind_speed' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf = pd.get_dummies(df, columns=['humidity'], prefix=['humidity'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['precipitation_clean'] = df['precipitation'].apply(clean)\ndf['wind_speed'].fillna(df['wind_speed'].median(), inplace=True)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then clean text data in column 'date' by removing punctuation and stopwords, then train a Linear Regression model to predict 'confirmed', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then plot a histogram of 'temperature', then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['temperature'].hist()\nplt.xlabel('temperature')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'traffic_volume', then clean text data in column 'rain_1h' by removing punctuation and stopwords, then plot a histogram of 'rain_1h'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['rain_1h_clean'] = df['rain_1h'].apply(clean)\ndf['rain_1h'].hist()\nplt.xlabel('rain_1h')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Random Forest Classifier to predict 'text', then plot a histogram of 'likes'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nprint(df.describe())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['likes'].hist()\nplt.xlabel('likes')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'total_amount', then normalize the 'product_id' column using min-max scaling, then handle missing values in 'total_amount' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['product_id_scaled'] = (df['product_id'] - df['product_id'].min()) / (df['product_id'].max() - df['product_id'].min())\ndf['total_amount'].fillna(df['total_amount'].median(), inplace=True)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'SalePrice', then train a Random Forest Classifier to predict 'SalePrice', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'y', then evaluate the model performance using RMSE and R\u00b2 score, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nprint(df.describe())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then normalize the 'carrier' column using min-max scaling, then train a Random Forest Classifier to predict 'arrival_delay', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['carrier_scaled'] = (df['carrier'] - df['carrier'].min()) / (df['carrier'].max() - df['carrier'].min())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'Churn', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['TotalCharges'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then evaluate the model performance using RMSE and R\u00b2 score, then compute TF-IDF features for column 'deaths' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['deaths'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then detect outliers in 'post_id' using the IQR method, then display feature importances from the Random Forest model, then handle missing values in 'user_id' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nQ1 = df['post_id'].quantile(0.25)\nQ3 = df['post_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['post_id'] < (Q1 - 1.5*IQR)) | (df['post_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['user_id'].fillna(df['user_id'].median(), inplace=True)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'arrival_delay', then plot a histogram of 'distance'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['distance'].hist()\nplt.xlabel('distance')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'Fare' and display top 10 words, then clean text data in column 'Age' by removing punctuation and stopwords, then normalize the 'Pclass' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Fare'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Age_clean'] = df['Age'].apply(clean)\ndf['Pclass_scaled'] = (df['Pclass'] - df['Pclass'].min()) / (df['Pclass'].max() - df['Pclass'].min())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then one-hot encode the categorical column 'user_id', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['user_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf = pd.get_dummies(df, columns=['user_id'], prefix=['user_id'])\nprint(df.describe())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Close', then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'Open' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['Open'].quantile(0.25)\nQ3 = df['Open'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Open'] < (Q1 - 1.5*IQR)) | (df['Open'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then one-hot encode the categorical column 'carrier', then handle missing values in 'flight' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ncorr = df.corr()\nprint(corr)\ndf = pd.get_dummies(df, columns=['carrier'], prefix=['carrier'])\ndf['flight'].fillna(df['flight'].median(), inplace=True)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then clean text data in column 'tenure' by removing punctuation and stopwords, then split the data into training and testing sets with an 80-20 split, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['tenure_clean'] = df['tenure'].apply(clean)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['TotalCharges'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'no2' column using min-max scaling, then train a Random Forest Classifier to predict 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['no2'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['no2_scaled'] = (df['no2'] - df['no2'].min()) / (df['no2'].max() - df['no2'].min())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'arrival_delay', then plot a histogram of 'carrier', then train a Linear Regression model to predict 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['carrier'].hist()\nplt.xlabel('carrier')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sales', then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then clean text data in column 'High' by removing punctuation and stopwords, then detect outliers in 'Open' using the IQR method, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['High_clean'] = df['High'].apply(clean)\nQ1 = df['Open'].quantile(0.25)\nQ3 = df['Open'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Open'] < (Q1 - 1.5*IQR)) | (df['Open'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'confirmed' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['confirmed'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nprint(df.describe())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then compute TF-IDF features for column 'time' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['time'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'user_id' and display top 10 words, then create a new feature 'shares_ratio' as the ratio of 'shares' to 'text', then train a Random Forest Classifier to predict 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['user_id'])\nprint(vect.get_feature_names_out())\ndf['shares_ratio'] = df['shares'] / df['text']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'consumption', then plot a histogram of 'humidity', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['humidity'].hist()\nplt.xlabel('humidity')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'sensor_value', then normalize the 'sensor_value' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['sensor_value_scaled'] = (df['sensor_value'] - df['sensor_value'].min()) / (df['sensor_value'].max() - df['sensor_value'].min())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then create a new feature 'rating_ratio' as the ratio of 'rating' to 'sentiment', then one-hot encode the categorical column 'review', then detect outliers in 'length' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['rating_ratio'] = df['rating'] / df['sentiment']\ndf = pd.get_dummies(df, columns=['review'], prefix=['review'])\nQ1 = df['length'].quantile(0.25)\nQ3 = df['length'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['length'] < (Q1 - 1.5*IQR)) | (df['length'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then detect outliers in 'Neighborhood' using the IQR method, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ncorr = df.corr()\nprint(corr)\nQ1 = df['Neighborhood'].quantile(0.25)\nQ3 = df['Neighborhood'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Neighborhood'] < (Q1 - 1.5*IQR)) | (df['Neighborhood'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nprint(df.describe())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'Product' using the IQR method, then perform K-Means clustering with k=3 on numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nQ1 = df['Product'].quantile(0.25)\nQ3 = df['Product'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Product'] < (Q1 - 1.5*IQR)) | (df['Product'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Product'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'pm2_5', then perform K-Means clustering with k=3 on numeric features, then create a new feature 'no2_ratio' as the ratio of 'no2' to 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['no2_ratio'] = df['no2'] / df['pm2_5']"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['confirmed'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'oldbalanceOrg', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['oldbalanceOrg'], prefix=['oldbalanceOrg'])\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then display summary statistics of all numeric columns using df.describe(), then train a Linear Regression model to predict 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nprint(df.describe())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then normalize the 'SalePrice' column using min-max scaling, then detect outliers in 'LotArea' using the IQR method, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['SalePrice_scaled'] = (df['SalePrice'] - df['SalePrice'].min()) / (df['SalePrice'].max() - df['SalePrice'].min())\nQ1 = df['LotArea'].quantile(0.25)\nQ3 = df['LotArea'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['LotArea'] < (Q1 - 1.5*IQR)) | (df['LotArea'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'device_id' column using min-max scaling, then clean text data in column 'device_id' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sensor_value'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['device_id_scaled'] = (df['device_id'] - df['device_id'].min()) / (df['device_id'].max() - df['device_id'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['device_id_clean'] = df['device_id'].apply(clean)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then perform K-Means clustering with k=3 on numeric features, then create a new feature 'Low_ratio' as the ratio of 'Low' to 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['Low_ratio'] = df['Low'] / df['Close']"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'Close', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Close'])\ny = df['Close']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'product_id', then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'transaction_id' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf = pd.get_dummies(df, columns=['product_id'], prefix=['product_id'])\nprint(df.describe())\ndf['transaction_id'].fillna(df['transaction_id'].median(), inplace=True)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'sensor_value' using the IQR method, then compute TF-IDF features for column 'timestamp' and display top 10 words, then train a Random Forest Classifier to predict 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nQ1 = df['sensor_value'].quantile(0.25)\nQ3 = df['sensor_value'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sensor_value'] < (Q1 - 1.5*IQR)) | (df['sensor_value'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['timestamp'])\nprint(vect.get_feature_names_out())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'job' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score, then plot a histogram of 'age'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['job'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['age'].hist()\nplt.xlabel('age')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'YearBuilt', then display feature importances from the Random Forest model, then train a Linear Regression model to predict 'SalePrice'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf = pd.get_dummies(df, columns=['YearBuilt'], prefix=['YearBuilt'])\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then create a new feature 'ecg_reading_ratio' as the ratio of 'ecg_reading' to 'ecg_reading', then display feature importances from the Random Forest model, then train a Random Forest Classifier to predict 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['ecg_reading_ratio'] = df['ecg_reading'] / df['ecg_reading']\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then detect outliers in 'date' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then handle missing values in 'temp' by imputing with median, then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['temp'].fillna(df['temp'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then plot a histogram of 'total_amount', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ncorr = df.corr()\nprint(corr)\ndf['total_amount'].hist()\nplt.xlabel('total_amount')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Close', then one-hot encode the categorical column 'High', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['High'], prefix=['High'])\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then clean text data in column 'date' by removing punctuation and stopwords, then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'Survived' and display top 10 words, then create a new feature 'Sex_ratio' as the ratio of 'Sex' to 'Survived', then one-hot encode the categorical column 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Survived'])\nprint(vect.get_feature_names_out())\ndf['Sex_ratio'] = df['Sex'] / df['Survived']\ndf = pd.get_dummies(df, columns=['Survived'], prefix=['Survived'])"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Random Forest Classifier to predict 'sensor_value', then clean text data in column 'timestamp' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['timestamp_clean'] = df['timestamp'].apply(clean)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'device_id', then create a new feature 'timestamp_ratio' as the ratio of 'timestamp' to 'sensor_value', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['device_id'].hist()\nplt.xlabel('device_id')\nplt.ylabel('Frequency')\nplt.show()\ndf['timestamp_ratio'] = df['timestamp'] / df['sensor_value']\nprint(df.describe())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then plot a histogram of 'species', then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'petal_width' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['species'].hist()\nplt.xlabel('species')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['petal_length'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['petal_width_scaled'] = (df['petal_width'] - df['petal_width'].min()) / (df['petal_width'].max() - df['petal_width'].min())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'recovered' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['recovered_scaled'] = (df['recovered'] - df['recovered'].min()) / (df['recovered'].max() - df['recovered'].min())"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'Survived', then normalize the 'Survived' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Pclass'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['Survived_scaled'] = (df['Survived'] - df['Survived'].min()) / (df['Survived'].max() - df['Survived'].min())"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then handle missing values in 'Neighborhood' by imputing with median, then clean text data in column 'LotArea' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['SalePrice'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['Neighborhood'].fillna(df['Neighborhood'].median(), inplace=True)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['LotArea_clean'] = df['LotArea'].apply(clean)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'temp', then normalize the 'date_time' column using min-max scaling, then train a Random Forest Classifier to predict 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf = pd.get_dummies(df, columns=['temp'], prefix=['temp'])\ndf['date_time_scaled'] = (df['date_time'] - df['date_time'].min()) / (df['date_time'].max() - df['date_time'].min())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then detect outliers in 'date' using the IQR method, then one-hot encode the categorical column 'confirmed', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['confirmed'], prefix=['confirmed'])\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'sales', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['sales'], prefix=['sales'])\nprint(df.describe())"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then display feature importances from the Random Forest model, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nprint(df.describe())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then clean text data in column 'departure_delay' by removing punctuation and stopwords, then plot a histogram of 'departure_delay', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['departure_delay_clean'] = df['departure_delay'].apply(clean)\ndf['departure_delay'].hist()\nplt.xlabel('departure_delay')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'temperature', then compute TF-IDF features for column 'temperature' and display top 10 words, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['temperature'])\nprint(vect.get_feature_names_out())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features, then one-hot encode the categorical column 'open'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)\ndf = pd.get_dummies(df, columns=['open'], prefix=['open'])"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then detect outliers in 'Age' using the IQR method, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nQ1 = df['Age'].quantile(0.25)\nQ3 = df['Age'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Age'] < (Q1 - 1.5*IQR)) | (df['Age'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nprint(df.describe())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['wind_speed'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sales', then plot a histogram of 'sales', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['sales'].hist()\nplt.xlabel('sales')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then train a Linear Regression model to predict 'confirmed', then handle missing values in 'confirmed' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['confirmed'].fillna(df['confirmed'].median(), inplace=True)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then compute TF-IDF features for column 'Age' and display top 10 words, then train a Linear Regression model to predict 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Age'])\nprint(vect.get_feature_names_out())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then create a new feature 'Date_ratio' as the ratio of 'Date' to 'Revenue', then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Date_ratio'] = df['Date'] / df['Revenue']\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'UnitsSold' by imputing with median, then create a new feature 'Product_ratio' as the ratio of 'Product' to 'Revenue', then train a Linear Regression model to predict 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['UnitsSold'].fillna(df['UnitsSold'].median(), inplace=True)\ndf['Product_ratio'] = df['Product'] / df['Revenue']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'date' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ncorr = df.corr()\nprint(corr)\nprint(df.describe())\ndf['date'].fillna(df['date'].median(), inplace=True)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'traffic_volume', then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'temperature' by removing punctuation and stopwords, then plot a histogram of 'wind_speed', then normalize the 'precipitation' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['temperature_clean'] = df['temperature'].apply(clean)\ndf['wind_speed'].hist()\nplt.xlabel('wind_speed')\nplt.ylabel('Frequency')\nplt.show()\ndf['precipitation_scaled'] = (df['precipitation'] - df['precipitation'].min()) / (df['precipitation'].max() - df['precipitation'].min())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features, then detect outliers in 'pressure' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)\nQ1 = df['pressure'].quantile(0.25)\nQ3 = df['pressure'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['pressure'] < (Q1 - 1.5*IQR)) | (df['pressure'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'timestamp', then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf = pd.get_dummies(df, columns=['timestamp'], prefix=['timestamp'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sensor_value'])\ny = df['sensor_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'shares', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['shares'], prefix=['shares'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'departure_delay' by imputing with median, then train a Linear Regression model to predict 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nprint(df.describe())\ndf['departure_delay'].fillna(df['departure_delay'].median(), inplace=True)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then detect outliers in 'recovered' using the IQR method, then train a Random Forest Classifier to predict 'confirmed', then create a new feature 'deaths_ratio' as the ratio of 'deaths' to 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nQ1 = df['recovered'].quantile(0.25)\nQ3 = df['recovered'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['recovered'] < (Q1 - 1.5*IQR)) | (df['recovered'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['deaths_ratio'] = df['deaths'] / df['confirmed']"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'temp'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nprint(df.describe())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['temp'], prefix=['temp'])"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'date' and display top 10 words, then train a Random Forest Classifier to predict 'consumption', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date'])\nprint(vect.get_feature_names_out())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then train a Random Forest Classifier to predict 'text', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'sales' and display top 10 words, then perform time-series forecasting using ARIMA to predict the next 12 periods, then one-hot encode the categorical column 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sales'])\nprint(vect.get_feature_names_out())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sales'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf = pd.get_dummies(df, columns=['sales'], prefix=['sales'])"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods, then one-hot encode the categorical column 'Region'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['UnitsSold'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf = pd.get_dummies(df, columns=['Region'], prefix=['Region'])"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then create a new feature 'ContractType_ratio' as the ratio of 'ContractType' to 'Churn', then display feature importances from the Random Forest model, then compute TF-IDF features for column 'Churn' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['ContractType_ratio'] = df['ContractType'] / df['Churn']\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Churn'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Random Forest Classifier to predict 'Churn', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'traffic_volume', then create a new feature 'rain_1h_ratio' as the ratio of 'rain_1h' to 'traffic_volume', then one-hot encode the categorical column 'rain_1h'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['rain_1h_ratio'] = df['rain_1h'] / df['traffic_volume']\ndf = pd.get_dummies(df, columns=['rain_1h'], prefix=['rain_1h'])"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then detect outliers in 'date' using the IQR method, then train a Linear Regression model to predict 'consumption', then create a new feature 'temperature_ratio' as the ratio of 'temperature' to 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['temperature_ratio'] = df['temperature'] / df['consumption']"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then create a new feature 'Date_ratio' as the ratio of 'Date' to 'Close', then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Date_ratio'] = df['Date'] / df['Close']\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'heart_rate' using the IQR method, then perform K-Means clustering with k=3 on numeric features, then create a new feature 'patient_id_ratio' as the ratio of 'patient_id' to 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nQ1 = df['heart_rate'].quantile(0.25)\nQ3 = df['heart_rate'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['heart_rate'] < (Q1 - 1.5*IQR)) | (df['heart_rate'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['patient_id_ratio'] = df['patient_id'] / df['ecg_reading']"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then display feature importances from the Random Forest model, then handle missing values in 'recovered' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nprint(df.describe())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['recovered'].fillna(df['recovered'].median(), inplace=True)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'y', then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features, then plot a histogram of 'newbalanceOrig'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)\ndf['newbalanceOrig'].hist()\nplt.xlabel('newbalanceOrig')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then detect outliers in 'Survived' using the IQR method, then display feature importances from the Random Forest model, then compute TF-IDF features for column 'Sex' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nQ1 = df['Survived'].quantile(0.25)\nQ3 = df['Survived'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Survived'] < (Q1 - 1.5*IQR)) | (df['Survived'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Sex'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe(), then train a Random Forest Classifier to predict 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then normalize the 'UnitsSold' column using min-max scaling, then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['UnitsSold_scaled'] = (df['UnitsSold'] - df['UnitsSold'].min()) / (df['UnitsSold'].max() - df['UnitsSold'].min())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'traffic_volume', then evaluate the model performance using RMSE and R\u00b2 score, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'UnitsSold' by imputing with median, then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['UnitsSold'].fillna(df['UnitsSold'].median(), inplace=True)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Revenue'])\ny = df['Revenue']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then handle missing values in 'customer_id' by imputing with median, then evaluate the model performance using RMSE and R\u00b2 score, then compute TF-IDF features for column 'quantity' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['customer_id'].fillna(df['customer_id'].median(), inplace=True)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['quantity'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then normalize the 'sales' column using min-max scaling, then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['sales_scaled'] = (df['sales'] - df['sales'].min()) / (df['sales'].max() - df['sales'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'ecg_reading', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sales', then create a new feature 'customers_ratio' as the ratio of 'customers' to 'sales', then train a Random Forest Classifier to predict 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['customers_ratio'] = df['customers'] / df['sales']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'shares' column using min-max scaling, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['post_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['shares_scaled'] = (df['shares'] - df['shares'].min()) / (df['shares'].max() - df['shares'].min())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then handle missing values in 'status' by imputing with median, then normalize the 'location' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['status'].fillna(df['status'].median(), inplace=True)\ndf['location_scaled'] = (df['location'] - df['location'].min()) / (df['location'].max() - df['location'].min())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then plot a histogram of 'TotalCharges', then handle missing values in 'Churn' by imputing with median, then detect outliers in 'MonthlyCharges' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['TotalCharges'].hist()\nplt.xlabel('TotalCharges')\nplt.ylabel('Frequency')\nplt.show()\ndf['Churn'].fillna(df['Churn'].median(), inplace=True)\nQ1 = df['MonthlyCharges'].quantile(0.25)\nQ3 = df['MonthlyCharges'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['MonthlyCharges'] < (Q1 - 1.5*IQR)) | (df['MonthlyCharges'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sales', then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then clean text data in column 'patient_id' by removing punctuation and stopwords, then train a Linear Regression model to predict 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['patient_id_clean'] = df['patient_id'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'wind_speed' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score, then plot a histogram of 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['wind_speed'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['temperature'].hist()\nplt.xlabel('temperature')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then one-hot encode the categorical column 'Sex', then train a Linear Regression model to predict 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf = pd.get_dummies(df, columns=['Sex'], prefix=['Sex'])\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then compute TF-IDF features for column 'education' and display top 10 words, then normalize the 'age' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['education'])\nprint(vect.get_feature_names_out())\ndf['age_scaled'] = (df['age'] - df['age'].min()) / (df['age'].max() - df['age'].min())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'traffic_volume', then handle missing values in 'traffic_volume' by imputing with median, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['traffic_volume'].fillna(df['traffic_volume'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score, then detect outliers in 'so2' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nQ1 = df['so2'].quantile(0.25)\nQ3 = df['so2'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['so2'] < (Q1 - 1.5*IQR)) | (df['so2'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'species', then compute TF-IDF features for column 'sepal_width' and display top 10 words, then train a Random Forest Classifier to predict 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sepal_width'])\nprint(vect.get_feature_names_out())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'text', then plot a histogram of 'text', then train a Linear Regression model to predict 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['text'].hist()\nplt.xlabel('text')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then clean text data in column 'species' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'sepal_length' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['species_clean'] = df['species'].apply(clean)\nprint(df.describe())\ndf['sepal_length'].fillna(df['sepal_length'].median(), inplace=True)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'time', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nprint(df.describe())\ndf['time'].hist()\nplt.xlabel('time')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then plot a histogram of 'humidity', then compute TF-IDF features for column 'pressure' and display top 10 words, then train a Linear Regression model to predict 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['humidity'].hist()\nplt.xlabel('humidity')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['pressure'])\nprint(vect.get_feature_names_out())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'oldbalanceOrg', then handle missing values in 'isFraud' by imputing with median, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf = pd.get_dummies(df, columns=['oldbalanceOrg'], prefix=['oldbalanceOrg'])\ndf['isFraud'].fillna(df['isFraud'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then create a new feature 'sensor_value_ratio' as the ratio of 'sensor_value' to 'sensor_value', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['sensor_value_ratio'] = df['sensor_value'] / df['sensor_value']\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'MonthlyCharges' and display top 10 words, then plot a histogram of 'ContractType', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['MonthlyCharges'])\nprint(vect.get_feature_names_out())\ndf['ContractType'].hist()\nplt.xlabel('ContractType')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Close'])\ny = df['Close']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then plot a histogram of 'High', then clean text data in column 'Low' by removing punctuation and stopwords, then compute TF-IDF features for column 'Close' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['High'].hist()\nplt.xlabel('High')\nplt.ylabel('Frequency')\nplt.show()\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Low_clean'] = df['Low'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Close'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split, then normalize the 'confirmed' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['confirmed_scaled'] = (df['confirmed'] - df['confirmed'].min()) / (df['confirmed'].max() - df['confirmed'].min())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'High', then clean text data in column 'High' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nprint(df.describe())\ndf['High'].hist()\nplt.xlabel('High')\nplt.ylabel('Frequency')\nplt.show()\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['High_clean'] = df['High'].apply(clean)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then normalize the 'newbalanceOrig' column using min-max scaling, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nprint(df.describe())\ndf['newbalanceOrig_scaled'] = (df['newbalanceOrig'] - df['newbalanceOrig'].min()) / (df['newbalanceOrig'].max() - df['newbalanceOrig'].min())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['newbalanceOrig'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Revenue', then perform K-Means clustering with k=3 on numeric features, then create a new feature 'Region_ratio' as the ratio of 'Region' to 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf = pd.get_dummies(df, columns=['Revenue'], prefix=['Revenue'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['Region_ratio'] = df['Region'] / df['Revenue']"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'sentiment', then train a Random Forest Classifier to predict 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'temperature' and display top 10 words, then one-hot encode the categorical column 'humidity', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['temperature'])\nprint(vect.get_feature_names_out())\ndf = pd.get_dummies(df, columns=['humidity'], prefix=['humidity'])\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['humidity'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display feature importances from the Random Forest model, then compute TF-IDF features for column 'humidity' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['humidity'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'Revenue' by imputing with median, then detect outliers in 'Date' using the IQR method, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Revenue'].fillna(df['Revenue'].median(), inplace=True)\nQ1 = df['Date'].quantile(0.25)\nQ3 = df['Date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Date'] < (Q1 - 1.5*IQR)) | (df['Date'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then detect outliers in 'High' using the IQR method, then display summary statistics of all numeric columns using df.describe(), then train a Random Forest Classifier to predict 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nQ1 = df['High'].quantile(0.25)\nQ3 = df['High'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['High'] < (Q1 - 1.5*IQR)) | (df['High'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nprint(df.describe())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then create a new feature 'humidity_ratio' as the ratio of 'humidity' to 'consumption', then clean text data in column 'date' by removing punctuation and stopwords, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['humidity_ratio'] = df['humidity'] / df['consumption']\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['humidity'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'humidity', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['humidity'], prefix=['humidity'])\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'text', then plot a histogram of 'user_id', then clean text data in column 'user_id' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['user_id'].hist()\nplt.xlabel('user_id')\nplt.ylabel('Frequency')\nplt.show()\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['user_id_clean'] = df['user_id'].apply(clean)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then create a new feature 'newbalanceOrig_ratio' as the ratio of 'newbalanceOrig' to 'isFraud', then detect outliers in 'newbalanceOrig' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nprint(df.describe())\ndf['newbalanceOrig_ratio'] = df['newbalanceOrig'] / df['isFraud']\nQ1 = df['newbalanceOrig'].quantile(0.25)\nQ3 = df['newbalanceOrig'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['newbalanceOrig'] < (Q1 - 1.5*IQR)) | (df['newbalanceOrig'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Random Forest Classifier to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nprint(df.describe())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['tenure'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then display summary statistics of all numeric columns using df.describe(), then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nprint(df.describe())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'y', then plot a histogram of 'y', then detect outliers in 'marital' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf = pd.get_dummies(df, columns=['y'], prefix=['y'])\ndf['y'].hist()\nplt.xlabel('y')\nplt.ylabel('Frequency')\nplt.show()\nQ1 = df['marital'].quantile(0.25)\nQ3 = df['marital'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['marital'] < (Q1 - 1.5*IQR)) | (df['marital'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then clean text data in column 'wind_speed' by removing punctuation and stopwords, then one-hot encode the categorical column 'humidity'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['wind_speed_clean'] = df['wind_speed'].apply(clean)\ndf = pd.get_dummies(df, columns=['humidity'], prefix=['humidity'])"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform K-Means clustering with k=3 on numeric features, then compute TF-IDF features for column 'store' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nprint(df.describe())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['store'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'transaction_id', then train a Random Forest Classifier to predict 'total_amount', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf = pd.get_dummies(df, columns=['transaction_id'], prefix=['transaction_id'])\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['total_amount'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'quality' by imputing with median, then create a new feature 'quality_ratio' as the ratio of 'quality' to 'ecg_reading', then compute TF-IDF features for column 'heart_rate' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['quality'].fillna(df['quality'].median(), inplace=True)\ndf['quality_ratio'] = df['quality'] / df['ecg_reading']\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['heart_rate'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'pm2_5', then normalize the 'date' column using min-max scaling, then plot a histogram of 'o3'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['date_scaled'] = (df['date'] - df['date'].min()) / (df['date'].max() - df['date'].min())\ndf['o3'].hist()\nplt.xlabel('o3')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Survived', then normalize the 'Pclass' column using min-max scaling, then create a new feature 'Age_ratio' as the ratio of 'Age' to 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['Pclass_scaled'] = (df['Pclass'] - df['Pclass'].min()) / (df['Pclass'].max() - df['Pclass'].min())\ndf['Age_ratio'] = df['Age'] / df['Survived']"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then detect outliers in 'arrival_delay' using the IQR method, then clean text data in column 'departure_delay' by removing punctuation and stopwords, then normalize the 'departure_delay' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nQ1 = df['arrival_delay'].quantile(0.25)\nQ3 = df['arrival_delay'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['arrival_delay'] < (Q1 - 1.5*IQR)) | (df['arrival_delay'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['departure_delay_clean'] = df['departure_delay'].apply(clean)\ndf['departure_delay_scaled'] = (df['departure_delay'] - df['departure_delay'].min()) / (df['departure_delay'].max() - df['departure_delay'].min())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then evaluate the model performance using RMSE and R\u00b2 score, then handle missing values in 'arrival_delay' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['arrival_delay'].fillna(df['arrival_delay'].median(), inplace=True)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sensor_value', then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sensor_value'])\ny = df['sensor_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'ecg_reading' by imputing with median, then display summary statistics of all numeric columns using df.describe(), then clean text data in column 'time' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['ecg_reading'].fillna(df['ecg_reading'].median(), inplace=True)\nprint(df.describe())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['time_clean'] = df['time'].apply(clean)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then create a new feature 'arrival_delay_ratio' as the ratio of 'arrival_delay' to 'arrival_delay', then plot a histogram of 'distance'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['arrival_delay_ratio'] = df['arrival_delay'] / df['arrival_delay']\ndf['distance'].hist()\nplt.xlabel('distance')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sensor_value', then clean text data in column 'timestamp' by removing punctuation and stopwords, then plot a histogram of 'timestamp'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['timestamp_clean'] = df['timestamp'].apply(clean)\ndf['timestamp'].hist()\nplt.xlabel('timestamp')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['ContractType'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe(), then normalize the 'precipitation' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())\ndf['precipitation_scaled'] = (df['precipitation'] - df['precipitation'].min()) / (df['precipitation'].max() - df['precipitation'].min())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then clean text data in column 'Date' by removing punctuation and stopwords, then create a new feature 'Low_ratio' as the ratio of 'Low' to 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Close'])\ny = df['Close']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Date_clean'] = df['Date'].apply(clean)\ndf['Low_ratio'] = df['Low'] / df['Close']"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then plot a histogram of 'pm2_5', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['no2'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['pm2_5'].hist()\nplt.xlabel('pm2_5')\nplt.ylabel('Frequency')\nplt.show()\nprint(df.describe())"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'sensor_value' and display top 10 words, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sensor_value'])\nprint(vect.get_feature_names_out())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then clean text data in column 'temperature' by removing punctuation and stopwords, then normalize the 'date' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['humidity'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['temperature_clean'] = df['temperature'].apply(clean)\ndf['date_scaled'] = (df['date'] - df['date'].min()) / (df['date'].max() - df['date'].min())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'arrival_delay', then plot a histogram of 'departure_delay', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['departure_delay'].hist()\nplt.xlabel('departure_delay')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe(), then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['time'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then evaluate the model performance using RMSE and R\u00b2 score, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'education' and display top 10 words, then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'education'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['education'])\nprint(vect.get_feature_names_out())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['education'], prefix=['education'])"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'pm2_5', then clean text data in column 'o3' by removing punctuation and stopwords, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf = pd.get_dummies(df, columns=['pm2_5'], prefix=['pm2_5'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['o3_clean'] = df['o3'].apply(clean)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then create a new feature 'wind_speed_ratio' as the ratio of 'wind_speed' to 'temperature', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nprint(df.describe())\ndf['wind_speed_ratio'] = df['wind_speed'] / df['temperature']\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then normalize the 'country' column using min-max scaling, then clean text data in column 'deaths' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['country_scaled'] = (df['country'] - df['country'].min()) / (df['country'].max() - df['country'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['deaths_clean'] = df['deaths'].apply(clean)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then clean text data in column 'Churn' by removing punctuation and stopwords, then detect outliers in 'ContractType' using the IQR method, then compute TF-IDF features for column 'Churn' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Churn_clean'] = df['Churn'].apply(clean)\nQ1 = df['ContractType'].quantile(0.25)\nQ3 = df['ContractType'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['ContractType'] < (Q1 - 1.5*IQR)) | (df['ContractType'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Churn'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then one-hot encode the categorical column 'Low', then create a new feature 'Volume_ratio' as the ratio of 'Volume' to 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Open'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf = pd.get_dummies(df, columns=['Low'], prefix=['Low'])\ndf['Volume_ratio'] = df['Volume'] / df['Close']"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then plot a histogram of 'Date', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ncorr = df.corr()\nprint(corr)\ndf['Date'].hist()\nplt.xlabel('Date')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Revenue'])\ny = df['Revenue']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then normalize the 'genre' column using min-max scaling, then plot a histogram of 'length', then create a new feature 'sentiment_ratio' as the ratio of 'sentiment' to 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['genre_scaled'] = (df['genre'] - df['genre'].min()) / (df['genre'].max() - df['genre'].min())\ndf['length'].hist()\nplt.xlabel('length')\nplt.ylabel('Frequency')\nplt.show()\ndf['sentiment_ratio'] = df['sentiment'] / df['sentiment']"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'Close', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Close'])\ny = df['Close']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['Close'].hist()\nplt.xlabel('Close')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform K-Means clustering with k=3 on numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nprint(df.describe())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['amount'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then one-hot encode the categorical column 'isFraud', then handle missing values in 'oldbalanceOrg' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf = pd.get_dummies(df, columns=['isFraud'], prefix=['isFraud'])\ndf['oldbalanceOrg'].fillna(df['oldbalanceOrg'].median(), inplace=True)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'store' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['store'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'isFraud', then detect outliers in 'oldbalanceOrg' using the IQR method, then compute TF-IDF features for column 'oldbalanceOrg' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nQ1 = df['oldbalanceOrg'].quantile(0.25)\nQ3 = df['oldbalanceOrg'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['oldbalanceOrg'] < (Q1 - 1.5*IQR)) | (df['oldbalanceOrg'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['oldbalanceOrg'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then handle missing values in 'sepal_width' by imputing with median, then display summary statistics of all numeric columns using df.describe(), then one-hot encode the categorical column 'sepal_width'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['sepal_width'].fillna(df['sepal_width'].median(), inplace=True)\nprint(df.describe())\ndf = pd.get_dummies(df, columns=['sepal_width'], prefix=['sepal_width'])"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then clean text data in column 'text' by removing punctuation and stopwords, then create a new feature 'post_id_ratio' as the ratio of 'post_id' to 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['text'])\ny = df['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['text_clean'] = df['text'].apply(clean)\ndf['post_id_ratio'] = df['post_id'] / df['text']"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then clean text data in column 'departure_delay' by removing punctuation and stopwords, then train a Linear Regression model to predict 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['carrier'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['departure_delay_clean'] = df['departure_delay'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then handle missing values in 'review' by imputing with median, then split the data into training and testing sets with an 80-20 split, then clean text data in column 'genre' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['review'].fillna(df['review'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['genre_clean'] = df['genre'].apply(clean)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods, then clean text data in column 'distance' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['distance'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['distance_clean'] = df['distance'].apply(clean)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then detect outliers in 'review' using the IQR method, then compute TF-IDF features for column 'sentiment' and display top 10 words, then handle missing values in 'genre' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nQ1 = df['review'].quantile(0.25)\nQ3 = df['review'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['review'] < (Q1 - 1.5*IQR)) | (df['review'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sentiment'])\nprint(vect.get_feature_names_out())\ndf['genre'].fillna(df['genre'].median(), inplace=True)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then normalize the 'recovered' column using min-max scaling, then handle missing values in 'confirmed' by imputing with median, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['recovered_scaled'] = (df['recovered'] - df['recovered'].min()) / (df['recovered'].max() - df['recovered'].min())\ndf['confirmed'].fillna(df['confirmed'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then create a new feature 'SalePrice_ratio' as the ratio of 'SalePrice' to 'SalePrice', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['SalePrice_ratio'] = df['SalePrice'] / df['SalePrice']\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'UnitsSold', then normalize the 'Product' column using min-max scaling, then train a Linear Regression model to predict 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['UnitsSold'].hist()\nplt.xlabel('UnitsSold')\nplt.ylabel('Frequency')\nplt.show()\ndf['Product_scaled'] = (df['Product'] - df['Product'].min()) / (df['Product'].max() - df['Product'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then normalize the 'Age' column using min-max scaling, then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'Sex'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Age_scaled'] = (df['Age'] - df['Age'].min()) / (df['Age'].max() - df['Age'].min())\nprint(df.describe())\ndf['Sex'].hist()\nplt.xlabel('Sex')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'isFraud', then compute TF-IDF features for column 'time' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['time'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then clean text data in column 'Survived' by removing punctuation and stopwords, then train a Random Forest Classifier to predict 'Survived', then compute TF-IDF features for column 'Age' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Survived_clean'] = df['Survived'].apply(clean)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Age'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'temperature', then normalize the 'wind_speed' column using min-max scaling, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['wind_speed_scaled'] = (df['wind_speed'] - df['wind_speed'].min()) / (df['wind_speed'].max() - df['wind_speed'].min())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['wind_speed'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then create a new feature 'total_amount_ratio' as the ratio of 'total_amount' to 'total_amount', then detect outliers in 'total_amount' using the IQR method, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['total_amount_ratio'] = df['total_amount'] / df['total_amount']\nQ1 = df['total_amount'].quantile(0.25)\nQ3 = df['total_amount'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['total_amount'] < (Q1 - 1.5*IQR)) | (df['total_amount'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nprint(df.describe())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then create a new feature 'flight_ratio' as the ratio of 'flight' to 'arrival_delay', then perform K-Means clustering with k=3 on numeric features, then train a Linear Regression model to predict 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['flight_ratio'] = df['flight'] / df['arrival_delay']\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'no2', then split the data into training and testing sets with an 80-20 split, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf = pd.get_dummies(df, columns=['no2'], prefix=['no2'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then handle missing values in 'date_time' by imputing with median, then clean text data in column 'date_time' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['date_time'].fillna(df['date_time'].median(), inplace=True)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_time_clean'] = df['date_time'].apply(clean)\nprint(df.describe())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'y' and display top 10 words, then one-hot encode the categorical column 'marital', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['y'])\nprint(vect.get_feature_names_out())\ndf = pd.get_dummies(df, columns=['marital'], prefix=['marital'])\nprint(df.describe())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then handle missing values in 'TotalCharges' by imputing with median, then one-hot encode the categorical column 'MonthlyCharges', then train a Linear Regression model to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)\ndf = pd.get_dummies(df, columns=['MonthlyCharges'], prefix=['MonthlyCharges'])\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then train a Random Forest Classifier to predict 'text', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'UnitsSold' column using min-max scaling, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['UnitsSold_scaled'] = (df['UnitsSold'] - df['UnitsSold'].min()) / (df['UnitsSold'].max() - df['UnitsSold'].min())\nprint(df.describe())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then clean text data in column 'sentiment' by removing punctuation and stopwords, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sentiment_clean'] = df['sentiment'].apply(clean)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sentiment'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then clean text data in column 'confirmed' by removing punctuation and stopwords, then compute TF-IDF features for column 'country' and display top 10 words, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['confirmed_clean'] = df['confirmed'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['country'])\nprint(vect.get_feature_names_out())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then clean text data in column 'total_amount' by removing punctuation and stopwords, then compute TF-IDF features for column 'product_id' and display top 10 words, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['total_amount_clean'] = df['total_amount'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['product_id'])\nprint(vect.get_feature_names_out())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then split the data into training and testing sets with an 80-20 split, then clean text data in column 'amount' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['amount_clean'] = df['amount'].apply(clean)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'MonthlyCharges' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nprint(df.describe())\nQ1 = df['MonthlyCharges'].quantile(0.25)\nQ3 = df['MonthlyCharges'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['MonthlyCharges'] < (Q1 - 1.5*IQR)) | (df['MonthlyCharges'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'country', then evaluate the model performance using RMSE and R\u00b2 score, then clean text data in column 'country' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf = pd.get_dummies(df, columns=['country'], prefix=['country'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['country_clean'] = df['country'].apply(clean)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'wind_speed', then plot a histogram of 'date', then handle missing values in 'humidity' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf = pd.get_dummies(df, columns=['wind_speed'], prefix=['wind_speed'])\ndf['date'].hist()\nplt.xlabel('date')\nplt.ylabel('Frequency')\nplt.show()\ndf['humidity'].fillna(df['humidity'].median(), inplace=True)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'species', then one-hot encode the categorical column 'sepal_length', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['sepal_length'], prefix=['sepal_length'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features, then one-hot encode the categorical column 'time'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)\ndf = pd.get_dummies(df, columns=['time'], prefix=['time'])"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'traffic_volume', then one-hot encode the categorical column 'traffic_volume', then clean text data in column 'snow_1h' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['traffic_volume'], prefix=['traffic_volume'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['snow_1h_clean'] = df['snow_1h'].apply(clean)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'o3' column using min-max scaling, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['o3_scaled'] = (df['o3'] - df['o3'].min()) / (df['o3'].max() - df['o3'].min())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'date' and display top 10 words, then detect outliers in 'country' using the IQR method, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date'])\nprint(vect.get_feature_names_out())\nQ1 = df['country'].quantile(0.25)\nQ3 = df['country'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['country'] < (Q1 - 1.5*IQR)) | (df['country'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then normalize the 'country' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['country_scaled'] = (df['country'] - df['country'].min()) / (df['country'].max() - df['country'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'Sex', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nprint(df.describe())\ndf['Sex'].hist()\nplt.xlabel('Sex')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'customer_id', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['customer_id'], prefix=['customer_id'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'Churn' column using min-max scaling, then train a Random Forest Classifier to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['Churn_scaled'] = (df['Churn'] - df['Churn'].min()) / (df['Churn'].max() - df['Churn'].min())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then clean text data in column 'ContractType' by removing punctuation and stopwords, then normalize the 'TotalCharges' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['TotalCharges'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['ContractType_clean'] = df['ContractType'].apply(clean)\ndf['TotalCharges_scaled'] = (df['TotalCharges'] - df['TotalCharges'].min()) / (df['TotalCharges'].max() - df['TotalCharges'].min())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe(), then one-hot encode the categorical column 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())\ndf = pd.get_dummies(df, columns=['temperature'], prefix=['temperature'])"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'rain_1h' and display top 10 words, then split the data into training and testing sets with an 80-20 split, then normalize the 'traffic_volume' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['rain_1h'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['traffic_volume_scaled'] = (df['traffic_volume'] - df['traffic_volume'].min()) / (df['traffic_volume'].max() - df['traffic_volume'].min())"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'YearBuilt' and display top 10 words, then split the data into training and testing sets with an 80-20 split, then one-hot encode the categorical column 'OverallQual'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['YearBuilt'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf = pd.get_dummies(df, columns=['OverallQual'], prefix=['OverallQual'])"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then handle missing values in 'confirmed' by imputing with median, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['confirmed'].fillna(df['confirmed'].median(), inplace=True)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['confirmed'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then create a new feature 'date_ratio' as the ratio of 'date' to 'consumption', then detect outliers in 'consumption' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['date_ratio'] = df['date'] / df['consumption']\nQ1 = df['consumption'].quantile(0.25)\nQ3 = df['consumption'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['consumption'] < (Q1 - 1.5*IQR)) | (df['consumption'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Churn', then calculate the correlation matrix for numeric features, then create a new feature 'TotalCharges_ratio' as the ratio of 'TotalCharges' to 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)\ndf['TotalCharges_ratio'] = df['TotalCharges'] / df['Churn']"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'YearBuilt' and display top 10 words, then calculate the correlation matrix for numeric features, then normalize the 'LotArea' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['YearBuilt'])\nprint(vect.get_feature_names_out())\ncorr = df.corr()\nprint(corr)\ndf['LotArea_scaled'] = (df['LotArea'] - df['LotArea'].min()) / (df['LotArea'].max() - df['LotArea'].min())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Churn', then one-hot encode the categorical column 'Churn', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['Churn'], prefix=['Churn'])\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then normalize the 'temperature' column using min-max scaling, then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['temperature_scaled'] = (df['temperature'] - df['temperature'].min()) / (df['temperature'].max() - df['temperature'].min())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['consumption'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then detect outliers in 'age' using the IQR method, then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nQ1 = df['age'].quantile(0.25)\nQ3 = df['age'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['age'] < (Q1 - 1.5*IQR)) | (df['age'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'total_amount', then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then handle missing values in 'job' by imputing with median, then perform K-Means clustering with k=3 on numeric features, then create a new feature 'marital_ratio' as the ratio of 'marital' to 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['job'].fillna(df['job'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['marital_ratio'] = df['marital'] / df['y']"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform time-series forecasting using ARIMA to predict the next 12 periods, then plot a histogram of 'location'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nprint(df.describe())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['timestamp'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['location'].hist()\nplt.xlabel('location')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'post_id', then display summary statistics of all numeric columns using df.describe(), then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf = pd.get_dummies(df, columns=['post_id'], prefix=['post_id'])\nprint(df.describe())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['text'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'total_amount', then plot a histogram of 'total_amount', then normalize the 'quantity' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf = pd.get_dummies(df, columns=['total_amount'], prefix=['total_amount'])\ndf['total_amount'].hist()\nplt.xlabel('total_amount')\nplt.ylabel('Frequency')\nplt.show()\ndf['quantity_scaled'] = (df['quantity'] - df['quantity'].min()) / (df['quantity'].max() - df['quantity'].min())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then handle missing values in 'date' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['date'].fillna(df['date'].median(), inplace=True)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then plot a histogram of 'date', then handle missing values in 'humidity' by imputing with median, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['date'].hist()\nplt.xlabel('date')\nplt.ylabel('Frequency')\nplt.show()\ndf['humidity'].fillna(df['humidity'].median(), inplace=True)\nprint(df.describe())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'open', then compute TF-IDF features for column 'open' and display top 10 words, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf = pd.get_dummies(df, columns=['open'], prefix=['open'])\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['open'])\nprint(vect.get_feature_names_out())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['store'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then handle missing values in 'isFraud' by imputing with median, then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'isFraud' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['isFraud'].fillna(df['isFraud'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['isFraud'].quantile(0.25)\nQ3 = df['isFraud'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['isFraud'] < (Q1 - 1.5*IQR)) | (df['isFraud'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'UnitsSold' using the IQR method, then display feature importances from the Random Forest model, then clean text data in column 'Revenue' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nQ1 = df['UnitsSold'].quantile(0.25)\nQ3 = df['UnitsSold'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['UnitsSold'] < (Q1 - 1.5*IQR)) | (df['UnitsSold'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Revenue_clean'] = df['Revenue'].apply(clean)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then normalize the 'shares' column using min-max scaling, then create a new feature 'post_id_ratio' as the ratio of 'post_id' to 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['shares_scaled'] = (df['shares'] - df['shares'].min()) / (df['shares'].max() - df['shares'].min())\ndf['post_id_ratio'] = df['post_id'] / df['text']"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'Survived' and display top 10 words, then normalize the 'Sex' column using min-max scaling, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Survived'])\nprint(vect.get_feature_names_out())\ndf['Sex_scaled'] = (df['Sex'] - df['Sex'].min()) / (df['Sex'].max() - df['Sex'].min())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Pclass'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then create a new feature 'pm10_ratio' as the ratio of 'pm10' to 'pm2_5', then display feature importances from the Random Forest model, then detect outliers in 'o3' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['pm10_ratio'] = df['pm10'] / df['pm2_5']\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['o3'].quantile(0.25)\nQ3 = df['o3'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['o3'] < (Q1 - 1.5*IQR)) | (df['o3'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'distance', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['distance'].hist()\nplt.xlabel('distance')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then create a new feature 'patient_id_ratio' as the ratio of 'patient_id' to 'ecg_reading', then compute TF-IDF features for column 'heart_rate' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['patient_id_ratio'] = df['patient_id'] / df['ecg_reading']\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['heart_rate'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then plot a histogram of 'pressure', then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'date_ratio' as the ratio of 'date' to 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['pressure'].hist()\nplt.xlabel('pressure')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['humidity'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['date_ratio'] = df['date'] / df['consumption']"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then handle missing values in 'marital' by imputing with median, then calculate the correlation matrix for numeric features, then create a new feature 'marital_ratio' as the ratio of 'marital' to 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['marital'].fillna(df['marital'].median(), inplace=True)\ncorr = df.corr()\nprint(corr)\ndf['marital_ratio'] = df['marital'] / df['y']"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then create a new feature 'time_ratio' as the ratio of 'time' to 'isFraud', then evaluate the model performance using RMSE and R\u00b2 score, then handle missing values in 'isFraud' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['time_ratio'] = df['time'] / df['isFraud']\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['isFraud'].fillna(df['isFraud'].median(), inplace=True)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Neighborhood', then one-hot encode the categorical column 'SalePrice', then normalize the 'OverallQual' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['Neighborhood'].hist()\nplt.xlabel('Neighborhood')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['SalePrice'], prefix=['SalePrice'])\ndf['OverallQual_scaled'] = (df['OverallQual'] - df['OverallQual'].min()) / (df['OverallQual'].max() - df['OverallQual'].min())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'job' and display top 10 words, then detect outliers in 'job' using the IQR method, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['job'])\nprint(vect.get_feature_names_out())\nQ1 = df['job'].quantile(0.25)\nQ3 = df['job'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['job'] < (Q1 - 1.5*IQR)) | (df['job'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'departure_delay' and display top 10 words, then handle missing values in 'flight' by imputing with median, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['departure_delay'])\nprint(vect.get_feature_names_out())\ndf['flight'].fillna(df['flight'].median(), inplace=True)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'Pclass' by imputing with median, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nprint(df.describe())\ndf['Pclass'].fillna(df['Pclass'].median(), inplace=True)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'shares', then normalize the 'shares' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nprint(df.describe())\ndf['shares'].hist()\nplt.xlabel('shares')\nplt.ylabel('Frequency')\nplt.show()\ndf['shares_scaled'] = (df['shares'] - df['shares'].min()) / (df['shares'].max() - df['shares'].min())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'humidity' and display top 10 words, then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['humidity'])\nprint(vect.get_feature_names_out())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then handle missing values in 'time' by imputing with median, then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'isFraud' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['time'].fillna(df['time'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['isFraud'].quantile(0.25)\nQ3 = df['isFraud'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['isFraud'] < (Q1 - 1.5*IQR)) | (df['isFraud'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'MonthlyCharges' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nprint(df.describe())\nQ1 = df['MonthlyCharges'].quantile(0.25)\nQ3 = df['MonthlyCharges'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['MonthlyCharges'] < (Q1 - 1.5*IQR)) | (df['MonthlyCharges'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'sales' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())\nQ1 = df['sales'].quantile(0.25)\nQ3 = df['sales'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sales'] < (Q1 - 1.5*IQR)) | (df['sales'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'country', then train a Linear Regression model to predict 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['country'], prefix=['country'])\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'rating', then train a Random Forest Classifier to predict 'sentiment', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf = pd.get_dummies(df, columns=['rating'], prefix=['rating'])\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'Survived', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['Survived'].hist()\nplt.xlabel('Survived')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then handle missing values in 'date' by imputing with median, then normalize the 'so2' column using min-max scaling, then compute TF-IDF features for column 'pm10' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['date'].fillna(df['date'].median(), inplace=True)\ndf['so2_scaled'] = (df['so2'] - df['so2'].min()) / (df['so2'].max() - df['so2'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['pm10'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then normalize the 'date_time' column using min-max scaling, then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'date_time' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['date_time_scaled'] = (df['date_time'] - df['date_time'].min()) / (df['date_time'].max() - df['date_time'].min())\nprint(df.describe())\nQ1 = df['date_time'].quantile(0.25)\nQ3 = df['date_time'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date_time'] < (Q1 - 1.5*IQR)) | (df['date_time'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then create a new feature 'genre_ratio' as the ratio of 'genre' to 'sentiment', then one-hot encode the categorical column 'length', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['genre_ratio'] = df['genre'] / df['sentiment']\ndf = pd.get_dummies(df, columns=['length'], prefix=['length'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then create a new feature 'consumption_ratio' as the ratio of 'consumption' to 'consumption', then compute TF-IDF features for column 'humidity' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['consumption_ratio'] = df['consumption'] / df['consumption']\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['humidity'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then detect outliers in 'departure_delay' using the IQR method, then one-hot encode the categorical column 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nQ1 = df['departure_delay'].quantile(0.25)\nQ3 = df['departure_delay'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['departure_delay'] < (Q1 - 1.5*IQR)) | (df['departure_delay'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['arrival_delay'], prefix=['arrival_delay'])"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then create a new feature 'wind_speed_ratio' as the ratio of 'wind_speed' to 'temperature', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['wind_speed_ratio'] = df['wind_speed'] / df['temperature']\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then plot a histogram of 'total_amount', then handle missing values in 'customer_id' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['total_amount'].hist()\nplt.xlabel('total_amount')\nplt.ylabel('Frequency')\nplt.show()\ndf['customer_id'].fillna(df['customer_id'].median(), inplace=True)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then handle missing values in 'time' by imputing with median, then train a Linear Regression model to predict 'isFraud', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['time'].fillna(df['time'].median(), inplace=True)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Linear Regression model to predict 'Revenue', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nprint(df.describe())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Revenue'])\ny = df['Revenue']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'customers', then evaluate the model performance using RMSE and R\u00b2 score, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf = pd.get_dummies(df, columns=['customers'], prefix=['customers'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'Churn', then plot a histogram of 'ContractType'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['ContractType'].hist()\nplt.xlabel('ContractType')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then handle missing values in 'petal_length' by imputing with median, then evaluate the model performance using RMSE and R\u00b2 score, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['petal_length'].fillna(df['petal_length'].median(), inplace=True)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nprint(df.describe())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'no2'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['no2'], prefix=['no2'])"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'consumption', then plot a histogram of 'date', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['date'].hist()\nplt.xlabel('date')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then create a new feature 'traffic_volume_ratio' as the ratio of 'traffic_volume' to 'traffic_volume', then calculate the correlation matrix for numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['traffic_volume_ratio'] = df['traffic_volume'] / df['traffic_volume']\ncorr = df.corr()\nprint(corr)\nprint(df.describe())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'user_id' and display top 10 words, then one-hot encode the categorical column 'text', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['user_id'])\nprint(vect.get_feature_names_out())\ndf = pd.get_dummies(df, columns=['text'], prefix=['text'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then clean text data in column 'MonthlyCharges' by removing punctuation and stopwords, then train a Linear Regression model to predict 'Churn', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['MonthlyCharges_clean'] = df['MonthlyCharges'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then detect outliers in 'SalePrice' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['SalePrice'].quantile(0.25)\nQ3 = df['SalePrice'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['SalePrice'] < (Q1 - 1.5*IQR)) | (df['SalePrice'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)\nprint(df.describe())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['ecg_reading'])\ny = df['ecg_reading']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Revenue', then calculate the correlation matrix for numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Revenue'].hist()\nplt.xlabel('Revenue')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)\nprint(df.describe())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sentiment', then split the data into training and testing sets with an 80-20 split, then create a new feature 'review_ratio' as the ratio of 'review' to 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['review_ratio'] = df['review'] / df['sentiment']"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'traffic_volume', then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then clean text data in column 'date' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe(), then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)\nprint(df.describe())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then create a new feature 'likes_ratio' as the ratio of 'likes' to 'text', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['likes_ratio'] = df['likes'] / df['text']\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then handle missing values in 'date' by imputing with median, then perform time-series forecasting using ARIMA to predict the next 12 periods, then compute TF-IDF features for column 'pm2_5' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['date'].fillna(df['date'].median(), inplace=True)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['pm10'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['pm2_5'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then detect outliers in 'precipitation' using the IQR method, then plot a histogram of 'wind_speed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nQ1 = df['precipitation'].quantile(0.25)\nQ3 = df['precipitation'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['precipitation'] < (Q1 - 1.5*IQR)) | (df['precipitation'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['wind_speed'].hist()\nplt.xlabel('wind_speed')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then detect outliers in 'status' using the IQR method, then train a Random Forest Classifier to predict 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sensor_value'])\ny = df['sensor_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['status'].quantile(0.25)\nQ3 = df['status'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['status'] < (Q1 - 1.5*IQR)) | (df['status'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then handle missing values in 'sentiment' by imputing with median, then detect outliers in 'sentiment' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['sentiment'].fillna(df['sentiment'].median(), inplace=True)\nQ1 = df['sentiment'].quantile(0.25)\nQ3 = df['sentiment'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sentiment'] < (Q1 - 1.5*IQR)) | (df['sentiment'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'MonthlyCharges' column using min-max scaling, then create a new feature 'tenure_ratio' as the ratio of 'tenure' to 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['MonthlyCharges_scaled'] = (df['MonthlyCharges'] - df['MonthlyCharges'].min()) / (df['MonthlyCharges'].max() - df['MonthlyCharges'].min())\ndf['tenure_ratio'] = df['tenure'] / df['Churn']"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then handle missing values in 'no2' by imputing with median, then detect outliers in 'pm10' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['no2'].fillna(df['no2'].median(), inplace=True)\nQ1 = df['pm10'].quantile(0.25)\nQ3 = df['pm10'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['pm10'] < (Q1 - 1.5*IQR)) | (df['pm10'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then create a new feature 'Fare_ratio' as the ratio of 'Fare' to 'Survived', then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Fare_ratio'] = df['Fare'] / df['Survived']\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Fare'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then clean text data in column 'age' by removing punctuation and stopwords, then train a Random Forest Classifier to predict 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['age_clean'] = df['age'].apply(clean)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'sepal_length' and display top 10 words, then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sepal_length'])\nprint(vect.get_feature_names_out())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'y', then detect outliers in 'education' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['education'].quantile(0.25)\nQ3 = df['education'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['education'] < (Q1 - 1.5*IQR)) | (df['education'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform K-Means clustering with k=3 on numeric features, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nprint(df.describe())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then create a new feature 'time_ratio' as the ratio of 'time' to 'isFraud', then clean text data in column 'oldbalanceOrg' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['time_ratio'] = df['time'] / df['isFraud']\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['oldbalanceOrg_clean'] = df['oldbalanceOrg'].apply(clean)\nprint(df.describe())"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'sensor_value', then train a Random Forest Classifier to predict 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sensor_value'])\ny = df['sensor_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'ecg_reading', then one-hot encode the categorical column 'time', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['time'], prefix=['time'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then plot a histogram of 'LotArea', then calculate the correlation matrix for numeric features, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['LotArea'].hist()\nplt.xlabel('LotArea')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then clean text data in column 'confirmed' by removing punctuation and stopwords, then handle missing values in 'country' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ncorr = df.corr()\nprint(corr)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['confirmed_clean'] = df['confirmed'].apply(clean)\ndf['country'].fillna(df['country'].median(), inplace=True)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'sensor_value', then train a Random Forest Classifier to predict 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nprint(df.describe())\ndf['sensor_value'].hist()\nplt.xlabel('sensor_value')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then compute TF-IDF features for column 'flight' and display top 10 words, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['flight'])\nprint(vect.get_feature_names_out())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then handle missing values in 'rain_1h' by imputing with median, then compute TF-IDF features for column 'rain_1h' and display top 10 words, then train a Random Forest Classifier to predict 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['rain_1h'].fillna(df['rain_1h'].median(), inplace=True)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['rain_1h'])\nprint(vect.get_feature_names_out())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Revenue', then detect outliers in 'Region' using the IQR method, then plot a histogram of 'UnitsSold'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf = pd.get_dummies(df, columns=['Revenue'], prefix=['Revenue'])\nQ1 = df['Region'].quantile(0.25)\nQ3 = df['Region'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Region'] < (Q1 - 1.5*IQR)) | (df['Region'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['UnitsSold'].hist()\nplt.xlabel('UnitsSold')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'YearBuilt', then create a new feature 'SalePrice_ratio' as the ratio of 'SalePrice' to 'SalePrice', then compute TF-IDF features for column 'SalePrice' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf = pd.get_dummies(df, columns=['YearBuilt'], prefix=['YearBuilt'])\ndf['SalePrice_ratio'] = df['SalePrice'] / df['SalePrice']\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['SalePrice'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods, then compute TF-IDF features for column 'user_id' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['text'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['user_id'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sensor_value'])\ny = df['sensor_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then normalize the 'species' column using min-max scaling, then perform K-Means clustering with k=3 on numeric features, then clean text data in column 'species' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['species_scaled'] = (df['species'] - df['species'].min()) / (df['species'].max() - df['species'].min())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['species_clean'] = df['species'].apply(clean)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then normalize the 'genre' column using min-max scaling, then train a Random Forest Classifier to predict 'sentiment', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['genre_scaled'] = (df['genre'] - df['genre'].min()) / (df['genre'].max() - df['genre'].min())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'timestamp' using the IQR method, then train a Linear Regression model to predict 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['timestamp'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['timestamp'].quantile(0.25)\nQ3 = df['timestamp'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['timestamp'] < (Q1 - 1.5*IQR)) | (df['timestamp'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'Churn', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['ContractType'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'text', then display feature importances from the Random Forest model, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then plot a histogram of 'sales', then normalize the 'day_of_week' column using min-max scaling, then clean text data in column 'sales' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['sales'].hist()\nplt.xlabel('sales')\nplt.ylabel('Frequency')\nplt.show()\ndf['day_of_week_scaled'] = (df['day_of_week'] - df['day_of_week'].min()) / (df['day_of_week'].max() - df['day_of_week'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sales_clean'] = df['sales'].apply(clean)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'location' by removing punctuation and stopwords, then normalize the 'location' column using min-max scaling, then plot a histogram of 'timestamp'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['location_clean'] = df['location'].apply(clean)\ndf['location_scaled'] = (df['location'] - df['location'].min()) / (df['location'].max() - df['location'].min())\ndf['timestamp'].hist()\nplt.xlabel('timestamp')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then create a new feature 'Sex_ratio' as the ratio of 'Sex' to 'Survived', then train a Random Forest Classifier to predict 'Survived', then train a Linear Regression model to predict 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Sex_ratio'] = df['Sex'] / df['Survived']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'ecg_reading' and display top 10 words, then clean text data in column 'quality' by removing punctuation and stopwords, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['ecg_reading'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['quality_clean'] = df['quality'].apply(clean)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'y', then display feature importances from the Random Forest model, then plot a histogram of 'marital'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['marital'].hist()\nplt.xlabel('marital')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'UnitsSold' and display top 10 words, then train a Random Forest Classifier to predict 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['UnitsSold'])\nprint(vect.get_feature_names_out())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'flight' column using min-max scaling, then handle missing values in 'arrival_delay' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['flight_scaled'] = (df['flight'] - df['flight'].min()) / (df['flight'].max() - df['flight'].min())\ndf['arrival_delay'].fillna(df['arrival_delay'].median(), inplace=True)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'sensor_value', then one-hot encode the categorical column 'status', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['sensor_value'].hist()\nplt.xlabel('sensor_value')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['status'], prefix=['status'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then normalize the 'location' column using min-max scaling, then handle missing values in 'timestamp' by imputing with median, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['location_scaled'] = (df['location'] - df['location'].min()) / (df['location'].max() - df['location'].min())\ndf['timestamp'].fillna(df['timestamp'].median(), inplace=True)\nprint(df.describe())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then clean text data in column 'OverallQual' by removing punctuation and stopwords, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ncorr = df.corr()\nprint(corr)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['OverallQual_clean'] = df['OverallQual'].apply(clean)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['YearBuilt'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then normalize the 'deaths' column using min-max scaling, then compute TF-IDF features for column 'deaths' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['deaths_scaled'] = (df['deaths'] - df['deaths'].min()) / (df['deaths'].max() - df['deaths'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['deaths'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sales', then display feature importances from the Random Forest model, then normalize the 'store' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['store_scaled'] = (df['store'] - df['store'].min()) / (df['store'].max() - df['store'].min())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then handle missing values in 'snow_1h' by imputing with median, then evaluate the model performance using RMSE and R\u00b2 score, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['snow_1h'].fillna(df['snow_1h'].median(), inplace=True)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Low', then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'Open' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Low'].hist()\nplt.xlabel('Low')\nplt.ylabel('Frequency')\nplt.show()\nprint(df.describe())\ndf['Open'].fillna(df['Open'].median(), inplace=True)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then plot a histogram of 'shares', then one-hot encode the categorical column 'post_id', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['shares'].hist()\nplt.xlabel('shares')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['post_id'], prefix=['post_id'])\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then create a new feature 'temperature_ratio' as the ratio of 'temperature' to 'temperature', then clean text data in column 'wind_speed' by removing punctuation and stopwords, then normalize the 'precipitation' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['temperature_ratio'] = df['temperature'] / df['temperature']\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['wind_speed_clean'] = df['wind_speed'].apply(clean)\ndf['precipitation_scaled'] = (df['precipitation'] - df['precipitation'].min()) / (df['precipitation'].max() - df['precipitation'].min())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then handle missing values in 'customers' by imputing with median, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['customers'].fillna(df['customers'].median(), inplace=True)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['store'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'departure_delay' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['departure_delay'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nprint(df.describe())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then display summary statistics of all numeric columns using df.describe(), then create a new feature 'precipitation_ratio' as the ratio of 'precipitation' to 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ncorr = df.corr()\nprint(corr)\nprint(df.describe())\ndf['precipitation_ratio'] = df['precipitation'] / df['temperature']"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'LotArea'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['LotArea'], prefix=['LotArea'])"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'recovered', then detect outliers in 'recovered' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['recovered'], prefix=['recovered'])\nQ1 = df['recovered'].quantile(0.25)\nQ3 = df['recovered'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['recovered'] < (Q1 - 1.5*IQR)) | (df['recovered'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then handle missing values in 'transaction_id' by imputing with median, then detect outliers in 'transaction_id' using the IQR method, then normalize the 'total_amount' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['transaction_id'].fillna(df['transaction_id'].median(), inplace=True)\nQ1 = df['transaction_id'].quantile(0.25)\nQ3 = df['transaction_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['transaction_id'] < (Q1 - 1.5*IQR)) | (df['transaction_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['total_amount_scaled'] = (df['total_amount'] - df['total_amount'].min()) / (df['total_amount'].max() - df['total_amount'].min())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then create a new feature 'sepal_width_ratio' as the ratio of 'sepal_width' to 'species', then handle missing values in 'petal_length' by imputing with median, then normalize the 'sepal_width' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['sepal_width_ratio'] = df['sepal_width'] / df['species']\ndf['petal_length'].fillna(df['petal_length'].median(), inplace=True)\ndf['sepal_width_scaled'] = (df['sepal_width'] - df['sepal_width'].min()) / (df['sepal_width'].max() - df['sepal_width'].min())"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Fare', then train a Random Forest Classifier to predict 'Survived', then train a Linear Regression model to predict 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Fare'].hist()\nplt.xlabel('Fare')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'ecg_reading', then plot a histogram of 'heart_rate', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['heart_rate'].hist()\nplt.xlabel('heart_rate')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then plot a histogram of 'store', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['store'].hist()\nplt.xlabel('store')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then detect outliers in 'user_id' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods, then compute TF-IDF features for column 'likes' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nQ1 = df['user_id'].quantile(0.25)\nQ3 = df['user_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['user_id'] < (Q1 - 1.5*IQR)) | (df['user_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['likes'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['likes'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then clean text data in column 'sensor_value' by removing punctuation and stopwords, then one-hot encode the categorical column 'device_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ncorr = df.corr()\nprint(corr)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sensor_value_clean'] = df['sensor_value'].apply(clean)\ndf = pd.get_dummies(df, columns=['device_id'], prefix=['device_id'])"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then detect outliers in 'distance' using the IQR method, then display summary statistics of all numeric columns using df.describe(), then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nQ1 = df['distance'].quantile(0.25)\nQ3 = df['distance'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['distance'] < (Q1 - 1.5*IQR)) | (df['distance'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nprint(df.describe())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then normalize the 'Sex' column using min-max scaling, then split the data into training and testing sets with an 80-20 split, then detect outliers in 'Survived' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Sex_scaled'] = (df['Sex'] - df['Sex'].min()) / (df['Sex'].max() - df['Sex'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['Survived'].quantile(0.25)\nQ3 = df['Survived'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Survived'] < (Q1 - 1.5*IQR)) | (df['Survived'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then clean text data in column 'quality' by removing punctuation and stopwords, then normalize the 'time' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['heart_rate'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['quality_clean'] = df['quality'].apply(clean)\ndf['time_scaled'] = (df['time'] - df['time'].min()) / (df['time'].max() - df['time'].min())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe(), then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'temperature', then handle missing values in 'precipitation' by imputing with median, then plot a histogram of 'precipitation'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['precipitation'].fillna(df['precipitation'].median(), inplace=True)\ndf['precipitation'].hist()\nplt.xlabel('precipitation')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then handle missing values in 'MonthlyCharges' by imputing with median, then display feature importances from the Random Forest model, then train a Random Forest Classifier to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['MonthlyCharges'].fillna(df['MonthlyCharges'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then plot a histogram of 'temp', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['temp'].hist()\nplt.xlabel('temp')\nplt.ylabel('Frequency')\nplt.show()\nprint(df.describe())"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then detect outliers in 'Age' using the IQR method, then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nQ1 = df['Age'].quantile(0.25)\nQ3 = df['Age'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Age'] < (Q1 - 1.5*IQR)) | (df['Age'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'date' and display top 10 words, then one-hot encode the categorical column 'humidity', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date'])\nprint(vect.get_feature_names_out())\ndf = pd.get_dummies(df, columns=['humidity'], prefix=['humidity'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'traffic_volume' and display top 10 words, then display feature importances from the Random Forest model, then clean text data in column 'snow_1h' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['traffic_volume'])\nprint(vect.get_feature_names_out())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['snow_1h_clean'] = df['snow_1h'].apply(clean)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'arrival_delay', then split the data into training and testing sets with an 80-20 split, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'Neighborhood' and display top 10 words, then split the data into training and testing sets with an 80-20 split, then detect outliers in 'YearBuilt' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Neighborhood'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['YearBuilt'].quantile(0.25)\nQ3 = df['YearBuilt'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['YearBuilt'] < (Q1 - 1.5*IQR)) | (df['YearBuilt'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'pm2_5', then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'so2'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['so2'], prefix=['so2'])"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'confirmed', then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf = pd.get_dummies(df, columns=['confirmed'], prefix=['confirmed'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then handle missing values in 'temperature' by imputing with median, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['temperature'].fillna(df['temperature'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then plot a histogram of 'pressure', then clean text data in column 'temperature' by removing punctuation and stopwords, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['pressure'].hist()\nplt.xlabel('pressure')\nplt.ylabel('Frequency')\nplt.show()\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['temperature_clean'] = df['temperature'].apply(clean)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'y', then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'y' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['education'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['y'].quantile(0.25)\nQ3 = df['y'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['y'] < (Q1 - 1.5*IQR)) | (df['y'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then plot a histogram of 'Date', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['Date'].hist()\nplt.xlabel('Date')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Close'])\ny = df['Close']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then clean text data in column 'o3' by removing punctuation and stopwords, then perform K-Means clustering with k=3 on numeric features, then plot a histogram of 'o3'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['o3_clean'] = df['o3'].apply(clean)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['o3'].hist()\nplt.xlabel('o3')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then detect outliers in 'y' using the IQR method, then create a new feature 'job_ratio' as the ratio of 'job' to 'y', then compute TF-IDF features for column 'job' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nQ1 = df['y'].quantile(0.25)\nQ3 = df['y'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['y'] < (Q1 - 1.5*IQR)) | (df['y'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['job_ratio'] = df['job'] / df['y']\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['job'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then split the data into training and testing sets with an 80-20 split, then clean text data in column 'customer_id' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nprint(df.describe())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['customer_id_clean'] = df['customer_id'].apply(clean)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then normalize the 'MonthlyCharges' column using min-max scaling, then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'MonthlyCharges'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['MonthlyCharges_scaled'] = (df['MonthlyCharges'] - df['MonthlyCharges'].min()) / (df['MonthlyCharges'].max() - df['MonthlyCharges'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['MonthlyCharges'].hist()\nplt.xlabel('MonthlyCharges')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sensor_value', then detect outliers in 'status' using the IQR method, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['status'].quantile(0.25)\nQ3 = df['status'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['status'] < (Q1 - 1.5*IQR)) | (df['status'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then plot a histogram of 'day_of_week', then create a new feature 'sales_ratio' as the ratio of 'sales' to 'sales', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['day_of_week'].hist()\nplt.xlabel('day_of_week')\nplt.ylabel('Frequency')\nplt.show()\ndf['sales_ratio'] = df['sales'] / df['sales']\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Random Forest Classifier to predict 'total_amount', then clean text data in column 'product_id' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['transaction_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['product_id_clean'] = df['product_id'].apply(clean)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then plot a histogram of 'date', then one-hot encode the categorical column 'date', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['date'].hist()\nplt.xlabel('date')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['date'], prefix=['date'])\nprint(df.describe())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'Date'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['High'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['Date'], prefix=['Date'])"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then calculate the correlation matrix for numeric features, then train a Random Forest Classifier to predict 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then handle missing values in 'pm2_5' by imputing with median, then clean text data in column 'o3' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['pm2_5'].fillna(df['pm2_5'].median(), inplace=True)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['o3_clean'] = df['o3'].apply(clean)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'patient_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['patient_id'], prefix=['patient_id'])"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'ecg_reading', then detect outliers in 'ecg_reading' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['ecg_reading'].quantile(0.25)\nQ3 = df['ecg_reading'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['ecg_reading'] < (Q1 - 1.5*IQR)) | (df['ecg_reading'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then compute TF-IDF features for column 'country' and display top 10 words, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nprint(df.describe())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['country'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then create a new feature 'deaths_ratio' as the ratio of 'deaths' to 'confirmed', then plot a histogram of 'date', then one-hot encode the categorical column 'country'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['deaths_ratio'] = df['deaths'] / df['confirmed']\ndf['date'].hist()\nplt.xlabel('date')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['country'], prefix=['country'])"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'consumption', then create a new feature 'consumption_ratio' as the ratio of 'consumption' to 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['consumption'], prefix=['consumption'])\ndf['consumption_ratio'] = df['consumption'] / df['consumption']"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then clean text data in column 'date' by removing punctuation and stopwords, then display feature importances from the Random Forest model, then normalize the 'pressure' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['pressure_scaled'] = (df['pressure'] - df['pressure'].min()) / (df['pressure'].max() - df['pressure'].min())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then clean text data in column 'date' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'date' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)\nprint(df.describe())\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then create a new feature 'Close_ratio' as the ratio of 'Close' to 'Close', then train a Linear Regression model to predict 'Close', then one-hot encode the categorical column 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Close_ratio'] = df['Close'] / df['Close']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['Close'], prefix=['Close'])"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then handle missing values in 'review' by imputing with median, then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['review'].fillna(df['review'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then normalize the 'open' column using min-max scaling, then perform K-Means clustering with k=3 on numeric features, then compute TF-IDF features for column 'sales' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['open_scaled'] = (df['open'] - df['open'].min()) / (df['open'].max() - df['open'].min())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sales'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'OverallQual' and display top 10 words, then calculate the correlation matrix for numeric features, then detect outliers in 'Neighborhood' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['OverallQual'])\nprint(vect.get_feature_names_out())\ncorr = df.corr()\nprint(corr)\nQ1 = df['Neighborhood'].quantile(0.25)\nQ3 = df['Neighborhood'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Neighborhood'] < (Q1 - 1.5*IQR)) | (df['Neighborhood'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then detect outliers in 'Age' using the IQR method, then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nQ1 = df['Age'].quantile(0.25)\nQ3 = df['Age'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Age'] < (Q1 - 1.5*IQR)) | (df['Age'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'text', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['text'])\ny = df['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Volume', then normalize the 'Date' column using min-max scaling, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf = pd.get_dummies(df, columns=['Volume'], prefix=['Volume'])\ndf['Date_scaled'] = (df['Date'] - df['Date'].min()) / (df['Date'].max() - df['Date'].min())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'total_amount', then perform time-series forecasting using ARIMA to predict the next 12 periods, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['total_amount'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'marital' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['y'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nprint(df.describe())\nQ1 = df['marital'].quantile(0.25)\nQ3 = df['marital'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['marital'] < (Q1 - 1.5*IQR)) | (df['marital'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Survived', then train a Random Forest Classifier to predict 'Survived', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'precipitation' and display top 10 words, then detect outliers in 'date' using the IQR method, then train a Linear Regression model to predict 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['precipitation'])\nprint(vect.get_feature_names_out())\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'confirmed', then compute TF-IDF features for column 'deaths' and display top 10 words, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['deaths'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'Fare', then create a new feature 'Fare_ratio' as the ratio of 'Fare' to 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['Fare'], prefix=['Fare'])\ndf['Fare_ratio'] = df['Fare'] / df['Survived']"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'oldbalanceOrg'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['oldbalanceOrg'], prefix=['oldbalanceOrg'])"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['education'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nprint(df.describe())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then normalize the 'species' column using min-max scaling, then train a Linear Regression model to predict 'species', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['species_scaled'] = (df['species'] - df['species'].min()) / (df['species'].max() - df['species'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nprint(df.describe())\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then clean text data in column 'date_time' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'rain_1h' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_time_clean'] = df['date_time'].apply(clean)\nprint(df.describe())\nQ1 = df['rain_1h'].quantile(0.25)\nQ3 = df['rain_1h'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['rain_1h'] < (Q1 - 1.5*IQR)) | (df['rain_1h'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then normalize the 'heart_rate' column using min-max scaling, then plot a histogram of 'time', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['heart_rate_scaled'] = (df['heart_rate'] - df['heart_rate'].min()) / (df['heart_rate'].max() - df['heart_rate'].min())\ndf['time'].hist()\nplt.xlabel('time')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['ecg_reading'])\ny = df['ecg_reading']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then detect outliers in 'quantity' using the IQR method, then create a new feature 'quantity_ratio' as the ratio of 'quantity' to 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['quantity'].quantile(0.25)\nQ3 = df['quantity'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['quantity'] < (Q1 - 1.5*IQR)) | (df['quantity'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['quantity_ratio'] = df['quantity'] / df['total_amount']"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then calculate the correlation matrix for numeric features, then train a Random Forest Classifier to predict 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['transaction_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ncorr = df.corr()\nprint(corr)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'tenure' and display top 10 words, then plot a histogram of 'ContractType', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['tenure'])\nprint(vect.get_feature_names_out())\ndf['ContractType'].hist()\nplt.xlabel('ContractType')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['tenure'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then plot a histogram of 'Neighborhood', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['Neighborhood'].hist()\nplt.xlabel('Neighborhood')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then normalize the 'store' column using min-max scaling, then plot a histogram of 'open', then compute TF-IDF features for column 'sales' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['store_scaled'] = (df['store'] - df['store'].min()) / (df['store'].max() - df['store'].min())\ndf['open'].hist()\nplt.xlabel('open')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sales'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'temperature' by removing punctuation and stopwords, then train a Random Forest Classifier to predict 'temperature', then compute TF-IDF features for column 'humidity' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['temperature_clean'] = df['temperature'].apply(clean)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['humidity'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then one-hot encode the categorical column 'sentiment', then create a new feature 'length_ratio' as the ratio of 'length' to 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf = pd.get_dummies(df, columns=['sentiment'], prefix=['sentiment'])\ndf['length_ratio'] = df['length'] / df['sentiment']"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then handle missing values in 'oldbalanceOrg' by imputing with median, then normalize the 'oldbalanceOrg' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['oldbalanceOrg'].fillna(df['oldbalanceOrg'].median(), inplace=True)\ndf['oldbalanceOrg_scaled'] = (df['oldbalanceOrg'] - df['oldbalanceOrg'].min()) / (df['oldbalanceOrg'].max() - df['oldbalanceOrg'].min())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then clean text data in column 'Low' by removing punctuation and stopwords, then compute TF-IDF features for column 'Low' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Close'])\ny = df['Close']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Low_clean'] = df['Low'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Low'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then handle missing values in 'time' by imputing with median, then perform K-Means clustering with k=3 on numeric features, then compute TF-IDF features for column 'time' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['time'].fillna(df['time'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['time'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then plot a histogram of 'product_id', then display feature importances from the Random Forest model, then train a Linear Regression model to predict 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['product_id'].hist()\nplt.xlabel('product_id')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then plot a histogram of 'temperature', then detect outliers in 'consumption' using the IQR method, then handle missing values in 'date' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['temperature'].hist()\nplt.xlabel('temperature')\nplt.ylabel('Frequency')\nplt.show()\nQ1 = df['consumption'].quantile(0.25)\nQ3 = df['consumption'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['consumption'] < (Q1 - 1.5*IQR)) | (df['consumption'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['date'].fillna(df['date'].median(), inplace=True)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'pm2_5', then detect outliers in 'pm2_5' using the IQR method, then normalize the 'pm10' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nQ1 = df['pm2_5'].quantile(0.25)\nQ3 = df['pm2_5'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['pm2_5'] < (Q1 - 1.5*IQR)) | (df['pm2_5'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['pm10_scaled'] = (df['pm10'] - df['pm10'].min()) / (df['pm10'].max() - df['pm10'].min())"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sensor_value', then one-hot encode the categorical column 'timestamp', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['timestamp'], prefix=['timestamp'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sensor_value'])\ny = df['sensor_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split, then normalize the 'review' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['review_scaled'] = (df['review'] - df['review'].min()) / (df['review'].max() - df['review'].min())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Close', then display summary statistics of all numeric columns using df.describe(), then clean text data in column 'High' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['High_clean'] = df['High'].apply(clean)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'patient_id' and display top 10 words, then one-hot encode the categorical column 'patient_id', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['patient_id'])\nprint(vect.get_feature_names_out())\ndf = pd.get_dummies(df, columns=['patient_id'], prefix=['patient_id'])\nprint(df.describe())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then handle missing values in 'arrival_delay' by imputing with median, then one-hot encode the categorical column 'arrival_delay', then plot a histogram of 'carrier'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['arrival_delay'].fillna(df['arrival_delay'].median(), inplace=True)\ndf = pd.get_dummies(df, columns=['arrival_delay'], prefix=['arrival_delay'])\ndf['carrier'].hist()\nplt.xlabel('carrier')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then normalize the 'education' column using min-max scaling, then calculate the correlation matrix for numeric features, then one-hot encode the categorical column 'education'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['education_scaled'] = (df['education'] - df['education'].min()) / (df['education'].max() - df['education'].min())\ncorr = df.corr()\nprint(corr)\ndf = pd.get_dummies(df, columns=['education'], prefix=['education'])"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then plot a histogram of 'departure_delay', then train a Linear Regression model to predict 'arrival_delay', then normalize the 'carrier' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['departure_delay'].hist()\nplt.xlabel('departure_delay')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['carrier_scaled'] = (df['carrier'] - df['carrier'].min()) / (df['carrier'].max() - df['carrier'].min())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then detect outliers in 'temperature' using the IQR method, then clean text data in column 'temperature' by removing punctuation and stopwords, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nQ1 = df['temperature'].quantile(0.25)\nQ3 = df['temperature'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['temperature'] < (Q1 - 1.5*IQR)) | (df['temperature'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['temperature_clean'] = df['temperature'].apply(clean)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features, then handle missing values in 'o3' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)\ndf['o3'].fillna(df['o3'].median(), inplace=True)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Random Forest Classifier to predict 'confirmed', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nprint(df.describe())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'product_id' and display top 10 words, then split the data into training and testing sets with an 80-20 split, then create a new feature 'product_id_ratio' as the ratio of 'product_id' to 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['product_id'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['product_id_ratio'] = df['product_id'] / df['total_amount']"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'flight', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['flight'], prefix=['flight'])\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'text', then display summary statistics of all numeric columns using df.describe(), then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['post_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then clean text data in column 'Pclass' by removing punctuation and stopwords, then evaluate the model performance using RMSE and R\u00b2 score, then handle missing values in 'Pclass' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Pclass_clean'] = df['Pclass'].apply(clean)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['Pclass'].fillna(df['Pclass'].median(), inplace=True)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'Product' column using min-max scaling, then one-hot encode the categorical column 'Region'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Product'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['Product_scaled'] = (df['Product'] - df['Product'].min()) / (df['Product'].max() - df['Product'].min())\ndf = pd.get_dummies(df, columns=['Region'], prefix=['Region'])"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'species', then calculate the correlation matrix for numeric features, then handle missing values in 'species' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf = pd.get_dummies(df, columns=['species'], prefix=['species'])\ncorr = df.corr()\nprint(corr)\ndf['species'].fillna(df['species'].median(), inplace=True)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then plot a histogram of 'flight', then perform K-Means clustering with k=3 on numeric features, then create a new feature 'distance_ratio' as the ratio of 'distance' to 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['flight'].hist()\nplt.xlabel('flight')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['distance_ratio'] = df['distance'] / df['arrival_delay']"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'temp' using the IQR method, then handle missing values in 'date_time' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['temp'].quantile(0.25)\nQ3 = df['temp'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['temp'] < (Q1 - 1.5*IQR)) | (df['temp'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['date_time'].fillna(df['date_time'].median(), inplace=True)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then detect outliers in 'sentiment' using the IQR method, then normalize the 'sentiment' column using min-max scaling, then compute TF-IDF features for column 'rating' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nQ1 = df['sentiment'].quantile(0.25)\nQ3 = df['sentiment'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sentiment'] < (Q1 - 1.5*IQR)) | (df['sentiment'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['sentiment_scaled'] = (df['sentiment'] - df['sentiment'].min()) / (df['sentiment'].max() - df['sentiment'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['rating'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'text', then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then plot a histogram of 'date', then train a Random Forest Classifier to predict 'pm2_5', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['date'].hist()\nplt.xlabel('date')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then clean text data in column 'day_of_week' by removing punctuation and stopwords, then calculate the correlation matrix for numeric features, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['day_of_week_clean'] = df['day_of_week'].apply(clean)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then detect outliers in 'amount' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods, then one-hot encode the categorical column 'amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nQ1 = df['amount'].quantile(0.25)\nQ3 = df['amount'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['amount'] < (Q1 - 1.5*IQR)) | (df['amount'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['newbalanceOrig'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf = pd.get_dummies(df, columns=['amount'], prefix=['amount'])"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'arrival_delay', then normalize the 'carrier' column using min-max scaling, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['carrier_scaled'] = (df['carrier'] - df['carrier'].min()) / (df['carrier'].max() - df['carrier'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then detect outliers in 'Neighborhood' using the IQR method, then compute TF-IDF features for column 'YearBuilt' and display top 10 words, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nQ1 = df['Neighborhood'].quantile(0.25)\nQ3 = df['Neighborhood'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Neighborhood'] < (Q1 - 1.5*IQR)) | (df['Neighborhood'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['YearBuilt'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then normalize the 'country' column using min-max scaling, then split the data into training and testing sets with an 80-20 split, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['country_scaled'] = (df['country'] - df['country'].min()) / (df['country'].max() - df['country'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['deaths'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'likes' column using min-max scaling, then handle missing values in 'post_id' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['user_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['likes_scaled'] = (df['likes'] - df['likes'].min()) / (df['likes'].max() - df['likes'].min())\ndf['post_id'].fillna(df['post_id'].median(), inplace=True)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then create a new feature 'text_ratio' as the ratio of 'text' to 'text', then train a Random Forest Classifier to predict 'text', then train a Linear Regression model to predict 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['text_ratio'] = df['text'] / df['text']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'device_id', then normalize the 'timestamp' column using min-max scaling, then train a Linear Regression model to predict 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf = pd.get_dummies(df, columns=['device_id'], prefix=['device_id'])\ndf['timestamp_scaled'] = (df['timestamp'] - df['timestamp'].min()) / (df['timestamp'].max() - df['timestamp'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'TotalCharges', then train a Random Forest Classifier to predict 'Churn', then normalize the 'ContractType' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf = pd.get_dummies(df, columns=['TotalCharges'], prefix=['TotalCharges'])\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['ContractType_scaled'] = (df['ContractType'] - df['ContractType'].min()) / (df['ContractType'].max() - df['ContractType'].min())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then plot a histogram of 'date', then evaluate the model performance using RMSE and R\u00b2 score, then detect outliers in 'pm10' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['date'].hist()\nplt.xlabel('date')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nQ1 = df['pm10'].quantile(0.25)\nQ3 = df['pm10'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['pm10'] < (Q1 - 1.5*IQR)) | (df['pm10'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then clean text data in column 'product_id' by removing punctuation and stopwords, then handle missing values in 'total_amount' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['product_id_clean'] = df['product_id'].apply(clean)\ndf['total_amount'].fillna(df['total_amount'].median(), inplace=True)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'total_amount', then detect outliers in 'customer_id' using the IQR method, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nQ1 = df['customer_id'].quantile(0.25)\nQ3 = df['customer_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['customer_id'] < (Q1 - 1.5*IQR)) | (df['customer_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then normalize the 'sales' column using min-max scaling, then one-hot encode the categorical column 'customers'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ncorr = df.corr()\nprint(corr)\ndf['sales_scaled'] = (df['sales'] - df['sales'].min()) / (df['sales'].max() - df['sales'].min())\ndf = pd.get_dummies(df, columns=['customers'], prefix=['customers'])"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'YearBuilt' column using min-max scaling, then handle missing values in 'OverallQual' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Neighborhood'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['YearBuilt_scaled'] = (df['YearBuilt'] - df['YearBuilt'].min()) / (df['YearBuilt'].max() - df['YearBuilt'].min())\ndf['OverallQual'].fillna(df['OverallQual'].median(), inplace=True)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'date', then compute TF-IDF features for column 'temperature' and display top 10 words, then create a new feature 'date_ratio' as the ratio of 'date' to 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['date'].hist()\nplt.xlabel('date')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['temperature'])\nprint(vect.get_feature_names_out())\ndf['date_ratio'] = df['date'] / df['temperature']"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then detect outliers in 'tenure' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['tenure'].quantile(0.25)\nQ3 = df['tenure'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['tenure'] < (Q1 - 1.5*IQR)) | (df['tenure'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'Product', then train a Linear Regression model to predict 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['Product'], prefix=['Product'])\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then plot a histogram of 'text', then train a Linear Regression model to predict 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ncorr = df.corr()\nprint(corr)\ndf['text'].hist()\nplt.xlabel('text')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then create a new feature 'wind_speed_ratio' as the ratio of 'wind_speed' to 'temperature', then train a Random Forest Classifier to predict 'temperature', then detect outliers in 'wind_speed' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['wind_speed_ratio'] = df['wind_speed'] / df['temperature']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['wind_speed'].quantile(0.25)\nQ3 = df['wind_speed'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['wind_speed'] < (Q1 - 1.5*IQR)) | (df['wind_speed'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then clean text data in column 'rain_1h' by removing punctuation and stopwords, then calculate the correlation matrix for numeric features, then normalize the 'snow_1h' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['rain_1h_clean'] = df['rain_1h'].apply(clean)\ncorr = df.corr()\nprint(corr)\ndf['snow_1h_scaled'] = (df['snow_1h'] - df['snow_1h'].min()) / (df['snow_1h'].max() - df['snow_1h'].min())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then display feature importances from the Random Forest model, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'text', then split the data into training and testing sets with an 80-20 split, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['text'])\ny = df['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then detect outliers in 'product_id' using the IQR method, then display feature importances from the Random Forest model, then clean text data in column 'total_amount' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nQ1 = df['product_id'].quantile(0.25)\nQ3 = df['product_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['product_id'] < (Q1 - 1.5*IQR)) | (df['product_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['total_amount_clean'] = df['total_amount'].apply(clean)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sentiment', then one-hot encode the categorical column 'review', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['review'], prefix=['review'])\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'Revenue', then plot a histogram of 'Product'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['Product'].hist()\nplt.xlabel('Product')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'total_amount', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sales', then clean text data in column 'sales' by removing punctuation and stopwords, then handle missing values in 'day_of_week' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sales_clean'] = df['sales'].apply(clean)\ndf['day_of_week'].fillna(df['day_of_week'].median(), inplace=True)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then normalize the 'sensor_value' column using min-max scaling, then calculate the correlation matrix for numeric features, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['sensor_value_scaled'] = (df['sensor_value'] - df['sensor_value'].min()) / (df['sensor_value'].max() - df['sensor_value'].min())\ncorr = df.corr()\nprint(corr)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then normalize the 'quality' column using min-max scaling, then display feature importances from the Random Forest model, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['quality_scaled'] = (df['quality'] - df['quality'].min()) / (df['quality'].max() - df['quality'].min())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['ecg_reading'])\ny = df['ecg_reading']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'y', then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'education' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['education'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then handle missing values in 'Product' by imputing with median, then compute TF-IDF features for column 'UnitsSold' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ncorr = df.corr()\nprint(corr)\ndf['Product'].fillna(df['Product'].median(), inplace=True)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['UnitsSold'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then create a new feature 'Open_ratio' as the ratio of 'Open' to 'Close', then perform time-series forecasting using ARIMA to predict the next 12 periods, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Open_ratio'] = df['Open'] / df['Close']\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Low'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then evaluate the model performance using RMSE and R\u00b2 score, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['age'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then create a new feature 'store_ratio' as the ratio of 'store' to 'sales', then train a Linear Regression model to predict 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ncorr = df.corr()\nprint(corr)\ndf['store_ratio'] = df['store'] / df['sales']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then plot a histogram of 'distance', then normalize the 'distance' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['distance'].hist()\nplt.xlabel('distance')\nplt.ylabel('Frequency')\nplt.show()\ndf['distance_scaled'] = (df['distance'] - df['distance'].min()) / (df['distance'].max() - df['distance'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'traffic_volume', then calculate the correlation matrix for numeric features, then train a Random Forest Classifier to predict 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf = pd.get_dummies(df, columns=['traffic_volume'], prefix=['traffic_volume'])\ncorr = df.corr()\nprint(corr)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then clean text data in column 'petal_width' by removing punctuation and stopwords, then one-hot encode the categorical column 'petal_width', then handle missing values in 'petal_length' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['petal_width_clean'] = df['petal_width'].apply(clean)\ndf = pd.get_dummies(df, columns=['petal_width'], prefix=['petal_width'])\ndf['petal_length'].fillna(df['petal_length'].median(), inplace=True)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then display summary statistics of all numeric columns using df.describe(), then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ncorr = df.corr()\nprint(corr)\nprint(df.describe())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['y'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then plot a histogram of 'transaction_id', then train a Random Forest Classifier to predict 'total_amount', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['transaction_id'].hist()\nplt.xlabel('transaction_id')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['customer_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sensor_value', then detect outliers in 'timestamp' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nQ1 = df['timestamp'].quantile(0.25)\nQ3 = df['timestamp'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['timestamp'] < (Q1 - 1.5*IQR)) | (df['timestamp'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'date_time' and display top 10 words, then perform K-Means clustering with k=3 on numeric features, then normalize the 'traffic_volume' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date_time'])\nprint(vect.get_feature_names_out())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['traffic_volume_scaled'] = (df['traffic_volume'] - df['traffic_volume'].min()) / (df['traffic_volume'].max() - df['traffic_volume'].min())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'no2', then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'date'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf = pd.get_dummies(df, columns=['no2'], prefix=['no2'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['date'].hist()\nplt.xlabel('date')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then detect outliers in 'age' using the IQR method, then calculate the correlation matrix for numeric features, then plot a histogram of 'job'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nQ1 = df['age'].quantile(0.25)\nQ3 = df['age'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['age'] < (Q1 - 1.5*IQR)) | (df['age'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ncorr = df.corr()\nprint(corr)\ndf['job'].hist()\nplt.xlabel('job')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Low', then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'Close_ratio' as the ratio of 'Close' to 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Low'].hist()\nplt.xlabel('Low')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['Close_ratio'] = df['Close'] / df['Close']"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'amount' using the IQR method, then train a Random Forest Classifier to predict 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['amount'].quantile(0.25)\nQ3 = df['amount'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['amount'] < (Q1 - 1.5*IQR)) | (df['amount'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then compute TF-IDF features for column 'tenure' and display top 10 words, then plot a histogram of 'tenure'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nprint(df.describe())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['tenure'])\nprint(vect.get_feature_names_out())\ndf['tenure'].hist()\nplt.xlabel('tenure')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'TotalCharges_ratio' as the ratio of 'TotalCharges' to 'Churn', then handle missing values in 'tenure' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['MonthlyCharges'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['TotalCharges_ratio'] = df['TotalCharges'] / df['Churn']\ndf['tenure'].fillna(df['tenure'].median(), inplace=True)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'shares' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'user_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['shares'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['user_id'], prefix=['user_id'])"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then compute TF-IDF features for column 'shares' and display top 10 words, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nprint(df.describe())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['shares'])\nprint(vect.get_feature_names_out())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then train a Random Forest Classifier to predict 'temperature', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['precipitation'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then create a new feature 'OverallQual_ratio' as the ratio of 'OverallQual' to 'SalePrice', then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'SalePrice'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['OverallQual_ratio'] = df['OverallQual'] / df['SalePrice']\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'distance' and display top 10 words, then create a new feature 'departure_delay_ratio' as the ratio of 'departure_delay' to 'arrival_delay', then detect outliers in 'distance' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['distance'])\nprint(vect.get_feature_names_out())\ndf['departure_delay_ratio'] = df['departure_delay'] / df['arrival_delay']\nQ1 = df['distance'].quantile(0.25)\nQ3 = df['distance'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['distance'] < (Q1 - 1.5*IQR)) | (df['distance'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Churn', then split the data into training and testing sets with an 80-20 split, then create a new feature 'Churn_ratio' as the ratio of 'Churn' to 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['Churn_ratio'] = df['Churn'] / df['Churn']"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'wind_speed' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nQ1 = df['wind_speed'].quantile(0.25)\nQ3 = df['wind_speed'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['wind_speed'] < (Q1 - 1.5*IQR)) | (df['wind_speed'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Region', then plot a histogram of 'UnitsSold', then create a new feature 'Product_ratio' as the ratio of 'Product' to 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf = pd.get_dummies(df, columns=['Region'], prefix=['Region'])\ndf['UnitsSold'].hist()\nplt.xlabel('UnitsSold')\nplt.ylabel('Frequency')\nplt.show()\ndf['Product_ratio'] = df['Product'] / df['Revenue']"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then clean text data in column 'date' by removing punctuation and stopwords, then compute TF-IDF features for column 'temperature' and display top 10 words, then create a new feature 'pressure_ratio' as the ratio of 'pressure' to 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['temperature'])\nprint(vect.get_feature_names_out())\ndf['pressure_ratio'] = df['pressure'] / df['consumption']"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then detect outliers in 'YearBuilt' using the IQR method, then handle missing values in 'LotArea' by imputing with median, then plot a histogram of 'SalePrice'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nQ1 = df['YearBuilt'].quantile(0.25)\nQ3 = df['YearBuilt'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['YearBuilt'] < (Q1 - 1.5*IQR)) | (df['YearBuilt'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['LotArea'].fillna(df['LotArea'].median(), inplace=True)\ndf['SalePrice'].hist()\nplt.xlabel('SalePrice')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then handle missing values in 'departure_delay' by imputing with median, then evaluate the model performance using RMSE and R\u00b2 score, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['departure_delay'].fillna(df['departure_delay'].median(), inplace=True)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then handle missing values in 'sepal_length' by imputing with median, then evaluate the model performance using RMSE and R\u00b2 score, then create a new feature 'sepal_length_ratio' as the ratio of 'sepal_length' to 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['sepal_length'].fillna(df['sepal_length'].median(), inplace=True)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['sepal_length_ratio'] = df['sepal_length'] / df['species']"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Random Forest Classifier to predict 'isFraud', then normalize the 'oldbalanceOrg' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nprint(df.describe())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['oldbalanceOrg_scaled'] = (df['oldbalanceOrg'] - df['oldbalanceOrg'].min()) / (df['oldbalanceOrg'].max() - df['oldbalanceOrg'].min())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then detect outliers in 'isFraud' using the IQR method, then split the data into training and testing sets with an 80-20 split, then create a new feature 'newbalanceOrig_ratio' as the ratio of 'newbalanceOrig' to 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nQ1 = df['isFraud'].quantile(0.25)\nQ3 = df['isFraud'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['isFraud'] < (Q1 - 1.5*IQR)) | (df['isFraud'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['newbalanceOrig_ratio'] = df['newbalanceOrig'] / df['isFraud']"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then create a new feature 'humidity_ratio' as the ratio of 'humidity' to 'temperature', then one-hot encode the categorical column 'precipitation'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nprint(df.describe())\ndf['humidity_ratio'] = df['humidity'] / df['temperature']\ndf = pd.get_dummies(df, columns=['precipitation'], prefix=['precipitation'])"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display summary statistics of all numeric columns using df.describe(), then compute TF-IDF features for column 'recovered' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['country'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nprint(df.describe())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['recovered'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Pclass', then compute TF-IDF features for column 'Fare' and display top 10 words, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf = pd.get_dummies(df, columns=['Pclass'], prefix=['Pclass'])\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Fare'])\nprint(vect.get_feature_names_out())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'wind_speed', then handle missing values in 'wind_speed' by imputing with median, then train a Random Forest Classifier to predict 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf = pd.get_dummies(df, columns=['wind_speed'], prefix=['wind_speed'])\ndf['wind_speed'].fillna(df['wind_speed'].median(), inplace=True)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Churn', then clean text data in column 'TotalCharges' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['TotalCharges_clean'] = df['TotalCharges'].apply(clean)\nprint(df.describe())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then plot a histogram of 'country', then train a Linear Regression model to predict 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['country'].hist()\nplt.xlabel('country')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sensor_value', then detect outliers in 'location' using the IQR method, then compute TF-IDF features for column 'timestamp' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nQ1 = df['location'].quantile(0.25)\nQ3 = df['location'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['location'] < (Q1 - 1.5*IQR)) | (df['location'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['timestamp'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then normalize the 'recovered' column using min-max scaling, then compute TF-IDF features for column 'recovered' and display top 10 words, then clean text data in column 'date' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['recovered_scaled'] = (df['recovered'] - df['recovered'].min()) / (df['recovered'].max() - df['recovered'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['recovered'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then train a Linear Regression model to predict 'sentiment', then one-hot encode the categorical column 'length'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['length'], prefix=['length'])"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'UnitsSold' and display top 10 words, then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['UnitsSold'])\nprint(vect.get_feature_names_out())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Region'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then handle missing values in 'LotArea' by imputing with median, then perform time-series forecasting using ARIMA to predict the next 12 periods, then clean text data in column 'YearBuilt' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['LotArea'].fillna(df['LotArea'].median(), inplace=True)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['OverallQual'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['YearBuilt_clean'] = df['YearBuilt'].apply(clean)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'consumption', then detect outliers in 'humidity' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['humidity'].quantile(0.25)\nQ3 = df['humidity'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['humidity'] < (Q1 - 1.5*IQR)) | (df['humidity'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then normalize the 'flight' column using min-max scaling, then create a new feature 'flight_ratio' as the ratio of 'flight' to 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['flight_scaled'] = (df['flight'] - df['flight'].min()) / (df['flight'].max() - df['flight'].min())\ndf['flight_ratio'] = df['flight'] / df['arrival_delay']"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then clean text data in column 'o3' by removing punctuation and stopwords, then train a Linear Regression model to predict 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nprint(df.describe())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['o3_clean'] = df['o3'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then create a new feature 'departure_delay_ratio' as the ratio of 'departure_delay' to 'arrival_delay', then detect outliers in 'arrival_delay' using the IQR method, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['departure_delay_ratio'] = df['departure_delay'] / df['arrival_delay']\nQ1 = df['arrival_delay'].quantile(0.25)\nQ3 = df['arrival_delay'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['arrival_delay'] < (Q1 - 1.5*IQR)) | (df['arrival_delay'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then plot a histogram of 'High', then one-hot encode the categorical column 'High', then create a new feature 'Volume_ratio' as the ratio of 'Volume' to 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['High'].hist()\nplt.xlabel('High')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['High'], prefix=['High'])\ndf['Volume_ratio'] = df['Volume'] / df['Close']"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Random Forest Classifier to predict 'species', then train a Linear Regression model to predict 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then compute TF-IDF features for column 'Date' and display top 10 words, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nprint(df.describe())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Date'])\nprint(vect.get_feature_names_out())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then train a Linear Regression model to predict 'total_amount', then one-hot encode the categorical column 'transaction_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['transaction_id'], prefix=['transaction_id'])"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'ecg_reading' column using min-max scaling, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['ecg_reading_scaled'] = (df['ecg_reading'] - df['ecg_reading'].min()) / (df['ecg_reading'].max() - df['ecg_reading'].min())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'education' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['education'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Survived', then split the data into training and testing sets with an 80-20 split, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then normalize the 'carrier' column using min-max scaling, then detect outliers in 'distance' using the IQR method, then create a new feature 'carrier_ratio' as the ratio of 'carrier' to 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['carrier_scaled'] = (df['carrier'] - df['carrier'].min()) / (df['carrier'].max() - df['carrier'].min())\nQ1 = df['distance'].quantile(0.25)\nQ3 = df['distance'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['distance'] < (Q1 - 1.5*IQR)) | (df['distance'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['carrier_ratio'] = df['carrier'] / df['arrival_delay']"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'OverallQual', then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'SalePrice' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf = pd.get_dummies(df, columns=['OverallQual'], prefix=['OverallQual'])\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Neighborhood'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['SalePrice_scaled'] = (df['SalePrice'] - df['SalePrice'].min()) / (df['SalePrice'].max() - df['SalePrice'].min())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then perform K-Means clustering with k=3 on numeric features, then compute TF-IDF features for column 'date' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'education' using the IQR method, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nprint(df.describe())\nQ1 = df['education'].quantile(0.25)\nQ3 = df['education'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['education'] < (Q1 - 1.5*IQR)) | (df['education'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then normalize the 'recovered' column using min-max scaling, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['recovered_scaled'] = (df['recovered'] - df['recovered'].min()) / (df['recovered'].max() - df['recovered'].min())\nprint(df.describe())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'sales', then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf = pd.get_dummies(df, columns=['sales'], prefix=['sales'])\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['customers'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'patient_id', then compute TF-IDF features for column 'patient_id' and display top 10 words, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['patient_id'].hist()\nplt.xlabel('patient_id')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['patient_id'])\nprint(vect.get_feature_names_out())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then compute TF-IDF features for column 'Survived' and display top 10 words, then create a new feature 'Fare_ratio' as the ratio of 'Fare' to 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Sex'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Survived'])\nprint(vect.get_feature_names_out())\ndf['Fare_ratio'] = df['Fare'] / df['Survived']"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'pm2_5', then compute TF-IDF features for column 'date' and display top 10 words, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'user_id', then detect outliers in 'user_id' using the IQR method, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf = pd.get_dummies(df, columns=['user_id'], prefix=['user_id'])\nQ1 = df['user_id'].quantile(0.25)\nQ3 = df['user_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['user_id'] < (Q1 - 1.5*IQR)) | (df['user_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nprint(df.describe())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then create a new feature 'Churn_ratio' as the ratio of 'Churn' to 'Churn', then one-hot encode the categorical column 'TotalCharges', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['Churn_ratio'] = df['Churn'] / df['Churn']\ndf = pd.get_dummies(df, columns=['TotalCharges'], prefix=['TotalCharges'])\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['TotalCharges'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sensor_value'])\ny = df['sensor_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['user_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then handle missing values in 'education' by imputing with median, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['education'].fillna(df['education'].median(), inplace=True)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Open'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Close', then one-hot encode the categorical column 'Volume', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['Volume'], prefix=['Volume'])\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'o3', then split the data into training and testing sets with an 80-20 split, then compute TF-IDF features for column 'so2' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf = pd.get_dummies(df, columns=['o3'], prefix=['o3'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['so2'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then plot a histogram of 'sepal_width', then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['sepal_width'].hist()\nplt.xlabel('sepal_width')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then create a new feature 'Survived_ratio' as the ratio of 'Survived' to 'Survived', then train a Linear Regression model to predict 'Survived', then clean text data in column 'Sex' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Survived_ratio'] = df['Survived'] / df['Survived']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Sex_clean'] = df['Sex'].apply(clean)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then create a new feature 'Close_ratio' as the ratio of 'Close' to 'Close', then train a Random Forest Classifier to predict 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Close'])\ny = df['Close']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['Close_ratio'] = df['Close'] / df['Close']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'UnitsSold', then display feature importances from the Random Forest model, then train a Random Forest Classifier to predict 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf = pd.get_dummies(df, columns=['UnitsSold'], prefix=['UnitsSold'])\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then detect outliers in 'sepal_length' using the IQR method, then display feature importances from the Random Forest model, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nQ1 = df['sepal_length'].quantile(0.25)\nQ3 = df['sepal_length'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sepal_length'] < (Q1 - 1.5*IQR)) | (df['sepal_length'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['time'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then clean text data in column 'marital' by removing punctuation and stopwords, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['marital_clean'] = df['marital'].apply(clean)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then clean text data in column 'Fare' by removing punctuation and stopwords, then create a new feature 'Fare_ratio' as the ratio of 'Fare' to 'Survived', then train a Random Forest Classifier to predict 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Fare_clean'] = df['Fare'].apply(clean)\ndf['Fare_ratio'] = df['Fare'] / df['Survived']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'precipitation', then create a new feature 'temperature_ratio' as the ratio of 'temperature' to 'temperature', then train a Random Forest Classifier to predict 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf = pd.get_dummies(df, columns=['precipitation'], prefix=['precipitation'])\ndf['temperature_ratio'] = df['temperature'] / df['temperature']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then create a new feature 'temp_ratio' as the ratio of 'temp' to 'traffic_volume', then detect outliers in 'traffic_volume' using the IQR method, then plot a histogram of 'date_time'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['temp_ratio'] = df['temp'] / df['traffic_volume']\nQ1 = df['traffic_volume'].quantile(0.25)\nQ3 = df['traffic_volume'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['traffic_volume'] < (Q1 - 1.5*IQR)) | (df['traffic_volume'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['date_time'].hist()\nplt.xlabel('date_time')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'time' using the IQR method, then clean text data in column 'newbalanceOrig' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['isFraud'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['time'].quantile(0.25)\nQ3 = df['time'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['time'] < (Q1 - 1.5*IQR)) | (df['time'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['newbalanceOrig_clean'] = df['newbalanceOrig'].apply(clean)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'transaction_id' and display top 10 words, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['transaction_id'])\nprint(vect.get_feature_names_out())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['total_amount'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features, then clean text data in column 'departure_delay' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['departure_delay_clean'] = df['departure_delay'].apply(clean)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'Pclass' and display top 10 words, then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Pclass'])\nprint(vect.get_feature_names_out())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'status' by removing punctuation and stopwords, then evaluate the model performance using RMSE and R\u00b2 score, then handle missing values in 'status' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['status_clean'] = df['status'].apply(clean)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['status'].fillna(df['status'].median(), inplace=True)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'customers' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nprint(df.describe())\ndf['customers'].fillna(df['customers'].median(), inplace=True)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then create a new feature 'recovered_ratio' as the ratio of 'recovered' to 'confirmed', then plot a histogram of 'country'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['recovered_ratio'] = df['recovered'] / df['confirmed']\ndf['country'].hist()\nplt.xlabel('country')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'MonthlyCharges', then calculate the correlation matrix for numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf = pd.get_dummies(df, columns=['MonthlyCharges'], prefix=['MonthlyCharges'])\ncorr = df.corr()\nprint(corr)\nprint(df.describe())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then clean text data in column 'traffic_volume' by removing punctuation and stopwords, then train a Linear Regression model to predict 'traffic_volume', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['traffic_volume_clean'] = df['traffic_volume'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then handle missing values in 'date' by imputing with median, then plot a histogram of 'pressure', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['date'].fillna(df['date'].median(), inplace=True)\ndf['pressure'].hist()\nplt.xlabel('pressure')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then create a new feature 'pm10_ratio' as the ratio of 'pm10' to 'pm2_5', then one-hot encode the categorical column 'no2', then detect outliers in 'o3' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['pm10_ratio'] = df['pm10'] / df['pm2_5']\ndf = pd.get_dummies(df, columns=['no2'], prefix=['no2'])\nQ1 = df['o3'].quantile(0.25)\nQ3 = df['o3'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['o3'] < (Q1 - 1.5*IQR)) | (df['o3'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then detect outliers in 'rain_1h' using the IQR method, then train a Random Forest Classifier to predict 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['rain_1h'].quantile(0.25)\nQ3 = df['rain_1h'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['rain_1h'] < (Q1 - 1.5*IQR)) | (df['rain_1h'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then create a new feature 'customers_ratio' as the ratio of 'customers' to 'sales', then compute TF-IDF features for column 'sales' and display top 10 words, then plot a histogram of 'store'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['customers_ratio'] = df['customers'] / df['sales']\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sales'])\nprint(vect.get_feature_names_out())\ndf['store'].hist()\nplt.xlabel('store')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'patient_id' and display top 10 words, then split the data into training and testing sets with an 80-20 split, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['patient_id'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['ecg_reading'])\ny = df['ecg_reading']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then clean text data in column 'Volume' by removing punctuation and stopwords, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Close'])\ny = df['Close']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Volume_clean'] = df['Volume'].apply(clean)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'likes' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['likes'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['likes_scaled'] = (df['likes'] - df['likes'].min()) / (df['likes'].max() - df['likes'].min())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'Product' using the IQR method, then split the data into training and testing sets with an 80-20 split, then compute TF-IDF features for column 'Date' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nQ1 = df['Product'].quantile(0.25)\nQ3 = df['Product'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Product'] < (Q1 - 1.5*IQR)) | (df['Product'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Revenue'])\ny = df['Revenue']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Date'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'traffic_volume', then create a new feature 'traffic_volume_ratio' as the ratio of 'traffic_volume' to 'traffic_volume', then train a Linear Regression model to predict 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['traffic_volume_ratio'] = df['traffic_volume'] / df['traffic_volume']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'location', then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'device_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['location'].hist()\nplt.xlabel('location')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['device_id'], prefix=['device_id'])"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'ecg_reading', then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then plot a histogram of 'newbalanceOrig', then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['newbalanceOrig'].hist()\nplt.xlabel('newbalanceOrig')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then evaluate the model performance using RMSE and R\u00b2 score, then create a new feature 'customer_id_ratio' as the ratio of 'customer_id' to 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['quantity'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['customer_id_ratio'] = df['customer_id'] / df['total_amount']"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'isFraud', then clean text data in column 'isFraud' by removing punctuation and stopwords, then one-hot encode the categorical column 'time'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['isFraud_clean'] = df['isFraud'].apply(clean)\ndf = pd.get_dummies(df, columns=['time'], prefix=['time'])"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then normalize the 'Product' column using min-max scaling, then detect outliers in 'Region' using the IQR method, then plot a histogram of 'Product'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Product_scaled'] = (df['Product'] - df['Product'].min()) / (df['Product'].max() - df['Product'].min())\nQ1 = df['Region'].quantile(0.25)\nQ3 = df['Region'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Region'] < (Q1 - 1.5*IQR)) | (df['Region'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['Product'].hist()\nplt.xlabel('Product')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then calculate the correlation matrix for numeric features, then train a Random Forest Classifier to predict 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['distance'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ncorr = df.corr()\nprint(corr)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then create a new feature 'snow_1h_ratio' as the ratio of 'snow_1h' to 'traffic_volume', then handle missing values in 'rain_1h' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['snow_1h_ratio'] = df['snow_1h'] / df['traffic_volume']\ndf['rain_1h'].fillna(df['rain_1h'].median(), inplace=True)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'species', then normalize the 'petal_width' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['petal_width_scaled'] = (df['petal_width'] - df['petal_width'].min()) / (df['petal_width'].max() - df['petal_width'].min())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'traffic_volume' by imputing with median, then clean text data in column 'temp' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nprint(df.describe())\ndf['traffic_volume'].fillna(df['traffic_volume'].median(), inplace=True)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['temp_clean'] = df['temp'].apply(clean)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model, then train a Linear Regression model to predict 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display feature importances from the Random Forest model, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nprint(df.describe())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then clean text data in column 'humidity' by removing punctuation and stopwords, then plot a histogram of 'humidity'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['pressure'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['humidity_clean'] = df['humidity'].apply(clean)\ndf['humidity'].hist()\nplt.xlabel('humidity')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score, then compute TF-IDF features for column 'flight' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['flight'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'sepal_length' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['species'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sepal_length'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then normalize the 'date' column using min-max scaling, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['date_scaled'] = (df['date'] - df['date'].min()) / (df['date'].max() - df['date'].min())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'Revenue' and display top 10 words, then detect outliers in 'UnitsSold' using the IQR method, then normalize the 'UnitsSold' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Revenue'])\nprint(vect.get_feature_names_out())\nQ1 = df['UnitsSold'].quantile(0.25)\nQ3 = df['UnitsSold'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['UnitsSold'] < (Q1 - 1.5*IQR)) | (df['UnitsSold'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['UnitsSold_scaled'] = (df['UnitsSold'] - df['UnitsSold'].min()) / (df['UnitsSold'].max() - df['UnitsSold'].min())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform K-Means clustering with k=3 on numeric features, then create a new feature 'departure_delay_ratio' as the ratio of 'departure_delay' to 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nprint(df.describe())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['departure_delay_ratio'] = df['departure_delay'] / df['arrival_delay']"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'device_id', then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['device_id'].hist()\nplt.xlabel('device_id')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then handle missing values in 'rating' by imputing with median, then detect outliers in 'review' using the IQR method, then train a Linear Regression model to predict 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['rating'].fillna(df['rating'].median(), inplace=True)\nQ1 = df['review'].quantile(0.25)\nQ3 = df['review'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['review'] < (Q1 - 1.5*IQR)) | (df['review'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then create a new feature 'device_id_ratio' as the ratio of 'device_id' to 'sensor_value', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sensor_value'])\ny = df['sensor_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['device_id_ratio'] = df['device_id'] / df['sensor_value']\nprint(df.describe())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then one-hot encode the categorical column 'TotalCharges', then handle missing values in 'ContractType' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['tenure'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf = pd.get_dummies(df, columns=['TotalCharges'], prefix=['TotalCharges'])\ndf['ContractType'].fillna(df['ContractType'].median(), inplace=True)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then handle missing values in 'YearBuilt' by imputing with median, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['YearBuilt'].fillna(df['YearBuilt'].median(), inplace=True)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then detect outliers in 'rating' using the IQR method, then compute TF-IDF features for column 'genre' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ncorr = df.corr()\nprint(corr)\nQ1 = df['rating'].quantile(0.25)\nQ3 = df['rating'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['rating'] < (Q1 - 1.5*IQR)) | (df['rating'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['genre'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'isFraud', then handle missing values in 'oldbalanceOrg' by imputing with median, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['oldbalanceOrg'].fillna(df['oldbalanceOrg'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'flight' and display top 10 words, then clean text data in column 'arrival_delay' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['flight'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['arrival_delay_clean'] = df['arrival_delay'].apply(clean)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'consumption', then train a Linear Regression model to predict 'consumption', then one-hot encode the categorical column 'date'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['date'], prefix=['date'])"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then plot a histogram of 'traffic_volume', then perform time-series forecasting using ARIMA to predict the next 12 periods, then compute TF-IDF features for column 'rain_1h' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['traffic_volume'].hist()\nplt.xlabel('traffic_volume')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['temp'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['rain_1h'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then display feature importances from the Random Forest model, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'Product', then train a Random Forest Classifier to predict 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['Product'], prefix=['Product'])\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then normalize the 'country' column using min-max scaling, then train a Random Forest Classifier to predict 'confirmed', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['country_scaled'] = (df['country'] - df['country'].min()) / (df['country'].max() - df['country'].min())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['deaths'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then create a new feature 'sales_ratio' as the ratio of 'sales' to 'sales', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['sales_ratio'] = df['sales'] / df['sales']\nprint(df.describe())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'petal_width' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['petal_width'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then clean text data in column 'Survived' by removing punctuation and stopwords, then perform K-Means clustering with k=3 on numeric features, then plot a histogram of 'Fare'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Survived_clean'] = df['Survived'].apply(clean)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['Fare'].hist()\nplt.xlabel('Fare')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then create a new feature 'confirmed_ratio' as the ratio of 'confirmed' to 'confirmed', then one-hot encode the categorical column 'recovered'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nprint(df.describe())\ndf['confirmed_ratio'] = df['confirmed'] / df['confirmed']\ndf = pd.get_dummies(df, columns=['recovered'], prefix=['recovered'])"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then plot a histogram of 'marital', then handle missing values in 'job' by imputing with median, then create a new feature 'education_ratio' as the ratio of 'education' to 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['marital'].hist()\nplt.xlabel('marital')\nplt.ylabel('Frequency')\nplt.show()\ndf['job'].fillna(df['job'].median(), inplace=True)\ndf['education_ratio'] = df['education'] / df['y']"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then normalize the 'TotalCharges' column using min-max scaling, then calculate the correlation matrix for numeric features, then clean text data in column 'tenure' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['TotalCharges_scaled'] = (df['TotalCharges'] - df['TotalCharges'].min()) / (df['TotalCharges'].max() - df['TotalCharges'].min())\ncorr = df.corr()\nprint(corr)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['tenure_clean'] = df['tenure'].apply(clean)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'user_id', then train a Linear Regression model to predict 'text', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf = pd.get_dummies(df, columns=['user_id'], prefix=['user_id'])\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then detect outliers in 'Neighborhood' using the IQR method, then train a Random Forest Classifier to predict 'SalePrice', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nQ1 = df['Neighborhood'].quantile(0.25)\nQ3 = df['Neighborhood'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Neighborhood'] < (Q1 - 1.5*IQR)) | (df['Neighborhood'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then handle missing values in 'review' by imputing with median, then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['review'].fillna(df['review'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['length'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then create a new feature 'post_id_ratio' as the ratio of 'post_id' to 'text', then calculate the correlation matrix for numeric features, then handle missing values in 'post_id' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['post_id_ratio'] = df['post_id'] / df['text']\ncorr = df.corr()\nprint(corr)\ndf['post_id'].fillna(df['post_id'].median(), inplace=True)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then one-hot encode the categorical column 'sales', then clean text data in column 'store' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ncorr = df.corr()\nprint(corr)\ndf = pd.get_dummies(df, columns=['sales'], prefix=['sales'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['store_clean'] = df['store'].apply(clean)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Revenue', then display summary statistics of all numeric columns using df.describe(), then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'pm10' and display top 10 words, then one-hot encode the categorical column 'pm2_5', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['pm10'])\nprint(vect.get_feature_names_out())\ndf = pd.get_dummies(df, columns=['pm2_5'], prefix=['pm2_5'])\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'traffic_volume', then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'snow_1h'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf = pd.get_dummies(df, columns=['traffic_volume'], prefix=['traffic_volume'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['snow_1h'].hist()\nplt.xlabel('snow_1h')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then plot a histogram of 'YearBuilt', then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['YearBuilt'].hist()\nplt.xlabel('YearBuilt')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Close', then display feature importances from the Random Forest model, then train a Linear Regression model to predict 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'Volume', then handle missing values in 'High' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['Volume'], prefix=['Volume'])\ndf['High'].fillna(df['High'].median(), inplace=True)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then normalize the 'product_id' column using min-max scaling, then one-hot encode the categorical column 'product_id', then train a Random Forest Classifier to predict 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['product_id_scaled'] = (df['product_id'] - df['product_id'].min()) / (df['product_id'].max() - df['product_id'].min())\ndf = pd.get_dummies(df, columns=['product_id'], prefix=['product_id'])\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then normalize the 'date_time' column using min-max scaling, then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'temp' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['date_time_scaled'] = (df['date_time'] - df['date_time'].min()) / (df['date_time'].max() - df['date_time'].min())\nprint(df.describe())\nQ1 = df['temp'].quantile(0.25)\nQ3 = df['temp'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['temp'] < (Q1 - 1.5*IQR)) | (df['temp'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then detect outliers in 'carrier' using the IQR method, then normalize the 'flight' column using min-max scaling, then train a Linear Regression model to predict 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nQ1 = df['carrier'].quantile(0.25)\nQ3 = df['carrier'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['carrier'] < (Q1 - 1.5*IQR)) | (df['carrier'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['flight_scaled'] = (df['flight'] - df['flight'].min()) / (df['flight'].max() - df['flight'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features, then clean text data in column 'y' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['education'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['y_clean'] = df['y'].apply(clean)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'total_amount', then clean text data in column 'quantity' by removing punctuation and stopwords, then plot a histogram of 'customer_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['quantity_clean'] = df['quantity'].apply(clean)\ndf['customer_id'].hist()\nplt.xlabel('customer_id')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display feature importances from the Random Forest model, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then plot a histogram of 'petal_length', then compute TF-IDF features for column 'petal_length' and display top 10 words, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['petal_length'].hist()\nplt.xlabel('petal_length')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['petal_length'])\nprint(vect.get_feature_names_out())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sepal_width'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform K-Means clustering with k=3 on numeric features, then plot a histogram of 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nprint(df.describe())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['species'].hist()\nplt.xlabel('species')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then clean text data in column 'customer_id' by removing punctuation and stopwords, then one-hot encode the categorical column 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['customer_id_clean'] = df['customer_id'].apply(clean)\ndf = pd.get_dummies(df, columns=['total_amount'], prefix=['total_amount'])"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then handle missing values in 'sepal_width' by imputing with median, then display feature importances from the Random Forest model, then detect outliers in 'sepal_width' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['sepal_width'].fillna(df['sepal_width'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['sepal_width'].quantile(0.25)\nQ3 = df['sepal_width'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sepal_width'] < (Q1 - 1.5*IQR)) | (df['sepal_width'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then clean text data in column 'store' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'day_of_week'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['store_clean'] = df['store'].apply(clean)\nprint(df.describe())\ndf['day_of_week'].hist()\nplt.xlabel('day_of_week')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'total_amount' and display top 10 words, then display feature importances from the Random Forest model, then detect outliers in 'transaction_id' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['total_amount'])\nprint(vect.get_feature_names_out())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['transaction_id'].quantile(0.25)\nQ3 = df['transaction_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['transaction_id'] < (Q1 - 1.5*IQR)) | (df['transaction_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then detect outliers in 'job' using the IQR method, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nQ1 = df['job'].quantile(0.25)\nQ3 = df['job'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['job'] < (Q1 - 1.5*IQR)) | (df['job'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then clean text data in column 'Open' by removing punctuation and stopwords, then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'Close' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Open_clean'] = df['Open'].apply(clean)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Low'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['Close'].quantile(0.25)\nQ3 = df['Close'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Close'] < (Q1 - 1.5*IQR)) | (df['Close'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then create a new feature 'Open_ratio' as the ratio of 'Open' to 'Close', then handle missing values in 'Open' by imputing with median, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Open_ratio'] = df['Open'] / df['Close']\ndf['Open'].fillna(df['Open'].median(), inplace=True)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then clean text data in column 'quantity' by removing punctuation and stopwords, then compute TF-IDF features for column 'transaction_id' and display top 10 words, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['quantity_clean'] = df['quantity'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['transaction_id'])\nprint(vect.get_feature_names_out())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods, then compute TF-IDF features for column 'newbalanceOrig' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['time'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['newbalanceOrig'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then normalize the 'Pclass' column using min-max scaling, then one-hot encode the categorical column 'Survived', then plot a histogram of 'Fare'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Pclass_scaled'] = (df['Pclass'] - df['Pclass'].min()) / (df['Pclass'].max() - df['Pclass'].min())\ndf = pd.get_dummies(df, columns=['Survived'], prefix=['Survived'])\ndf['Fare'].hist()\nplt.xlabel('Fare')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then normalize the 'arrival_delay' column using min-max scaling, then compute TF-IDF features for column 'departure_delay' and display top 10 words, then train a Linear Regression model to predict 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['arrival_delay_scaled'] = (df['arrival_delay'] - df['arrival_delay'].min()) / (df['arrival_delay'].max() - df['arrival_delay'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['departure_delay'])\nprint(vect.get_feature_names_out())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then create a new feature 'store_ratio' as the ratio of 'store' to 'sales', then detect outliers in 'sales' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['store_ratio'] = df['store'] / df['sales']\nQ1 = df['sales'].quantile(0.25)\nQ3 = df['sales'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sales'] < (Q1 - 1.5*IQR)) | (df['sales'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['open'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'TotalCharges' using the IQR method, then train a Random Forest Classifier to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['TotalCharges'].quantile(0.25)\nQ3 = df['TotalCharges'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['TotalCharges'] < (Q1 - 1.5*IQR)) | (df['TotalCharges'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then create a new feature 'length_ratio' as the ratio of 'length' to 'sentiment', then train a Linear Regression model to predict 'sentiment', then train a Random Forest Classifier to predict 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['length_ratio'] = df['length'] / df['sentiment']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then detect outliers in 'petal_width' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nQ1 = df['petal_width'].quantile(0.25)\nQ3 = df['petal_width'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['petal_width'] < (Q1 - 1.5*IQR)) | (df['petal_width'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['petal_length'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform K-Means clustering with k=3 on numeric features, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nprint(df.describe())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'product_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['product_id'], prefix=['product_id'])"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then train a Linear Regression model to predict 'ecg_reading', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then create a new feature 'recovered_ratio' as the ratio of 'recovered' to 'confirmed', then perform K-Means clustering with k=3 on numeric features, then normalize the 'deaths' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['recovered_ratio'] = df['recovered'] / df['confirmed']\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['deaths_scaled'] = (df['deaths'] - df['deaths'].min()) / (df['deaths'].max() - df['deaths'].min())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then detect outliers in 'deaths' using the IQR method, then compute TF-IDF features for column 'recovered' and display top 10 words, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nQ1 = df['deaths'].quantile(0.25)\nQ3 = df['deaths'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['deaths'] < (Q1 - 1.5*IQR)) | (df['deaths'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['recovered'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then create a new feature 'UnitsSold_ratio' as the ratio of 'UnitsSold' to 'Revenue', then evaluate the model performance using RMSE and R\u00b2 score, then plot a histogram of 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['UnitsSold_ratio'] = df['UnitsSold'] / df['Revenue']\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['Revenue'].hist()\nplt.xlabel('Revenue')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then handle missing values in 'age' by imputing with median, then normalize the 'age' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['age'].fillna(df['age'].median(), inplace=True)\ndf['age_scaled'] = (df['age'] - df['age'].min()) / (df['age'].max() - df['age'].min())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then handle missing values in 'open' by imputing with median, then train a Random Forest Classifier to predict 'sales', then one-hot encode the categorical column 'day_of_week'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['open'].fillna(df['open'].median(), inplace=True)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['day_of_week'], prefix=['day_of_week'])"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'confirmed', then compute TF-IDF features for column 'confirmed' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['confirmed'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then handle missing values in 'rain_1h' by imputing with median, then detect outliers in 'temp' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['rain_1h'].fillna(df['rain_1h'].median(), inplace=True)\nQ1 = df['temp'].quantile(0.25)\nQ3 = df['temp'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['temp'] < (Q1 - 1.5*IQR)) | (df['temp'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'post_id' and display top 10 words, then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['post_id'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['text'])\ny = df['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then clean text data in column 'isFraud' by removing punctuation and stopwords, then normalize the 'time' column using min-max scaling, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['isFraud_clean'] = df['isFraud'].apply(clean)\ndf['time_scaled'] = (df['time'] - df['time'].min()) / (df['time'].max() - df['time'].min())\nprint(df.describe())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then normalize the 'pm2_5' column using min-max scaling, then perform time-series forecasting using ARIMA to predict the next 12 periods, then one-hot encode the categorical column 'so2'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['pm2_5_scaled'] = (df['pm2_5'] - df['pm2_5'].min()) / (df['pm2_5'].max() - df['pm2_5'].min())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf = pd.get_dummies(df, columns=['so2'], prefix=['so2'])"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then clean text data in column 'temperature' by removing punctuation and stopwords, then normalize the 'pressure' column using min-max scaling, then detect outliers in 'date' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['temperature_clean'] = df['temperature'].apply(clean)\ndf['pressure_scaled'] = (df['pressure'] - df['pressure'].min()) / (df['pressure'].max() - df['pressure'].min())\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'heart_rate' by removing punctuation and stopwords, then create a new feature 'time_ratio' as the ratio of 'time' to 'ecg_reading', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['heart_rate_clean'] = df['heart_rate'].apply(clean)\ndf['time_ratio'] = df['time'] / df['ecg_reading']\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then handle missing values in 'quantity' by imputing with median, then clean text data in column 'customer_id' by removing punctuation and stopwords, then train a Random Forest Classifier to predict 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['quantity'].fillna(df['quantity'].median(), inplace=True)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['customer_id_clean'] = df['customer_id'].apply(clean)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'traffic_volume', then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date_time'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'open' and display top 10 words, then create a new feature 'customers_ratio' as the ratio of 'customers' to 'sales', then normalize the 'day_of_week' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['open'])\nprint(vect.get_feature_names_out())\ndf['customers_ratio'] = df['customers'] / df['sales']\ndf['day_of_week_scaled'] = (df['day_of_week'] - df['day_of_week'].min()) / (df['day_of_week'].max() - df['day_of_week'].min())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'confirmed', then detect outliers in 'country' using the IQR method, then clean text data in column 'country' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nQ1 = df['country'].quantile(0.25)\nQ3 = df['country'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['country'] < (Q1 - 1.5*IQR)) | (df['country'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['country_clean'] = df['country'].apply(clean)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then compute TF-IDF features for column 'y' and display top 10 words, then train a Random Forest Classifier to predict 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['y'])\nprint(vect.get_feature_names_out())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then clean text data in column 'snow_1h' by removing punctuation and stopwords, then perform K-Means clustering with k=3 on numeric features, then create a new feature 'rain_1h_ratio' as the ratio of 'rain_1h' to 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['snow_1h_clean'] = df['snow_1h'].apply(clean)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['rain_1h_ratio'] = df['rain_1h'] / df['traffic_volume']"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then split the data into training and testing sets with an 80-20 split, then one-hot encode the categorical column 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['ecg_reading'])\ny = df['ecg_reading']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf = pd.get_dummies(df, columns=['ecg_reading'], prefix=['ecg_reading'])"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then split the data into training and testing sets with an 80-20 split, then compute TF-IDF features for column 'Neighborhood' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Neighborhood'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'traffic_volume', then clean text data in column 'traffic_volume' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['traffic_volume_clean'] = df['traffic_volume'].apply(clean)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'ecg_reading', then plot a histogram of 'patient_id', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['patient_id'].hist()\nplt.xlabel('patient_id')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then create a new feature 'pm10_ratio' as the ratio of 'pm10' to 'pm2_5', then plot a histogram of 'pm10', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['pm10_ratio'] = df['pm10'] / df['pm2_5']\ndf['pm10'].hist()\nplt.xlabel('pm10')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then normalize the 'Date' column using min-max scaling, then train a Linear Regression model to predict 'Close', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Date_scaled'] = (df['Date'] - df['Date'].min()) / (df['Date'].max() - df['Date'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Volume'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then split the data into training and testing sets with an 80-20 split, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['country'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then clean text data in column 'quantity' by removing punctuation and stopwords, then one-hot encode the categorical column 'transaction_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['quantity_clean'] = df['quantity'].apply(clean)\ndf = pd.get_dummies(df, columns=['transaction_id'], prefix=['transaction_id'])"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Close', then compute TF-IDF features for column 'High' and display top 10 words, then handle missing values in 'High' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf = pd.get_dummies(df, columns=['Close'], prefix=['Close'])\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['High'])\nprint(vect.get_feature_names_out())\ndf['High'].fillna(df['High'].median(), inplace=True)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)\nprint(df.describe())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then handle missing values in 'education' by imputing with median, then train a Random Forest Classifier to predict 'y', then plot a histogram of 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['education'].fillna(df['education'].median(), inplace=True)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['y'].hist()\nplt.xlabel('y')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'job' and display top 10 words, then create a new feature 'age_ratio' as the ratio of 'age' to 'y', then train a Linear Regression model to predict 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['job'])\nprint(vect.get_feature_names_out())\ndf['age_ratio'] = df['age'] / df['y']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then create a new feature 'humidity_ratio' as the ratio of 'humidity' to 'temperature', then train a Random Forest Classifier to predict 'temperature', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['humidity_ratio'] = df['humidity'] / df['temperature']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then normalize the 'ecg_reading' column using min-max scaling, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['ecg_reading_scaled'] = (df['ecg_reading'] - df['ecg_reading'].min()) / (df['ecg_reading'].max() - df['ecg_reading'].min())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['time'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nprint(df.describe())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then compute TF-IDF features for column 'shares' and display top 10 words, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['likes'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['shares'])\nprint(vect.get_feature_names_out())\nprint(df.describe())"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then handle missing values in 'sensor_value' by imputing with median, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['sensor_value'].fillna(df['sensor_value'].median(), inplace=True)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['location'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'SalePrice', then display summary statistics of all numeric columns using df.describe(), then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nprint(df.describe())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'SalePrice', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['SalePrice'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then detect outliers in 'pm2_5' using the IQR method, then train a Linear Regression model to predict 'pm2_5', then compute TF-IDF features for column 'so2' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nQ1 = df['pm2_5'].quantile(0.25)\nQ3 = df['pm2_5'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['pm2_5'] < (Q1 - 1.5*IQR)) | (df['pm2_5'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['so2'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then create a new feature 'pm10_ratio' as the ratio of 'pm10' to 'pm2_5', then perform time-series forecasting using ARIMA to predict the next 12 periods, then clean text data in column 'pm10' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['pm10_ratio'] = df['pm10'] / df['pm2_5']\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['o3'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['pm10_clean'] = df['pm10'].apply(clean)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'open', then compute TF-IDF features for column 'open' and display top 10 words, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf = pd.get_dummies(df, columns=['open'], prefix=['open'])\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['open'])\nprint(vect.get_feature_names_out())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then detect outliers in 'o3' using the IQR method, then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nQ1 = df['o3'].quantile(0.25)\nQ3 = df['o3'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['o3'] < (Q1 - 1.5*IQR)) | (df['o3'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'recovered' and display top 10 words, then train a Random Forest Classifier to predict 'confirmed', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['recovered'])\nprint(vect.get_feature_names_out())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then create a new feature 'consumption_ratio' as the ratio of 'consumption' to 'consumption', then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'humidity' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['consumption_ratio'] = df['consumption'] / df['consumption']\nprint(df.describe())\ndf['humidity'].fillna(df['humidity'].median(), inplace=True)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'shares', then perform time-series forecasting using ARIMA to predict the next 12 periods, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf = pd.get_dummies(df, columns=['shares'], prefix=['shares'])\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['post_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'MonthlyCharges', then perform K-Means clustering with k=3 on numeric features, then create a new feature 'MonthlyCharges_ratio' as the ratio of 'MonthlyCharges' to 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf = pd.get_dummies(df, columns=['MonthlyCharges'], prefix=['MonthlyCharges'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['MonthlyCharges_ratio'] = df['MonthlyCharges'] / df['Churn']"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then train a Linear Regression model to predict 'isFraud', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then normalize the 'MonthlyCharges' column using min-max scaling, then clean text data in column 'tenure' by removing punctuation and stopwords, then handle missing values in 'ContractType' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['MonthlyCharges_scaled'] = (df['MonthlyCharges'] - df['MonthlyCharges'].min()) / (df['MonthlyCharges'].max() - df['MonthlyCharges'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['tenure_clean'] = df['tenure'].apply(clean)\ndf['ContractType'].fillna(df['ContractType'].median(), inplace=True)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Random Forest Classifier to predict 'Close', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nprint(df.describe())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'ContractType', then detect outliers in 'tenure' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['ContractType'], prefix=['ContractType'])\nQ1 = df['tenure'].quantile(0.25)\nQ3 = df['tenure'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['tenure'] < (Q1 - 1.5*IQR)) | (df['tenure'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then clean text data in column 'open' by removing punctuation and stopwords, then create a new feature 'sales_ratio' as the ratio of 'sales' to 'sales', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['open_clean'] = df['open'].apply(clean)\ndf['sales_ratio'] = df['sales'] / df['sales']\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then handle missing values in 'date_time' by imputing with median, then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['date_time'].fillna(df['date_time'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['traffic_volume'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'confirmed', then one-hot encode the categorical column 'recovered', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['recovered'], prefix=['recovered'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'quality' by removing punctuation and stopwords, then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['quality_clean'] = df['quality'].apply(clean)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['patient_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'location', then handle missing values in 'device_id' by imputing with median, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['location'].hist()\nplt.xlabel('location')\nplt.ylabel('Frequency')\nplt.show()\ndf['device_id'].fillna(df['device_id'].median(), inplace=True)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then handle missing values in 'oldbalanceOrg' by imputing with median, then train a Random Forest Classifier to predict 'isFraud', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['oldbalanceOrg'].fillna(df['oldbalanceOrg'].median(), inplace=True)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nprint(df.describe())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'rain_1h', then train a Linear Regression model to predict 'traffic_volume', then detect outliers in 'temp' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf = pd.get_dummies(df, columns=['rain_1h'], prefix=['rain_1h'])\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nQ1 = df['temp'].quantile(0.25)\nQ3 = df['temp'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['temp'] < (Q1 - 1.5*IQR)) | (df['temp'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display feature importances from the Random Forest model, then normalize the 'so2' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['pm10'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['so2_scaled'] = (df['so2'] - df['so2'].min()) / (df['so2'].max() - df['so2'].min())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then detect outliers in 'MonthlyCharges' using the IQR method, then one-hot encode the categorical column 'Churn', then train a Random Forest Classifier to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nQ1 = df['MonthlyCharges'].quantile(0.25)\nQ3 = df['MonthlyCharges'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['MonthlyCharges'] < (Q1 - 1.5*IQR)) | (df['MonthlyCharges'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['Churn'], prefix=['Churn'])\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'pm10' and display top 10 words, then detect outliers in 'pm10' using the IQR method, then one-hot encode the categorical column 'o3'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['pm10'])\nprint(vect.get_feature_names_out())\nQ1 = df['pm10'].quantile(0.25)\nQ3 = df['pm10'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['pm10'] < (Q1 - 1.5*IQR)) | (df['pm10'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['o3'], prefix=['o3'])"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then create a new feature 'Sex_ratio' as the ratio of 'Sex' to 'Survived', then clean text data in column 'Age' by removing punctuation and stopwords, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Sex_ratio'] = df['Sex'] / df['Survived']\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Age_clean'] = df['Age'].apply(clean)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'YearBuilt' and display top 10 words, then plot a histogram of 'YearBuilt', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['YearBuilt'])\nprint(vect.get_feature_names_out())\ndf['YearBuilt'].hist()\nplt.xlabel('YearBuilt')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then clean text data in column 'humidity' by removing punctuation and stopwords, then compute TF-IDF features for column 'consumption' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['temperature'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['humidity_clean'] = df['humidity'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['consumption'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'consumption', then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'temperature' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['consumption'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['temperature'].quantile(0.25)\nQ3 = df['temperature'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['temperature'] < (Q1 - 1.5*IQR)) | (df['temperature'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then normalize the 'likes' column using min-max scaling, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nprint(df.describe())\ndf['likes_scaled'] = (df['likes'] - df['likes'].min()) / (df['likes'].max() - df['likes'].min())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then handle missing values in 'newbalanceOrig' by imputing with median, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['newbalanceOrig'].fillna(df['newbalanceOrig'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then create a new feature 'patient_id_ratio' as the ratio of 'patient_id' to 'ecg_reading', then calculate the correlation matrix for numeric features, then one-hot encode the categorical column 'time'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['patient_id_ratio'] = df['patient_id'] / df['ecg_reading']\ncorr = df.corr()\nprint(corr)\ndf = pd.get_dummies(df, columns=['time'], prefix=['time'])"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then plot a histogram of 'TotalCharges', then train a Random Forest Classifier to predict 'Churn', then one-hot encode the categorical column 'TotalCharges'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['TotalCharges'].hist()\nplt.xlabel('TotalCharges')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['TotalCharges'], prefix=['TotalCharges'])"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'ecg_reading', then display feature importances from the Random Forest model, then clean text data in column 'quality' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['quality_clean'] = df['quality'].apply(clean)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then compute TF-IDF features for column 'Close' and display top 10 words, then detect outliers in 'Close' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Close'])\nprint(vect.get_feature_names_out())\nQ1 = df['Close'].quantile(0.25)\nQ3 = df['Close'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Close'] < (Q1 - 1.5*IQR)) | (df['Close'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)\nprint(df.describe())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model, then clean text data in column 'job' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['job_clean'] = df['job'].apply(clean)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then clean text data in column 'humidity' by removing punctuation and stopwords, then split the data into training and testing sets with an 80-20 split, then handle missing values in 'date' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['humidity_clean'] = df['humidity'].apply(clean)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['date'].fillna(df['date'].median(), inplace=True)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then detect outliers in 'tenure' using the IQR method, then display feature importances from the Random Forest model, then train a Random Forest Classifier to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nQ1 = df['tenure'].quantile(0.25)\nQ3 = df['tenure'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['tenure'] < (Q1 - 1.5*IQR)) | (df['tenure'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then detect outliers in 'day_of_week' using the IQR method, then plot a histogram of 'day_of_week'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ncorr = df.corr()\nprint(corr)\nQ1 = df['day_of_week'].quantile(0.25)\nQ3 = df['day_of_week'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['day_of_week'] < (Q1 - 1.5*IQR)) | (df['day_of_week'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['day_of_week'].hist()\nplt.xlabel('day_of_week')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then plot a histogram of 'customer_id', then detect outliers in 'customer_id' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['customer_id'].hist()\nplt.xlabel('customer_id')\nplt.ylabel('Frequency')\nplt.show()\nQ1 = df['customer_id'].quantile(0.25)\nQ3 = df['customer_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['customer_id'] < (Q1 - 1.5*IQR)) | (df['customer_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then plot a histogram of 'date', then normalize the 'pressure' column using min-max scaling, then clean text data in column 'pressure' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['date'].hist()\nplt.xlabel('date')\nplt.ylabel('Frequency')\nplt.show()\ndf['pressure_scaled'] = (df['pressure'] - df['pressure'].min()) / (df['pressure'].max() - df['pressure'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['pressure_clean'] = df['pressure'].apply(clean)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then create a new feature 'UnitsSold_ratio' as the ratio of 'UnitsSold' to 'Revenue', then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'UnitsSold'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['UnitsSold_ratio'] = df['UnitsSold'] / df['Revenue']\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['UnitsSold'], prefix=['UnitsSold'])"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sensor_value', then one-hot encode the categorical column 'device_id', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['device_id'], prefix=['device_id'])\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods, then one-hot encode the categorical column 'length'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sentiment'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf = pd.get_dummies(df, columns=['length'], prefix=['length'])"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'SalePrice', then clean text data in column 'OverallQual' by removing punctuation and stopwords, then train a Random Forest Classifier to predict 'SalePrice'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['OverallQual_clean'] = df['OverallQual'].apply(clean)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then clean text data in column 'recovered' by removing punctuation and stopwords, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['recovered_clean'] = df['recovered'].apply(clean)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then normalize the 'precipitation' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score, then create a new feature 'humidity_ratio' as the ratio of 'humidity' to 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['precipitation_scaled'] = (df['precipitation'] - df['precipitation'].min()) / (df['precipitation'].max() - df['precipitation'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['humidity_ratio'] = df['humidity'] / df['temperature']"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then create a new feature 'store_ratio' as the ratio of 'store' to 'sales', then train a Random Forest Classifier to predict 'sales', then handle missing values in 'customers' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['store_ratio'] = df['store'] / df['sales']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['customers'].fillna(df['customers'].median(), inplace=True)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then create a new feature 'Pclass_ratio' as the ratio of 'Pclass' to 'Survived', then train a Linear Regression model to predict 'Survived', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Pclass_ratio'] = df['Pclass'] / df['Survived']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'date' using the IQR method, then create a new feature 'temperature_ratio' as the ratio of 'temperature' to 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['temperature_ratio'] = df['temperature'] / df['temperature']"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'status', then train a Linear Regression model to predict 'sensor_value', then create a new feature 'timestamp_ratio' as the ratio of 'timestamp' to 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf = pd.get_dummies(df, columns=['status'], prefix=['status'])\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['timestamp_ratio'] = df['timestamp'] / df['sensor_value']"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then create a new feature 'o3_ratio' as the ratio of 'o3' to 'pm2_5', then split the data into training and testing sets with an 80-20 split, then compute TF-IDF features for column 'o3' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['o3_ratio'] = df['o3'] / df['pm2_5']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['o3'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then detect outliers in 'High' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nQ1 = df['High'].quantile(0.25)\nQ3 = df['High'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['High'] < (Q1 - 1.5*IQR)) | (df['High'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Low'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Close'])\ny = df['Close']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'text', then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'likes' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['likes'].quantile(0.25)\nQ3 = df['likes'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['likes'] < (Q1 - 1.5*IQR)) | (df['likes'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'flight', then clean text data in column 'carrier' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nprint(df.describe())\ndf['flight'].hist()\nplt.xlabel('flight')\nplt.ylabel('Frequency')\nplt.show()\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['carrier_clean'] = df['carrier'].apply(clean)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'confirmed', then detect outliers in 'deaths' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nQ1 = df['deaths'].quantile(0.25)\nQ3 = df['deaths'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['deaths'] < (Q1 - 1.5*IQR)) | (df['deaths'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods, then plot a histogram of 'TotalCharges'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['TotalCharges'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['TotalCharges'].hist()\nplt.xlabel('TotalCharges')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then plot a histogram of 'sentiment', then detect outliers in 'genre' using the IQR method, then one-hot encode the categorical column 'rating'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['sentiment'].hist()\nplt.xlabel('sentiment')\nplt.ylabel('Frequency')\nplt.show()\nQ1 = df['genre'].quantile(0.25)\nQ3 = df['genre'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['genre'] < (Q1 - 1.5*IQR)) | (df['genre'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['rating'], prefix=['rating'])"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then handle missing values in 'date' by imputing with median, then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['date'].fillna(df['date'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then detect outliers in 'date' using the IQR method, then display feature importances from the Random Forest model, then train a Random Forest Classifier to predict 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sensor_value', then detect outliers in 'device_id' using the IQR method, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['device_id'].quantile(0.25)\nQ3 = df['device_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['device_id'] < (Q1 - 1.5*IQR)) | (df['device_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nprint(df.describe())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then detect outliers in 'humidity' using the IQR method, then compute TF-IDF features for column 'consumption' and display top 10 words, then clean text data in column 'humidity' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nQ1 = df['humidity'].quantile(0.25)\nQ3 = df['humidity'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['humidity'] < (Q1 - 1.5*IQR)) | (df['humidity'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['consumption'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['humidity_clean'] = df['humidity'].apply(clean)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'precipitation', then create a new feature 'date_ratio' as the ratio of 'date' to 'temperature', then handle missing values in 'date' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf = pd.get_dummies(df, columns=['precipitation'], prefix=['precipitation'])\ndf['date_ratio'] = df['date'] / df['temperature']\ndf['date'].fillna(df['date'].median(), inplace=True)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then plot a histogram of 'job', then detect outliers in 'y' using the IQR method, then normalize the 'y' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['job'].hist()\nplt.xlabel('job')\nplt.ylabel('Frequency')\nplt.show()\nQ1 = df['y'].quantile(0.25)\nQ3 = df['y'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['y'] < (Q1 - 1.5*IQR)) | (df['y'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['y_scaled'] = (df['y'] - df['y'].min()) / (df['y'].max() - df['y'].min())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'heart_rate' by removing punctuation and stopwords, then evaluate the model performance using RMSE and R\u00b2 score, then compute TF-IDF features for column 'patient_id' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['heart_rate_clean'] = df['heart_rate'].apply(clean)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['patient_id'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then normalize the 'age' column using min-max scaling, then compute TF-IDF features for column 'marital' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nprint(df.describe())\ndf['age_scaled'] = (df['age'] - df['age'].min()) / (df['age'].max() - df['age'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['marital'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then detect outliers in 'text' using the IQR method, then create a new feature 'likes_ratio' as the ratio of 'likes' to 'text', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nQ1 = df['text'].quantile(0.25)\nQ3 = df['text'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['text'] < (Q1 - 1.5*IQR)) | (df['text'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['likes_ratio'] = df['likes'] / df['text']\nprint(df.describe())"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'timestamp', then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'location' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['timestamp'].hist()\nplt.xlabel('timestamp')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sensor_value'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['location'].quantile(0.25)\nQ3 = df['location'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['location'] < (Q1 - 1.5*IQR)) | (df['location'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'total_amount', then create a new feature 'product_id_ratio' as the ratio of 'product_id' to 'total_amount', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['product_id_ratio'] = df['product_id'] / df['total_amount']\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['customer_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'MonthlyCharges'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['MonthlyCharges'].hist()\nplt.xlabel('MonthlyCharges')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'review' using the IQR method, then create a new feature 'sentiment_ratio' as the ratio of 'sentiment' to 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['length'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['review'].quantile(0.25)\nQ3 = df['review'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['review'] < (Q1 - 1.5*IQR)) | (df['review'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['sentiment_ratio'] = df['sentiment'] / df['sentiment']"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'oldbalanceOrg' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['newbalanceOrig'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['oldbalanceOrg_scaled'] = (df['oldbalanceOrg'] - df['oldbalanceOrg'].min()) / (df['oldbalanceOrg'].max() - df['oldbalanceOrg'].min())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'text', then detect outliers in 'user_id' using the IQR method, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['user_id'].quantile(0.25)\nQ3 = df['user_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['user_id'] < (Q1 - 1.5*IQR)) | (df['user_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then plot a histogram of 'marital', then one-hot encode the categorical column 'age', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['marital'].hist()\nplt.xlabel('marital')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['age'], prefix=['age'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'distance' and display top 10 words, then plot a histogram of 'arrival_delay', then create a new feature 'flight_ratio' as the ratio of 'flight' to 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['distance'])\nprint(vect.get_feature_names_out())\ndf['arrival_delay'].hist()\nplt.xlabel('arrival_delay')\nplt.ylabel('Frequency')\nplt.show()\ndf['flight_ratio'] = df['flight'] / df['arrival_delay']"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then detect outliers in 'genre' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ncorr = df.corr()\nprint(corr)\nQ1 = df['genre'].quantile(0.25)\nQ3 = df['genre'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['genre'] < (Q1 - 1.5*IQR)) | (df['genre'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sentiment'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then create a new feature 'ecg_reading_ratio' as the ratio of 'ecg_reading' to 'ecg_reading', then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['ecg_reading_ratio'] = df['ecg_reading'] / df['ecg_reading']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['ecg_reading'])\ny = df['ecg_reading']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'consumption', then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['humidity'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Close', then handle missing values in 'Date' by imputing with median, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf = pd.get_dummies(df, columns=['Close'], prefix=['Close'])\ndf['Date'].fillna(df['Date'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'isFraud', then perform time-series forecasting using ARIMA to predict the next 12 periods, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['oldbalanceOrg'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Survived', then evaluate the model performance using RMSE and R\u00b2 score, then detect outliers in 'Sex' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Survived'].hist()\nplt.xlabel('Survived')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nQ1 = df['Sex'].quantile(0.25)\nQ3 = df['Sex'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Sex'] < (Q1 - 1.5*IQR)) | (df['Sex'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then create a new feature 'MonthlyCharges_ratio' as the ratio of 'MonthlyCharges' to 'Churn', then one-hot encode the categorical column 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['MonthlyCharges_ratio'] = df['MonthlyCharges'] / df['Churn']\ndf = pd.get_dummies(df, columns=['Churn'], prefix=['Churn'])"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Random Forest Classifier to predict 'sensor_value', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nprint(df.describe())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sensor_value'])\ny = df['sensor_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then handle missing values in 'humidity' by imputing with median, then create a new feature 'date_ratio' as the ratio of 'date' to 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['humidity'].fillna(df['humidity'].median(), inplace=True)\ndf['date_ratio'] = df['date'] / df['consumption']"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then clean text data in column 'petal_length' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe(), then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['petal_length_clean'] = df['petal_length'].apply(clean)\nprint(df.describe())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sepal_width'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then normalize the 'day_of_week' column using min-max scaling, then split the data into training and testing sets with an 80-20 split, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['day_of_week_scaled'] = (df['day_of_week'] - df['day_of_week'].min()) / (df['day_of_week'].max() - df['day_of_week'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['open'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then detect outliers in 'newbalanceOrig' using the IQR method, then handle missing values in 'oldbalanceOrg' by imputing with median, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nQ1 = df['newbalanceOrig'].quantile(0.25)\nQ3 = df['newbalanceOrig'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['newbalanceOrig'] < (Q1 - 1.5*IQR)) | (df['newbalanceOrig'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['oldbalanceOrg'].fillna(df['oldbalanceOrg'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then detect outliers in 'marital' using the IQR method, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['marital'].quantile(0.25)\nQ3 = df['marital'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['marital'] < (Q1 - 1.5*IQR)) | (df['marital'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nprint(df.describe())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then create a new feature 'time_ratio' as the ratio of 'time' to 'isFraud', then split the data into training and testing sets with an 80-20 split, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['time_ratio'] = df['time'] / df['isFraud']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Linear Regression model to predict 'y', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nprint(df.describe())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Survived', then create a new feature 'Survived_ratio' as the ratio of 'Survived' to 'Survived', then clean text data in column 'Age' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['Survived_ratio'] = df['Survived'] / df['Survived']\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Age_clean'] = df['Age'].apply(clean)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'SalePrice', then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then clean text data in column 'sentiment' by removing punctuation and stopwords, then handle missing values in 'review' by imputing with median, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sentiment_clean'] = df['sentiment'].apply(clean)\ndf['review'].fillna(df['review'].median(), inplace=True)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display summary statistics of all numeric columns using df.describe(), then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sepal_length'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nprint(df.describe())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'consumption', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['humidity'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then create a new feature 'Churn_ratio' as the ratio of 'Churn' to 'Churn', then one-hot encode the categorical column 'ContractType', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['Churn_ratio'] = df['Churn'] / df['Churn']\ndf = pd.get_dummies(df, columns=['ContractType'], prefix=['ContractType'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'date' and display top 10 words, then detect outliers in 'pm10' using the IQR method, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date'])\nprint(vect.get_feature_names_out())\nQ1 = df['pm10'].quantile(0.25)\nQ3 = df['pm10'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['pm10'] < (Q1 - 1.5*IQR)) | (df['pm10'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'snow_1h', then handle missing values in 'rain_1h' by imputing with median, then plot a histogram of 'temp'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf = pd.get_dummies(df, columns=['snow_1h'], prefix=['snow_1h'])\ndf['rain_1h'].fillna(df['rain_1h'].median(), inplace=True)\ndf['temp'].hist()\nplt.xlabel('temp')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'YearBuilt' and display top 10 words, then clean text data in column 'SalePrice' by removing punctuation and stopwords, then handle missing values in 'Neighborhood' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['YearBuilt'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['SalePrice_clean'] = df['SalePrice'].apply(clean)\ndf['Neighborhood'].fillna(df['Neighborhood'].median(), inplace=True)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Survived', then display feature importances from the Random Forest model, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Survived'].hist()\nplt.xlabel('Survived')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Pclass'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then detect outliers in 'transaction_id' using the IQR method, then normalize the 'quantity' column using min-max scaling, then compute TF-IDF features for column 'transaction_id' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nQ1 = df['transaction_id'].quantile(0.25)\nQ3 = df['transaction_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['transaction_id'] < (Q1 - 1.5*IQR)) | (df['transaction_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['quantity_scaled'] = (df['quantity'] - df['quantity'].min()) / (df['quantity'].max() - df['quantity'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['transaction_id'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'pm2_5', then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Random Forest Classifier to predict 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['pm10'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'species' and display top 10 words, then detect outliers in 'petal_length' using the IQR method, then one-hot encode the categorical column 'petal_width'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['species'])\nprint(vect.get_feature_names_out())\nQ1 = df['petal_length'].quantile(0.25)\nQ3 = df['petal_length'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['petal_length'] < (Q1 - 1.5*IQR)) | (df['petal_length'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['petal_width'], prefix=['petal_width'])"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'ecg_reading', then evaluate the model performance using RMSE and R\u00b2 score, then handle missing values in 'quality' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['quality'].fillna(df['quality'].median(), inplace=True)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then normalize the 'sepal_length' column using min-max scaling, then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['sepal_length_scaled'] = (df['sepal_length'] - df['sepal_length'].min()) / (df['sepal_length'].max() - df['sepal_length'].min())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then handle missing values in 'date' by imputing with median, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['wind_speed'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['date'].fillna(df['date'].median(), inplace=True)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then create a new feature 'consumption_ratio' as the ratio of 'consumption' to 'consumption', then detect outliers in 'consumption' using the IQR method, then normalize the 'consumption' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['consumption_ratio'] = df['consumption'] / df['consumption']\nQ1 = df['consumption'].quantile(0.25)\nQ3 = df['consumption'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['consumption'] < (Q1 - 1.5*IQR)) | (df['consumption'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['consumption_scaled'] = (df['consumption'] - df['consumption'].min()) / (df['consumption'].max() - df['consumption'].min())"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then create a new feature 'carrier_ratio' as the ratio of 'carrier' to 'arrival_delay', then plot a histogram of 'distance', then normalize the 'arrival_delay' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['carrier_ratio'] = df['carrier'] / df['arrival_delay']\ndf['distance'].hist()\nplt.xlabel('distance')\nplt.ylabel('Frequency')\nplt.show()\ndf['arrival_delay_scaled'] = (df['arrival_delay'] - df['arrival_delay'].min()) / (df['arrival_delay'].max() - df['arrival_delay'].min())"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'sensor_value', then detect outliers in 'timestamp' using the IQR method, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['timestamp'].quantile(0.25)\nQ3 = df['timestamp'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['timestamp'] < (Q1 - 1.5*IQR)) | (df['timestamp'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'distance' and display top 10 words, then train a Linear Regression model to predict 'arrival_delay', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['distance'])\nprint(vect.get_feature_names_out())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then create a new feature 'oldbalanceOrg_ratio' as the ratio of 'oldbalanceOrg' to 'isFraud', then evaluate the model performance using RMSE and R\u00b2 score, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['oldbalanceOrg_ratio'] = df['oldbalanceOrg'] / df['isFraud']\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'Age', then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'Sex'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf = pd.get_dummies(df, columns=['Age'], prefix=['Age'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['Sex'].hist()\nplt.xlabel('Sex')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sensor_value', then clean text data in column 'device_id' by removing punctuation and stopwords, then normalize the 'location' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['device_id_clean'] = df['device_id'].apply(clean)\ndf['location_scaled'] = (df['location'] - df['location'].min()) / (df['location'].max() - df['location'].min())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then clean text data in column 'consumption' by removing punctuation and stopwords, then one-hot encode the categorical column 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ncorr = df.corr()\nprint(corr)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['consumption_clean'] = df['consumption'].apply(clean)\ndf = pd.get_dummies(df, columns=['consumption'], prefix=['consumption'])"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then create a new feature 'shares_ratio' as the ratio of 'shares' to 'text', then display feature importances from the Random Forest model, then handle missing values in 'likes' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['shares_ratio'] = df['shares'] / df['text']\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['likes'].fillna(df['likes'].median(), inplace=True)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'tenure', then clean text data in column 'ContractType' by removing punctuation and stopwords, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf = pd.get_dummies(df, columns=['tenure'], prefix=['tenure'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['ContractType_clean'] = df['ContractType'].apply(clean)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then clean text data in column 'consumption' by removing punctuation and stopwords, then train a Random Forest Classifier to predict 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['consumption_clean'] = df['consumption'].apply(clean)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then handle missing values in 'humidity' by imputing with median, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['humidity'].fillna(df['humidity'].median(), inplace=True)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'isFraud', then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['isFraud'])\ny = df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'YearBuilt', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['YearBuilt'], prefix=['YearBuilt'])\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['YearBuilt'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'date', then handle missing values in 'pressure' by imputing with median, then clean text data in column 'pressure' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf = pd.get_dummies(df, columns=['date'], prefix=['date'])\ndf['pressure'].fillna(df['pressure'].median(), inplace=True)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['pressure_clean'] = df['pressure'].apply(clean)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then handle missing values in 'open' by imputing with median, then create a new feature 'sales_ratio' as the ratio of 'sales' to 'sales', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['open'].fillna(df['open'].median(), inplace=True)\ndf['sales_ratio'] = df['sales'] / df['sales']\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features, then plot a histogram of 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)\ndf['traffic_volume'].hist()\nplt.xlabel('traffic_volume')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then normalize the 'oldbalanceOrg' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['oldbalanceOrg_scaled'] = (df['oldbalanceOrg'] - df['oldbalanceOrg'].min()) / (df['oldbalanceOrg'].max() - df['oldbalanceOrg'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['amount'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then handle missing values in 'length' by imputing with median, then split the data into training and testing sets with an 80-20 split, then normalize the 'sentiment' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['length'].fillna(df['length'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['sentiment_scaled'] = (df['sentiment'] - df['sentiment'].min()) / (df['sentiment'].max() - df['sentiment'].min())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then display summary statistics of all numeric columns using df.describe(), then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nprint(df.describe())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then plot a histogram of 'oldbalanceOrg', then clean text data in column 'time' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['oldbalanceOrg'].hist()\nplt.xlabel('oldbalanceOrg')\nplt.ylabel('Frequency')\nplt.show()\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['time_clean'] = df['time'].apply(clean)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then plot a histogram of 'recovered', then split the data into training and testing sets with an 80-20 split, then clean text data in column 'recovered' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ndf['recovered'].hist()\nplt.xlabel('recovered')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['recovered_clean'] = df['recovered'].apply(clean)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then create a new feature 'petal_length_ratio' as the ratio of 'petal_length' to 'species', then normalize the 'sepal_width' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['petal_length_ratio'] = df['petal_length'] / df['species']\ndf['sepal_width_scaled'] = (df['sepal_width'] - df['sepal_width'].min()) / (df['sepal_width'].max() - df['sepal_width'].min())"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then normalize the 'review' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['review_scaled'] = (df['review'] - df['review'].min()) / (df['review'].max() - df['review'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then plot a histogram of 'temp', then train a Random Forest Classifier to predict 'traffic_volume', then create a new feature 'temp_ratio' as the ratio of 'temp' to 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['temp'].hist()\nplt.xlabel('temp')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['temp_ratio'] = df['temp'] / df['traffic_volume']"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then create a new feature 'Low_ratio' as the ratio of 'Low' to 'Close', then detect outliers in 'Volume' using the IQR method, then compute TF-IDF features for column 'Open' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Low_ratio'] = df['Low'] / df['Close']\nQ1 = df['Volume'].quantile(0.25)\nQ3 = df['Volume'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Volume'] < (Q1 - 1.5*IQR)) | (df['Volume'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Open'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Random Forest Classifier to predict 'sales', then clean text data in column 'customers' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['customers_clean'] = df['customers'].apply(clean)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'time' and display top 10 words, then normalize the 'ecg_reading' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['time'])\nprint(vect.get_feature_names_out())\ndf['ecg_reading_scaled'] = (df['ecg_reading'] - df['ecg_reading'].min()) / (df['ecg_reading'].max() - df['ecg_reading'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then handle missing values in 'sentiment' by imputing with median, then train a Random Forest Classifier to predict 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ncorr = df.corr()\nprint(corr)\ndf['sentiment'].fillna(df['sentiment'].median(), inplace=True)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then handle missing values in 'transaction_id' by imputing with median, then evaluate the model performance using RMSE and R\u00b2 score, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf['transaction_id'].fillna(df['transaction_id'].median(), inplace=True)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nprint(df.describe())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then handle missing values in 'consumption' by imputing with median, then evaluate the model performance using RMSE and R\u00b2 score, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['consumption'].fillna(df['consumption'].median(), inplace=True)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'sentiment' by imputing with median, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nprint(df.describe())\ndf['sentiment'].fillna(df['sentiment'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features, then create a new feature 'total_amount_ratio' as the ratio of 'total_amount' to 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['product_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['total_amount_ratio'] = df['total_amount'] / df['total_amount']"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then normalize the 'o3' column using min-max scaling, then handle missing values in 'no2' by imputing with median, then compute TF-IDF features for column 'so2' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['o3_scaled'] = (df['o3'] - df['o3'].min()) / (df['o3'].max() - df['o3'].min())\ndf['no2'].fillna(df['no2'].median(), inplace=True)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['so2'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then train a Linear Regression model to predict 'confirmed', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then handle missing values in 'distance' by imputing with median, then plot a histogram of 'departure_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nprint(df.describe())\ndf['distance'].fillna(df['distance'].median(), inplace=True)\ndf['departure_delay'].hist()\nplt.xlabel('departure_delay')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then perform K-Means clustering with k=3 on numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sepal_width'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'YearBuilt', then detect outliers in 'OverallQual' using the IQR method, then clean text data in column 'SalePrice' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf = pd.get_dummies(df, columns=['YearBuilt'], prefix=['YearBuilt'])\nQ1 = df['OverallQual'].quantile(0.25)\nQ3 = df['OverallQual'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['OverallQual'] < (Q1 - 1.5*IQR)) | (df['OverallQual'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['SalePrice_clean'] = df['SalePrice'].apply(clean)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then normalize the 'device_id' column using min-max scaling, then display summary statistics of all numeric columns using df.describe(), then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['device_id_scaled'] = (df['device_id'] - df['device_id'].min()) / (df['device_id'].max() - df['device_id'].min())\nprint(df.describe())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then plot a histogram of 'no2', then clean text data in column 'pm10' by removing punctuation and stopwords, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['no2'].hist()\nplt.xlabel('no2')\nplt.ylabel('Frequency')\nplt.show()\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['pm10_clean'] = df['pm10'].apply(clean)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'humidity' and display top 10 words, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['humidity'])\nprint(vect.get_feature_names_out())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['temperature'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'education', then calculate the correlation matrix for numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf = pd.get_dummies(df, columns=['education'], prefix=['education'])\ncorr = df.corr()\nprint(corr)\nprint(df.describe())"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then create a new feature 'device_id_ratio' as the ratio of 'device_id' to 'sensor_value', then plot a histogram of 'timestamp', then clean text data in column 'sensor_value' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['device_id_ratio'] = df['device_id'] / df['sensor_value']\ndf['timestamp'].hist()\nplt.xlabel('timestamp')\nplt.ylabel('Frequency')\nplt.show()\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sensor_value_clean'] = df['sensor_value'].apply(clean)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'pm2_5', then display feature importances from the Random Forest model, then detect outliers in 'date' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf = pd.get_dummies(df, columns=['pm2_5'], prefix=['pm2_5'])\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then create a new feature 'carrier_ratio' as the ratio of 'carrier' to 'arrival_delay', then normalize the 'distance' column using min-max scaling, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['carrier_ratio'] = df['carrier'] / df['arrival_delay']\ndf['distance_scaled'] = (df['distance'] - df['distance'].min()) / (df['distance'].max() - df['distance'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then handle missing values in 'distance' by imputing with median, then normalize the 'carrier' column using min-max scaling, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['distance'].fillna(df['distance'].median(), inplace=True)\ndf['carrier_scaled'] = (df['carrier'] - df['carrier'].min()) / (df['carrier'].max() - df['carrier'].min())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display summary statistics of all numeric columns using df.describe(), then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['flight'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nprint(df.describe())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['arrival_delay'])\ny = df['arrival_delay']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then create a new feature 'Product_ratio' as the ratio of 'Product' to 'Revenue', then train a Random Forest Classifier to predict 'Revenue', then handle missing values in 'Date' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Product_ratio'] = df['Product'] / df['Revenue']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['Date'].fillna(df['Date'].median(), inplace=True)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'LotArea' and display top 10 words, then detect outliers in 'Neighborhood' using the IQR method, then normalize the 'Neighborhood' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['LotArea'])\nprint(vect.get_feature_names_out())\nQ1 = df['Neighborhood'].quantile(0.25)\nQ3 = df['Neighborhood'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Neighborhood'] < (Q1 - 1.5*IQR)) | (df['Neighborhood'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['Neighborhood_scaled'] = (df['Neighborhood'] - df['Neighborhood'].min()) / (df['Neighborhood'].max() - df['Neighborhood'].min())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then clean text data in column 'Volume' by removing punctuation and stopwords, then create a new feature 'Open_ratio' as the ratio of 'Open' to 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Volume_clean'] = df['Volume'].apply(clean)\ndf['Open_ratio'] = df['Open'] / df['Close']"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Random Forest Classifier to predict 'sensor_value', then clean text data in column 'status' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nprint(df.describe())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['status_clean'] = df['status'].apply(clean)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'ecg_reading', then normalize the 'patient_id' column using min-max scaling, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['patient_id_scaled'] = (df['patient_id'] - df['patient_id'].min()) / (df['patient_id'].max() - df['patient_id'].min())\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Churn', then calculate the correlation matrix for numeric features, then normalize the 'MonthlyCharges' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)\ndf['MonthlyCharges_scaled'] = (df['MonthlyCharges'] - df['MonthlyCharges'].min()) / (df['MonthlyCharges'].max() - df['MonthlyCharges'].min())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then handle missing values in 'petal_width' by imputing with median, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['petal_width'].fillna(df['petal_width'].median(), inplace=True)\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features, then compute TF-IDF features for column 'confirmed' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['confirmed'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then compute TF-IDF features for column 'job' and display top 10 words, then clean text data in column 'job' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['job'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['job_clean'] = df['job'].apply(clean)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then detect outliers in 'age' using the IQR method, then clean text data in column 'job' by removing punctuation and stopwords, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nQ1 = df['age'].quantile(0.25)\nQ3 = df['age'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['age'] < (Q1 - 1.5*IQR)) | (df['age'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['job_clean'] = df['job'].apply(clean)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['y'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then detect outliers in 'Survived' using the IQR method, then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nQ1 = df['Survived'].quantile(0.25)\nQ3 = df['Survived'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Survived'] < (Q1 - 1.5*IQR)) | (df['Survived'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then handle missing values in 'Pclass' by imputing with median, then calculate the correlation matrix for numeric features, then clean text data in column 'Pclass' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Pclass'].fillna(df['Pclass'].median(), inplace=True)\ncorr = df.corr()\nprint(corr)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Pclass_clean'] = df['Pclass'].apply(clean)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then detect outliers in 'Open' using the IQR method, then one-hot encode the categorical column 'Date'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ncorr = df.corr()\nprint(corr)\nQ1 = df['Open'].quantile(0.25)\nQ3 = df['Open'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Open'] < (Q1 - 1.5*IQR)) | (df['Open'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['Date'], prefix=['Date'])"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'genre', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['genre'].hist()\nplt.xlabel('genre')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['review'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'location' by imputing with median, then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'timestamp' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['location'].fillna(df['location'].median(), inplace=True)\nprint(df.describe())\nQ1 = df['timestamp'].quantile(0.25)\nQ3 = df['timestamp'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['timestamp'] < (Q1 - 1.5*IQR)) | (df['timestamp'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'MonthlyCharges', then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf = pd.get_dummies(df, columns=['MonthlyCharges'], prefix=['MonthlyCharges'])\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then plot a histogram of 'pm10', then one-hot encode the categorical column 'date', then train a Linear Regression model to predict 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['pm10'].hist()\nplt.xlabel('pm10')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['date'], prefix=['date'])\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then clean text data in column 'flight' by removing punctuation and stopwords, then create a new feature 'distance_ratio' as the ratio of 'distance' to 'arrival_delay', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['flight_clean'] = df['flight'].apply(clean)\ndf['distance_ratio'] = df['distance'] / df['arrival_delay']\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Linear Regression model to predict 'Survived', then handle missing values in 'Pclass' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nprint(df.describe())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['Pclass'].fillna(df['Pclass'].median(), inplace=True)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'quantity', then handle missing values in 'transaction_id' by imputing with median, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf = pd.get_dummies(df, columns=['quantity'], prefix=['quantity'])\ndf['transaction_id'].fillna(df['transaction_id'].median(), inplace=True)\nprint(df.describe())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then calculate the correlation matrix for numeric features, then detect outliers in 'amount' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nprint(df.describe())\ncorr = df.corr()\nprint(corr)\nQ1 = df['amount'].quantile(0.25)\nQ3 = df['amount'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['amount'] < (Q1 - 1.5*IQR)) | (df['amount'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'ecg_reading', then clean text data in column 'time' by removing punctuation and stopwords, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['time_clean'] = df['time'].apply(clean)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'rain_1h' and display top 10 words, then clean text data in column 'rain_1h' by removing punctuation and stopwords, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['rain_1h'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['rain_1h_clean'] = df['rain_1h'].apply(clean)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'date_time' and display top 10 words, then clean text data in column 'traffic_volume' by removing punctuation and stopwords, then train a Random Forest Classifier to predict 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date_time'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['traffic_volume_clean'] = df['traffic_volume'].apply(clean)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then clean text data in column 'sales' by removing punctuation and stopwords, then normalize the 'day_of_week' column using min-max scaling, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sales_clean'] = df['sales'].apply(clean)\ndf['day_of_week_scaled'] = (df['day_of_week'] - df['day_of_week'].min()) / (df['day_of_week'].max() - df['day_of_week'].min())\nprint(df.describe())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'species', then train a Random Forest Classifier to predict 'species', then normalize the 'petal_width' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['petal_width_scaled'] = (df['petal_width'] - df['petal_width'].min()) / (df['petal_width'].max() - df['petal_width'].min())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'precipitation' by imputing with median, then display summary statistics of all numeric columns using df.describe(), then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['precipitation'].fillna(df['precipitation'].median(), inplace=True)\nprint(df.describe())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'patient_id' and display top 10 words, then train a Random Forest Classifier to predict 'ecg_reading', then train a Linear Regression model to predict 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['patient_id'])\nprint(vect.get_feature_names_out())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then one-hot encode the categorical column 'device_id', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf = pd.get_dummies(df, columns=['device_id'], prefix=['device_id'])\nprint(df.describe())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Random Forest Classifier to predict 'pm2_5', then one-hot encode the categorical column 'no2'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['no2'], prefix=['no2'])"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'SalePrice', then plot a histogram of 'YearBuilt', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf = pd.get_dummies(df, columns=['SalePrice'], prefix=['SalePrice'])\ndf['YearBuilt'].hist()\nplt.xlabel('YearBuilt')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then plot a histogram of 'length', then display feature importances from the Random Forest model, then normalize the 'length' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['length'].hist()\nplt.xlabel('length')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['length_scaled'] = (df['length'] - df['length'].min()) / (df['length'].max() - df['length'].min())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then clean text data in column 'Churn' by removing punctuation and stopwords, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Churn_clean'] = df['Churn'].apply(clean)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['MonthlyCharges'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'pm10', then display summary statistics of all numeric columns using df.describe(), then clean text data in column 'no2' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf = pd.get_dummies(df, columns=['pm10'], prefix=['pm10'])\nprint(df.describe())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['no2_clean'] = df['no2'].apply(clean)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then normalize the 'sepal_length' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score, then create a new feature 'species_ratio' as the ratio of 'species' to 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['sepal_length_scaled'] = (df['sepal_length'] - df['sepal_length'].min()) / (df['sepal_length'].max() - df['sepal_length'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['species_ratio'] = df['species'] / df['species']"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'humidity' by imputing with median, then display summary statistics of all numeric columns using df.describe(), then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['humidity'].fillna(df['humidity'].median(), inplace=True)\nprint(df.describe())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'temperature', then one-hot encode the categorical column 'humidity', then create a new feature 'date_ratio' as the ratio of 'date' to 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['humidity'], prefix=['humidity'])\ndf['date_ratio'] = df['date'] / df['temperature']"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then compute TF-IDF features for column 'date' and display top 10 words, then plot a histogram of 'humidity'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date'])\nprint(vect.get_feature_names_out())\ndf['humidity'].hist()\nplt.xlabel('humidity')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then normalize the 'device_id' column using min-max scaling, then compute TF-IDF features for column 'sensor_value' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sensor_value'])\ny = df['sensor_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['device_id_scaled'] = (df['device_id'] - df['device_id'].min()) / (df['device_id'].max() - df['device_id'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sensor_value'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then one-hot encode the categorical column 'so2', then compute TF-IDF features for column 'o3' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf = pd.get_dummies(df, columns=['so2'], prefix=['so2'])\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['o3'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then plot a histogram of 'store', then create a new feature 'day_of_week_ratio' as the ratio of 'day_of_week' to 'sales', then train a Random Forest Classifier to predict 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['store'].hist()\nplt.xlabel('store')\nplt.ylabel('Frequency')\nplt.show()\ndf['day_of_week_ratio'] = df['day_of_week'] / df['sales']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then detect outliers in 'customers' using the IQR method, then one-hot encode the categorical column 'sales', then compute TF-IDF features for column 'store' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nQ1 = df['customers'].quantile(0.25)\nQ3 = df['customers'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['customers'] < (Q1 - 1.5*IQR)) | (df['customers'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf = pd.get_dummies(df, columns=['sales'], prefix=['sales'])\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['store'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then handle missing values in 'traffic_volume' by imputing with median, then detect outliers in 'date_time' using the IQR method, then clean text data in column 'temp' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['traffic_volume'].fillna(df['traffic_volume'].median(), inplace=True)\nQ1 = df['date_time'].quantile(0.25)\nQ3 = df['date_time'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date_time'] < (Q1 - 1.5*IQR)) | (df['date_time'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['temp_clean'] = df['temp'].apply(clean)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Survived'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then clean text data in column 'review' by removing punctuation and stopwords, then display feature importances from the Random Forest model, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['review_clean'] = df['review'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'sensor_value' by removing punctuation and stopwords, then train a Linear Regression model to predict 'sensor_value', then handle missing values in 'device_id' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sensor_value_clean'] = df['sensor_value'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['device_id'].fillna(df['device_id'].median(), inplace=True)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then create a new feature 'text_ratio' as the ratio of 'text' to 'text', then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['text_ratio'] = df['text'] / df['text']\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'text', then handle missing values in 'shares' by imputing with median, then train a Random Forest Classifier to predict 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['shares'].fillna(df['shares'].median(), inplace=True)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then compute TF-IDF features for column 'isFraud' and display top 10 words, then plot a histogram of 'time'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nprint(df.describe())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['isFraud'])\nprint(vect.get_feature_names_out())\ndf['time'].hist()\nplt.xlabel('time')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'patient_id', then normalize the 'ecg_reading' column using min-max scaling, then plot a histogram of 'heart_rate'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf = pd.get_dummies(df, columns=['patient_id'], prefix=['patient_id'])\ndf['ecg_reading_scaled'] = (df['ecg_reading'] - df['ecg_reading'].min()) / (df['ecg_reading'].max() - df['ecg_reading'].min())\ndf['heart_rate'].hist()\nplt.xlabel('heart_rate')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then create a new feature 'ecg_reading_ratio' as the ratio of 'ecg_reading' to 'ecg_reading', then train a Linear Regression model to predict 'ecg_reading', then train a Random Forest Classifier to predict 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['ecg_reading_ratio'] = df['ecg_reading'] / df['ecg_reading']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'species', then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'petal_length'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['petal_length'], prefix=['petal_length'])"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then clean text data in column 'Open' by removing punctuation and stopwords, then train a Random Forest Classifier to predict 'Close', then train a Linear Regression model to predict 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Open_clean'] = df['Open'].apply(clean)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then handle missing values in 'ContractType' by imputing with median, then train a Linear Regression model to predict 'Churn', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['ContractType'].fillna(df['ContractType'].median(), inplace=True)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['MonthlyCharges'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then detect outliers in 'Close' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['Close'].quantile(0.25)\nQ3 = df['Close'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Close'] < (Q1 - 1.5*IQR)) | (df['Close'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then normalize the 'post_id' column using min-max scaling, then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'likes' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['post_id_scaled'] = (df['post_id'] - df['post_id'].min()) / (df['post_id'].max() - df['post_id'].min())\nprint(df.describe())\nQ1 = df['likes'].quantile(0.25)\nQ3 = df['likes'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['likes'] < (Q1 - 1.5*IQR)) | (df['likes'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then clean text data in column 'temperature' by removing punctuation and stopwords, then create a new feature 'consumption_ratio' as the ratio of 'consumption' to 'consumption', then normalize the 'consumption' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['temperature_clean'] = df['temperature'].apply(clean)\ndf['consumption_ratio'] = df['consumption'] / df['consumption']\ndf['consumption_scaled'] = (df['consumption'] - df['consumption'].min()) / (df['consumption'].max() - df['consumption'].min())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'marital' and display top 10 words, then detect outliers in 'marital' using the IQR method, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['marital'])\nprint(vect.get_feature_names_out())\nQ1 = df['marital'].quantile(0.25)\nQ3 = df['marital'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['marital'] < (Q1 - 1.5*IQR)) | (df['marital'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then perform time-series forecasting using ARIMA to predict the next 12 periods, then one-hot encode the categorical column 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Churn'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf = pd.get_dummies(df, columns=['Churn'], prefix=['Churn'])"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then perform time-series forecasting using ARIMA to predict the next 12 periods, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['oldbalanceOrg'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then display feature importances from the Random Forest model, then clean text data in column 'consumption' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['consumption_clean'] = df['consumption'].apply(clean)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'SalePrice', then display summary statistics of all numeric columns using df.describe(), then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['MonthlyCharges'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nprint(df.describe())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'pm10', then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf = pd.get_dummies(df, columns=['pm10'], prefix=['pm10'])\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then plot a histogram of 'sepal_length', then handle missing values in 'species' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['sepal_length'].hist()\nplt.xlabel('sepal_length')\nplt.ylabel('Frequency')\nplt.show()\ndf['species'].fillna(df['species'].median(), inplace=True)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'sensor_value' using the IQR method, then train a Linear Regression model to predict 'sensor_value', then one-hot encode the categorical column 'sensor_value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nQ1 = df['sensor_value'].quantile(0.25)\nQ3 = df['sensor_value'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sensor_value'] < (Q1 - 1.5*IQR)) | (df['sensor_value'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['sensor_value'], prefix=['sensor_value'])"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then compute TF-IDF features for column 'confirmed' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['confirmed'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then clean text data in column 'day_of_week' by removing punctuation and stopwords, then detect outliers in 'day_of_week' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['day_of_week_clean'] = df['day_of_week'].apply(clean)\nQ1 = df['day_of_week'].quantile(0.25)\nQ3 = df['day_of_week'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['day_of_week'] < (Q1 - 1.5*IQR)) | (df['day_of_week'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'job', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['job'], prefix=['job'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then create a new feature 'sentiment_ratio' as the ratio of 'sentiment' to 'sentiment', then detect outliers in 'rating' using the IQR method, then compute TF-IDF features for column 'rating' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['sentiment_ratio'] = df['sentiment'] / df['sentiment']\nQ1 = df['rating'].quantile(0.25)\nQ3 = df['rating'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['rating'] < (Q1 - 1.5*IQR)) | (df['rating'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['rating'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Linear Regression model to predict 'temperature', then detect outliers in 'date' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nprint(df.describe())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then normalize the 'Sex' column using min-max scaling, then plot a histogram of 'Fare', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Sex_scaled'] = (df['Sex'] - df['Sex'].min()) / (df['Sex'].max() - df['Sex'].min())\ndf['Fare'].hist()\nplt.xlabel('Fare')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then compute TF-IDF features for column 'pm2_5' and display top 10 words, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['pm10'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['pm2_5'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then clean text data in column 'Volume' by removing punctuation and stopwords, then handle missing values in 'Close' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Close'])\ny = df['Close']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Volume_clean'] = df['Volume'].apply(clean)\ndf['Close'].fillna(df['Close'].median(), inplace=True)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'store' column using min-max scaling, then plot a histogram of 'day_of_week'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sales'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['store_scaled'] = (df['store'] - df['store'].min()) / (df['store'].max() - df['store'].min())\ndf['day_of_week'].hist()\nplt.xlabel('day_of_week')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'OverallQual' and display top 10 words, then clean text data in column 'YearBuilt' by removing punctuation and stopwords, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['OverallQual'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['YearBuilt_clean'] = df['YearBuilt'].apply(clean)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then train a Random Forest Classifier to predict 'consumption', then handle missing values in 'humidity' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['humidity'].fillna(df['humidity'].median(), inplace=True)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nprint(df.describe())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then plot a histogram of 'consumption', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['consumption'].hist()\nplt.xlabel('consumption')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'petal_length', then compute TF-IDF features for column 'petal_length' and display top 10 words, then handle missing values in 'sepal_length' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf = pd.get_dummies(df, columns=['petal_length'], prefix=['petal_length'])\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['petal_length'])\nprint(vect.get_feature_names_out())\ndf['sepal_length'].fillna(df['sepal_length'].median(), inplace=True)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then display summary statistics of all numeric columns using df.describe(), then train a Linear Regression model to predict 'pm2_5'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nprint(df.describe())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'y', then normalize the 'age' column using min-max scaling, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf = pd.get_dummies(df, columns=['y'], prefix=['y'])\ndf['age_scaled'] = (df['age'] - df['age'].min()) / (df['age'].max() - df['age'].min())\nprint(df.describe())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then normalize the 'patient_id' column using min-max scaling, then split the data into training and testing sets with an 80-20 split, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\ndf['patient_id_scaled'] = (df['patient_id'] - df['patient_id'].min()) / (df['patient_id'].max() - df['patient_id'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['ecg_reading'])\ny = df['ecg_reading']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Revenue', then clean text data in column 'Product' by removing punctuation and stopwords, then normalize the 'Revenue' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Product_clean'] = df['Product'].apply(clean)\ndf['Revenue_scaled'] = (df['Revenue'] - df['Revenue'].min()) / (df['Revenue'].max() - df['Revenue'].min())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then normalize the 'Close' column using min-max scaling, then create a new feature 'Low_ratio' as the ratio of 'Low' to 'Close', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Close_scaled'] = (df['Close'] - df['Close'].min()) / (df['Close'].max() - df['Close'].min())\ndf['Low_ratio'] = df['Low'] / df['Close']\nprint(df.describe())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'Region' by removing punctuation and stopwords, then train a Random Forest Classifier to predict 'Revenue', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Region_clean'] = df['Region'].apply(clean)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nprint(df.describe())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then detect outliers in 'o3' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nQ1 = df['o3'].quantile(0.25)\nQ3 = df['o3'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['o3'] < (Q1 - 1.5*IQR)) | (df['o3'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['so2'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'arrival_delay', then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then clean text data in column 'day_of_week' by removing punctuation and stopwords, then normalize the 'customers' column using min-max scaling, then detect outliers in 'customers' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['day_of_week_clean'] = df['day_of_week'].apply(clean)\ndf['customers_scaled'] = (df['customers'] - df['customers'].min()) / (df['customers'].max() - df['customers'].min())\nQ1 = df['customers'].quantile(0.25)\nQ3 = df['customers'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['customers'] < (Q1 - 1.5*IQR)) | (df['customers'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then normalize the 'date' column using min-max scaling, then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'date' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['date_scaled'] = (df['date'] - df['date'].min()) / (df['date'].max() - df['date'].min())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['pm10'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['date'].quantile(0.25)\nQ3 = df['date'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['date'] < (Q1 - 1.5*IQR)) | (df['date'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sensor_value', then train a Random Forest Classifier to predict 'sensor_value', then clean text data in column 'timestamp' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['timestamp_clean'] = df['timestamp'].apply(clean)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'isFraud', then create a new feature 'isFraud_ratio' as the ratio of 'isFraud' to 'isFraud', then clean text data in column 'time' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['isFraud_ratio'] = df['isFraud'] / df['isFraud']\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['time_clean'] = df['time'].apply(clean)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then clean text data in column 'newbalanceOrig' by removing punctuation and stopwords, then train a Linear Regression model to predict 'isFraud', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['newbalanceOrig_clean'] = df['newbalanceOrig'].apply(clean)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'pm2_5', then calculate the correlation matrix for numeric features, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'confirmed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then clean text data in column 'Date' by removing punctuation and stopwords, then compute TF-IDF features for column 'High' and display top 10 words, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Date_clean'] = df['Date'].apply(clean)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['High'])\nprint(vect.get_feature_names_out())\nprint(df.describe())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then one-hot encode the categorical column 'wind_speed', then clean text data in column 'humidity' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf = pd.get_dummies(df, columns=['wind_speed'], prefix=['wind_speed'])\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['humidity_clean'] = df['humidity'].apply(clean)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then plot a histogram of 'SalePrice', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['SalePrice'].hist()\nplt.xlabel('SalePrice')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then normalize the 'time' column using min-max scaling, then create a new feature 'ecg_reading_ratio' as the ratio of 'ecg_reading' to 'ecg_reading'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['time_scaled'] = (df['time'] - df['time'].min()) / (df['time'].max() - df['time'].min())\ndf['ecg_reading_ratio'] = df['ecg_reading'] / df['ecg_reading']"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Close', then compute TF-IDF features for column 'Low' and display top 10 words, then plot a histogram of 'Open'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Low'])\nprint(vect.get_feature_names_out())\ndf['Open'].hist()\nplt.xlabel('Open')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe(), then compute TF-IDF features for column 'Fare' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Fare'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then one-hot encode the categorical column 'patient_id', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['ecg_reading'])\ny = df['ecg_reading']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf = pd.get_dummies(df, columns=['patient_id'], prefix=['patient_id'])\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Random Forest Classifier to predict 'SalePrice', then clean text data in column 'OverallQual' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nprint(df.describe())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['OverallQual_clean'] = df['OverallQual'].apply(clean)"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Close', then calculate the correlation matrix for numeric features, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Open'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'confirmed', then clean text data in column 'deaths' by removing punctuation and stopwords, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['deaths_clean'] = df['deaths'].apply(clean)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['deaths'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'LotArea' and display top 10 words, then train a Linear Regression model to predict 'SalePrice', then create a new feature 'SalePrice_ratio' as the ratio of 'SalePrice' to 'SalePrice'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['LotArea'])\nprint(vect.get_feature_names_out())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['SalePrice_ratio'] = df['SalePrice'] / df['SalePrice']"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Churn', then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split, then train a Linear Regression model to predict 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Revenue'])\ny = df['Revenue']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then display summary statistics of all numeric columns using df.describe(), then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nprint(df.describe())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'snow_1h', then evaluate the model performance using RMSE and R\u00b2 score, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf = pd.get_dummies(df, columns=['snow_1h'], prefix=['snow_1h'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then detect outliers in 'job' using the IQR method, then display feature importances from the Random Forest model, then clean text data in column 'y' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nQ1 = df['job'].quantile(0.25)\nQ3 = df['job'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['job'] < (Q1 - 1.5*IQR)) | (df['job'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['y_clean'] = df['y'].apply(clean)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'review_ratio' as the ratio of 'review' to 'sentiment', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['length'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['review_ratio'] = df['review'] / df['sentiment']\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'device_id' using the IQR method, then normalize the 'timestamp' column using min-max scaling, then plot a histogram of 'timestamp'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nQ1 = df['device_id'].quantile(0.25)\nQ3 = df['device_id'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['device_id'] < (Q1 - 1.5*IQR)) | (df['device_id'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['timestamp_scaled'] = (df['timestamp'] - df['timestamp'].min()) / (df['timestamp'].max() - df['timestamp'].min())\ndf['timestamp'].hist()\nplt.xlabel('timestamp')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then train a Linear Regression model to predict 'Revenue', then plot a histogram of 'Product'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['Product'].hist()\nplt.xlabel('Product')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then calculate the correlation matrix for numeric features, then clean text data in column 'deaths' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ncorr = df.corr()\nprint(corr)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['deaths_clean'] = df['deaths'].apply(clean)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then clean text data in column 'marital' by removing punctuation and stopwords, then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'education' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['marital_clean'] = df['marital'].apply(clean)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['education'].quantile(0.25)\nQ3 = df['education'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['education'] < (Q1 - 1.5*IQR)) | (df['education'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then plot a histogram of 'age', then train a Linear Regression model to predict 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['age'].hist()\nplt.xlabel('age')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'total_amount', then detect outliers in 'quantity' using the IQR method, then clean text data in column 'transaction_id' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['quantity'].quantile(0.25)\nQ3 = df['quantity'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['quantity'] < (Q1 - 1.5*IQR)) | (df['quantity'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['transaction_id_clean'] = df['transaction_id'].apply(clean)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then detect outliers in 'consumption' using the IQR method, then compute TF-IDF features for column 'consumption' and display top 10 words, then train a Linear Regression model to predict 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nQ1 = df['consumption'].quantile(0.25)\nQ3 = df['consumption'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['consumption'] < (Q1 - 1.5*IQR)) | (df['consumption'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['consumption'])\nprint(vect.get_feature_names_out())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'wind_speed', then normalize the 'precipitation' column using min-max scaling, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf = pd.get_dummies(df, columns=['wind_speed'], prefix=['wind_speed'])\ndf['precipitation_scaled'] = (df['precipitation'] - df['precipitation'].min()) / (df['precipitation'].max() - df['precipitation'].min())\nprint(df.describe())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then normalize the 'day_of_week' column using min-max scaling, then clean text data in column 'sales' by removing punctuation and stopwords, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['day_of_week_scaled'] = (df['day_of_week'] - df['day_of_week'].min()) / (df['day_of_week'].max() - df['day_of_week'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['sales_clean'] = df['sales'].apply(clean)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'education', then evaluate the model performance using RMSE and R\u00b2 score, then create a new feature 'job_ratio' as the ratio of 'job' to 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf = pd.get_dummies(df, columns=['education'], prefix=['education'])\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['job_ratio'] = df['job'] / df['y']"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Revenue', then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Revenue'])\ny = df['Revenue']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then calculate the correlation matrix for numeric features, then plot a histogram of 'device_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nprint(df.describe())\ncorr = df.corr()\nprint(corr)\ndf['device_id'].hist()\nplt.xlabel('device_id')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'traffic_volume', then detect outliers in 'temp' using the IQR method, then train a Linear Regression model to predict 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['temp'].quantile(0.25)\nQ3 = df['temp'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['temp'] < (Q1 - 1.5*IQR)) | (df['temp'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then handle missing values in 'Pclass' by imputing with median, then normalize the 'Fare' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['Pclass'].fillna(df['Pclass'].median(), inplace=True)\ndf['Fare_scaled'] = (df['Fare'] - df['Fare'].min()) / (df['Fare'].max() - df['Fare'].min())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then clean text data in column 'humidity' by removing punctuation and stopwords, then detect outliers in 'wind_speed' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['humidity_clean'] = df['humidity'].apply(clean)\nQ1 = df['wind_speed'].quantile(0.25)\nQ3 = df['wind_speed'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['wind_speed'] < (Q1 - 1.5*IQR)) | (df['wind_speed'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then clean text data in column 'traffic_volume' by removing punctuation and stopwords, then calculate the correlation matrix for numeric features, then train a Random Forest Classifier to predict 'traffic_volume'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['traffic_volume_clean'] = df['traffic_volume'].apply(clean)\ncorr = df.corr()\nprint(corr)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then create a new feature 'status_ratio' as the ratio of 'status' to 'sensor_value', then display feature importances from the Random Forest model, then compute TF-IDF features for column 'location' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['status_ratio'] = df['status'] / df['sensor_value']\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['location'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then plot a histogram of 'likes', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['likes'].hist()\nplt.xlabel('likes')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then compute TF-IDF features for column 'customers' and display top 10 words, then create a new feature 'day_of_week_ratio' as the ratio of 'day_of_week' to 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['customers'])\nprint(vect.get_feature_names_out())\ndf['day_of_week_ratio'] = df['day_of_week'] / df['sales']"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then handle missing values in 'traffic_volume' by imputing with median, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['traffic_volume'].fillna(df['traffic_volume'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then one-hot encode the categorical column 'Revenue', then train a Random Forest Classifier to predict 'Revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nprint(df.describe())\ndf = pd.get_dummies(df, columns=['Revenue'], prefix=['Revenue'])\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then create a new feature 'Survived_ratio' as the ratio of 'Survived' to 'Survived', then train a Linear Regression model to predict 'Survived'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nprint(df.describe())\ndf['Survived_ratio'] = df['Survived'] / df['Survived']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Product', then train a Linear Regression model to predict 'Revenue', then clean text data in column 'Product' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Product'].hist()\nplt.xlabel('Product')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Product_clean'] = df['Product'].apply(clean)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'rating', then plot a histogram of 'rating', then compute TF-IDF features for column 'sentiment' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf = pd.get_dummies(df, columns=['rating'], prefix=['rating'])\ndf['rating'].hist()\nplt.xlabel('rating')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sentiment'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then create a new feature 'open_ratio' as the ratio of 'open' to 'sales', then display feature importances from the Random Forest model, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['open_ratio'] = df['open'] / df['sales']\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nprint(df.describe())"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Survived', then split the data into training and testing sets with an 80-20 split, then detect outliers in 'Pclass' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['Pclass'].quantile(0.25)\nQ3 = df['Pclass'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Pclass'] < (Q1 - 1.5*IQR)) | (df['Pclass'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'y', then detect outliers in 'age' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['age'].quantile(0.25)\nQ3 = df['age'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['age'] < (Q1 - 1.5*IQR)) | (df['age'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['job'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then detect outliers in 'Age' using the IQR method, then handle missing values in 'Sex' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nQ1 = df['Age'].quantile(0.25)\nQ3 = df['Age'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Age'] < (Q1 - 1.5*IQR)) | (df['Age'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['Sex'].fillna(df['Sex'].median(), inplace=True)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then create a new feature 'open_ratio' as the ratio of 'open' to 'sales', then split the data into training and testing sets with an 80-20 split, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['open_ratio'] = df['open'] / df['sales']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sales'])\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'temperature', then display summary statistics of all numeric columns using df.describe(), then one-hot encode the categorical column 'wind_speed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())\ndf = pd.get_dummies(df, columns=['wind_speed'], prefix=['wind_speed'])"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then normalize the 'petal_length' column using min-max scaling, then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['petal_length_scaled'] = (df['petal_length'] - df['petal_length'].min()) / (df['petal_length'].max() - df['petal_length'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then display feature importances from the Random Forest model, then normalize the 'education' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['marital'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['education_scaled'] = (df['education'] - df['education'].min()) / (df['education'].max() - df['education'].min())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then compute TF-IDF features for column 'pm10' and display top 10 words, then handle missing values in 'pm10' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['pm10'])\nprint(vect.get_feature_names_out())\ndf['pm10'].fillna(df['pm10'].median(), inplace=True)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then normalize the 'petal_width' column using min-max scaling, then perform K-Means clustering with k=3 on numeric features, then create a new feature 'species_ratio' as the ratio of 'species' to 'species'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['petal_width_scaled'] = (df['petal_width'] - df['petal_width'].min()) / (df['petal_width'].max() - df['petal_width'].min())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['species_ratio'] = df['species'] / df['species']"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'humidity', then train a Linear Regression model to predict 'consumption', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf = pd.get_dummies(df, columns=['humidity'], prefix=['humidity'])\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then create a new feature 'sepal_width_ratio' as the ratio of 'sepal_width' to 'species', then train a Linear Regression model to predict 'species', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['sepal_width_ratio'] = df['sepal_width'] / df['species']\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['petal_width'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'y', then normalize the 'education' column using min-max scaling, then clean text data in column 'job' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['education_scaled'] = (df['education'] - df['education'].min()) / (df['education'].max() - df['education'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['job_clean'] = df['job'].apply(clean)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then plot a histogram of 'Survived', then split the data into training and testing sets with an 80-20 split, then compute TF-IDF features for column 'Sex' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Survived'].hist()\nplt.xlabel('Survived')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Survived'])\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Sex'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then train a Random Forest Classifier to predict 'consumption', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then normalize the 'rating' column using min-max scaling, then plot a histogram of 'genre', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['rating_scaled'] = (df['rating'] - df['rating'].min()) / (df['rating'].max() - df['rating'].min())\ndf['genre'].hist()\nplt.xlabel('genre')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['review'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'temperature', then create a new feature 'wind_speed_ratio' as the ratio of 'wind_speed' to 'temperature', then compute TF-IDF features for column 'humidity' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['wind_speed_ratio'] = df['wind_speed'] / df['temperature']\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['humidity'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then normalize the 'Date' column using min-max scaling, then perform time-series forecasting using ARIMA to predict the next 12 periods, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\ndf['Date_scaled'] = (df['Date'] - df['Date'].min()) / (df['Date'].max() - df['Date'].min())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Open'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then clean text data in column 'Churn' by removing punctuation and stopwords, then one-hot encode the categorical column 'ContractType', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['Churn_clean'] = df['Churn'].apply(clean)\ndf = pd.get_dummies(df, columns=['ContractType'], prefix=['ContractType'])\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then detect outliers in 'o3' using the IQR method, then plot a histogram of 'so2', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nQ1 = df['o3'].quantile(0.25)\nQ3 = df['o3'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['o3'] < (Q1 - 1.5*IQR)) | (df['o3'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['so2'].hist()\nplt.xlabel('so2')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['o3'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then plot a histogram of 'species', then create a new feature 'sepal_width_ratio' as the ratio of 'sepal_width' to 'species', then handle missing values in 'sepal_length' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['species'].hist()\nplt.xlabel('species')\nplt.ylabel('Frequency')\nplt.show()\ndf['sepal_width_ratio'] = df['sepal_width'] / df['species']\ndf['sepal_length'].fillna(df['sepal_length'].median(), inplace=True)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then plot a histogram of 'humidity', then detect outliers in 'consumption' using the IQR method, then train a Random Forest Classifier to predict 'consumption'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['humidity'].hist()\nplt.xlabel('humidity')\nplt.ylabel('Frequency')\nplt.show()\nQ1 = df['consumption'].quantile(0.25)\nQ3 = df['consumption'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['consumption'] < (Q1 - 1.5*IQR)) | (df['consumption'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then clean text data in column 'education' by removing punctuation and stopwords, then train a Random Forest Classifier to predict 'y', then one-hot encode the categorical column 'age'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['education_clean'] = df['education'].apply(clean)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf = pd.get_dummies(df, columns=['age'], prefix=['age'])"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then clean text data in column 'length' by removing punctuation and stopwords, then train a Random Forest Classifier to predict 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['length_clean'] = df['length'].apply(clean)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then normalize the 'location' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['location_scaled'] = (df['location'] - df['location'].min()) / (df['location'].max() - df['location'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['timestamp'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'product_id_ratio' as the ratio of 'product_id' to 'total_amount', then normalize the 'customer_id' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['transaction_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['product_id_ratio'] = df['product_id'] / df['total_amount']\ndf['customer_id_scaled'] = (df['customer_id'] - df['customer_id'].min()) / (df['customer_id'].max() - df['customer_id'].min())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then detect outliers in 'UnitsSold' using the IQR method, then display feature importances from the Random Forest model, then plot a histogram of 'Product'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nQ1 = df['UnitsSold'].quantile(0.25)\nQ3 = df['UnitsSold'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['UnitsSold'] < (Q1 - 1.5*IQR)) | (df['UnitsSold'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['Product'].hist()\nplt.xlabel('Product')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Random Forest Classifier to predict 'consumption', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['humidity'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'likes', then normalize the 'shares' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf = pd.get_dummies(df, columns=['likes'], prefix=['likes'])\ndf['shares_scaled'] = (df['shares'] - df['shares'].min()) / (df['shares'].max() - df['shares'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then normalize the 'OverallQual' column using min-max scaling, then detect outliers in 'LotArea' using the IQR method, then clean text data in column 'YearBuilt' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['OverallQual_scaled'] = (df['OverallQual'] - df['OverallQual'].min()) / (df['OverallQual'].max() - df['OverallQual'].min())\nQ1 = df['LotArea'].quantile(0.25)\nQ3 = df['LotArea'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['LotArea'] < (Q1 - 1.5*IQR)) | (df['LotArea'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['YearBuilt_clean'] = df['YearBuilt'].apply(clean)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then create a new feature 'education_ratio' as the ratio of 'education' to 'y', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['education_ratio'] = df['education'] / df['y']\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then detect outliers in 'temperature' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nQ1 = df['temperature'].quantile(0.25)\nQ3 = df['temperature'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['temperature'] < (Q1 - 1.5*IQR)) | (df['temperature'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then create a new feature 'day_of_week_ratio' as the ratio of 'day_of_week' to 'sales', then display feature importances from the Random Forest model, then train a Random Forest Classifier to predict 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['day_of_week_ratio'] = df['day_of_week'] / df['sales']\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then compute TF-IDF features for column 'tenure' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['tenure'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'confirmed', then create a new feature 'recovered_ratio' as the ratio of 'recovered' to 'confirmed', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['recovered_ratio'] = df['recovered'] / df['confirmed']\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then clean text data in column 'newbalanceOrig' by removing punctuation and stopwords, then display summary statistics of all numeric columns using df.describe(), then compute TF-IDF features for column 'newbalanceOrig' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['newbalanceOrig_clean'] = df['newbalanceOrig'].apply(clean)\nprint(df.describe())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['newbalanceOrig'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then detect outliers in 'Open' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Linear Regression model to predict 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nQ1 = df['Open'].quantile(0.25)\nQ3 = df['Open'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Open'] < (Q1 - 1.5*IQR)) | (df['Open'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Volume'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then normalize the 'likes' column using min-max scaling, then train a Random Forest Classifier to predict 'text', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['likes_scaled'] = (df['likes'] - df['likes'].min()) / (df['likes'].max() - df['likes'].min())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'amount', then create a new feature 'newbalanceOrig_ratio' as the ratio of 'newbalanceOrig' to 'isFraud', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf = pd.get_dummies(df, columns=['amount'], prefix=['amount'])\ndf['newbalanceOrig_ratio'] = df['newbalanceOrig'] / df['isFraud']\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'quantity' column using min-max scaling, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['transaction_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['quantity_scaled'] = (df['quantity'] - df['quantity'].min()) / (df['quantity'].max() - df['quantity'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['total_amount'])\ny = df['total_amount']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'temperature', then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'date_ratio' as the ratio of 'date' to 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['date'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['date_ratio'] = df['date'] / df['temperature']"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then detect outliers in 'traffic_volume' using the IQR method, then compute TF-IDF features for column 'temp' and display top 10 words, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nQ1 = df['traffic_volume'].quantile(0.25)\nQ3 = df['traffic_volume'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['traffic_volume'] < (Q1 - 1.5*IQR)) | (df['traffic_volume'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['temp'])\nprint(vect.get_feature_names_out())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then plot a histogram of 'temperature', then split the data into training and testing sets with an 80-20 split, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\ndf['temperature'].hist()\nplt.xlabel('temperature')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['temperature'])\ny = df['temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then create a new feature 'species_ratio' as the ratio of 'species' to 'species', then one-hot encode the categorical column 'sepal_width'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['species_ratio'] = df['species'] / df['species']\ndf = pd.get_dummies(df, columns=['sepal_width'], prefix=['sepal_width'])"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'y' and display top 10 words, then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['y'])\nprint(vect.get_feature_names_out())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then create a new feature 'Region_ratio' as the ratio of 'Region' to 'Revenue', then plot a histogram of 'Date'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['Region_ratio'] = df['Region'] / df['Revenue']\ndf['Date'].hist()\nplt.xlabel('Date')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then handle missing values in 'sensor_value' by imputing with median, then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\ndf['sensor_value'].fillna(df['sensor_value'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'UnitsSold' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nprint(df.describe())\nQ1 = df['UnitsSold'].quantile(0.25)\nQ3 = df['UnitsSold'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['UnitsSold'] < (Q1 - 1.5*IQR)) | (df['UnitsSold'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then handle missing values in 'rating' by imputing with median, then train a Linear Regression model to predict 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['rating'].fillna(df['rating'].median(), inplace=True)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then display summary statistics of all numeric columns using df.describe(), then clean text data in column 'length' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nprint(df.describe())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['length_clean'] = df['length'].apply(clean)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'pressure' using the IQR method, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nprint(df.describe())\nQ1 = df['pressure'].quantile(0.25)\nQ3 = df['pressure'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['pressure'] < (Q1 - 1.5*IQR)) | (df['pressure'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then create a new feature 'text_ratio' as the ratio of 'text' to 'text', then normalize the 'text' column using min-max scaling, then one-hot encode the categorical column 'shares'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['text_ratio'] = df['text'] / df['text']\ndf['text_scaled'] = (df['text'] - df['text'].min()) / (df['text'].max() - df['text'].min())\ndf = pd.get_dummies(df, columns=['shares'], prefix=['shares'])"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features, then normalize the 'species' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)\ndf['species_scaled'] = (df['species'] - df['species'].min()) / (df['species'].max() - df['species'].min())"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then clean text data in column 'species' by removing punctuation and stopwords, then plot a histogram of 'sepal_length', then detect outliers in 'petal_width' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['species_clean'] = df['species'].apply(clean)\ndf['sepal_length'].hist()\nplt.xlabel('sepal_length')\nplt.ylabel('Frequency')\nplt.show()\nQ1 = df['petal_width'].quantile(0.25)\nQ3 = df['petal_width'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['petal_width'] < (Q1 - 1.5*IQR)) | (df['petal_width'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then normalize the 'arrival_delay' column using min-max scaling, then plot a histogram of 'flight', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['arrival_delay_scaled'] = (df['arrival_delay'] - df['arrival_delay'].min()) / (df['arrival_delay'].max() - df['arrival_delay'].min())\ndf['flight'].hist()\nplt.xlabel('flight')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['distance'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'flight_delays.csv' dataset into a Pandas DataFrame, then create a new feature 'distance_ratio' as the ratio of 'distance' to 'arrival_delay', then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'arrival_delay'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('flight_delays.csv')\ndf['distance_ratio'] = df['distance'] / df['arrival_delay']\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'amount' using the IQR method, then normalize the 'time' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nprint(df.describe())\nQ1 = df['amount'].quantile(0.25)\nQ3 = df['amount'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['amount'] < (Q1 - 1.5*IQR)) | (df['amount'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['time_scaled'] = (df['time'] - df['time'].min()) / (df['time'].max() - df['time'].min())"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then normalize the 'Pclass' column using min-max scaling, then detect outliers in 'Pclass' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['Pclass_scaled'] = (df['Pclass'] - df['Pclass'].min()) / (df['Pclass'].max() - df['Pclass'].min())\nQ1 = df['Pclass'].quantile(0.25)\nQ3 = df['Pclass'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Pclass'] < (Q1 - 1.5*IQR)) | (df['Pclass'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then split the data into training and testing sets with an 80-20 split, then plot a histogram of 'length'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['length'].hist()\nplt.xlabel('length')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then create a new feature 'rain_1h_ratio' as the ratio of 'rain_1h' to 'traffic_volume', then normalize the 'date_time' column using min-max scaling, then clean text data in column 'date_time' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['rain_1h_ratio'] = df['rain_1h'] / df['traffic_volume']\ndf['date_time_scaled'] = (df['date_time'] - df['date_time'].min()) / (df['date_time'].max() - df['date_time'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_time_clean'] = df['date_time'].apply(clean)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then detect outliers in 'no2' using the IQR method, then handle missing values in 'o3' by imputing with median, then plot a histogram of 'o3'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nQ1 = df['no2'].quantile(0.25)\nQ3 = df['no2'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['no2'] < (Q1 - 1.5*IQR)) | (df['no2'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['o3'].fillna(df['o3'].median(), inplace=True)\ndf['o3'].hist()\nplt.xlabel('o3')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then normalize the 'day_of_week' column using min-max scaling, then compute TF-IDF features for column 'sales' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nprint(df.describe())\ndf['day_of_week_scaled'] = (df['day_of_week'] - df['day_of_week'].min()) / (df['day_of_week'].max() - df['day_of_week'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['sales'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then clean text data in column 'date' by removing punctuation and stopwords, then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'Date' and display top 10 words, then train a Random Forest Classifier to predict 'Revenue', then handle missing values in 'Region' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Date'])\nprint(vect.get_feature_names_out())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['Region'].fillna(df['Region'].median(), inplace=True)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then detect outliers in 'store' using the IQR method, then train a Linear Regression model to predict 'sales', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nQ1 = df['store'].quantile(0.25)\nQ3 = df['store'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['store'] < (Q1 - 1.5*IQR)) | (df['store'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Linear Regression model to predict 'text', then compute TF-IDF features for column 'likes' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nprint(df.describe())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['likes'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then plot a histogram of 'no2', then split the data into training and testing sets with an 80-20 split, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['no2'].hist()\nplt.xlabel('no2')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then split the data into training and testing sets with an 80-20 split, then detect outliers in 'tenure' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nprint(df.describe())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['tenure'].quantile(0.25)\nQ3 = df['tenure'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['tenure'] < (Q1 - 1.5*IQR)) | (df['tenure'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then train a Random Forest Classifier to predict 'traffic_volume', then normalize the 'rain_1h' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['traffic_volume'])\ny = df['traffic_volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['rain_1h_scaled'] = (df['rain_1h'] - df['rain_1h'].min()) / (df['rain_1h'].max() - df['rain_1h'].min())"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then clean text data in column 'o3' by removing punctuation and stopwords, then split the data into training and testing sets with an 80-20 split, then detect outliers in 'so2' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['o3_clean'] = df['o3'].apply(clean)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['so2'].quantile(0.25)\nQ3 = df['so2'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['so2'] < (Q1 - 1.5*IQR)) | (df['so2'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then one-hot encode the categorical column 'sepal_length', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf = pd.get_dummies(df, columns=['sepal_length'], prefix=['sepal_length'])\nprint(df.describe())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform time-series forecasting using ARIMA to predict the next 12 periods, then clean text data in column 'date' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nprint(df.describe())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['precipitation'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then normalize the 'patient_id' column using min-max scaling, then one-hot encode the categorical column 'time'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nprint(df.describe())\ndf['patient_id_scaled'] = (df['patient_id'] - df['patient_id'].min()) / (df['patient_id'].max() - df['patient_id'].min())\ndf = pd.get_dummies(df, columns=['time'], prefix=['time'])"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'date' and display top 10 words, then calculate the correlation matrix for numeric features, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date'])\nprint(vect.get_feature_names_out())\ncorr = df.corr()\nprint(corr)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then perform K-Means clustering with k=3 on numeric features, then handle missing values in 'date' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['o3'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['date'].fillna(df['date'].median(), inplace=True)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then evaluate the model performance using RMSE and R\u00b2 score, then train a Random Forest Classifier to predict 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nprint(df.describe())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then handle missing values in 'petal_length' by imputing with median, then train a Linear Regression model to predict 'species', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['petal_length'].fillna(df['petal_length'].median(), inplace=True)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['petal_width'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then train a Random Forest Classifier to predict 'confirmed', then detect outliers in 'deaths' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nQ1 = df['deaths'].quantile(0.25)\nQ3 = df['deaths'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['deaths'] < (Q1 - 1.5*IQR)) | (df['deaths'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'humidity', then handle missing values in 'temperature' by imputing with median, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf = pd.get_dummies(df, columns=['humidity'], prefix=['humidity'])\ndf['temperature'].fillna(df['temperature'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'MonthlyCharges', then detect outliers in 'TotalCharges' using the IQR method, then train a Random Forest Classifier to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf = pd.get_dummies(df, columns=['MonthlyCharges'], prefix=['MonthlyCharges'])\nQ1 = df['TotalCharges'].quantile(0.25)\nQ3 = df['TotalCharges'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['TotalCharges'] < (Q1 - 1.5*IQR)) | (df['TotalCharges'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then detect outliers in 'no2' using the IQR method, then normalize the 'pm10' column using min-max scaling, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nQ1 = df['no2'].quantile(0.25)\nQ3 = df['no2'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['no2'] < (Q1 - 1.5*IQR)) | (df['no2'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['pm10_scaled'] = (df['pm10'] - df['pm10'].min()) / (df['pm10'].max() - df['pm10'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then normalize the 'MonthlyCharges' column using min-max scaling, then train a Linear Regression model to predict 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['MonthlyCharges_scaled'] = (df['MonthlyCharges'] - df['MonthlyCharges'].min()) / (df['MonthlyCharges'].max() - df['MonthlyCharges'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then clean text data in column 'genre' by removing punctuation and stopwords, then normalize the 'sentiment' column using min-max scaling, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['genre_clean'] = df['genre'].apply(clean)\ndf['sentiment_scaled'] = (df['sentiment'] - df['sentiment'].min()) / (df['sentiment'].max() - df['sentiment'].min())\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'sentiment', then display summary statistics of all numeric columns using df.describe(), then clean text data in column 'rating' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(df.describe())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['rating_clean'] = df['rating'].apply(clean)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then detect outliers in 'customers' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nQ1 = df['customers'].quantile(0.25)\nQ3 = df['customers'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['customers'] < (Q1 - 1.5*IQR)) | (df['customers'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then detect outliers in 'o3' using the IQR method, then normalize the 'pm2_5' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['o3'].quantile(0.25)\nQ3 = df['o3'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['o3'] < (Q1 - 1.5*IQR)) | (df['o3'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['pm2_5_scaled'] = (df['pm2_5'] - df['pm2_5'].min()) / (df['pm2_5'].max() - df['pm2_5'].min())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then detect outliers in 'likes' using the IQR method, then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['text'])\ny = df['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nQ1 = df['likes'].quantile(0.25)\nQ3 = df['likes'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['likes'] < (Q1 - 1.5*IQR)) | (df['likes'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['post_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'date_ratio' as the ratio of 'date' to 'confirmed', then handle missing values in 'country' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['confirmed'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['date_ratio'] = df['date'] / df['confirmed']\ndf['country'].fillna(df['country'].median(), inplace=True)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then create a new feature 'heart_rate_ratio' as the ratio of 'heart_rate' to 'ecg_reading', then one-hot encode the categorical column 'heart_rate'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['heart_rate_ratio'] = df['heart_rate'] / df['ecg_reading']\ndf = pd.get_dummies(df, columns=['heart_rate'], prefix=['heart_rate'])"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then normalize the 'temp' column using min-max scaling, then clean text data in column 'date_time' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['traffic_volume'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['temp_scaled'] = (df['temp'] - df['temp'].min()) / (df['temp'].max() - df['temp'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_time_clean'] = df['date_time'].apply(clean)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'temperature', then display feature importances from the Random Forest model, then normalize the 'humidity' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['humidity_scaled'] = (df['humidity'] - df['humidity'].min()) / (df['humidity'].max() - df['humidity'].min())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then handle missing values in 'Revenue' by imputing with median, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ncorr = df.corr()\nprint(corr)\ndf['Revenue'].fillna(df['Revenue'].median(), inplace=True)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Revenue'])\ny = df['Revenue']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'job' column using min-max scaling, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['job_scaled'] = (df['job'] - df['job'].min()) / (df['job'].max() - df['job'].min())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then handle missing values in 'traffic_volume' by imputing with median, then display feature importances from the Random Forest model, then plot a histogram of 'temp'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['traffic_volume'].fillna(df['traffic_volume'].median(), inplace=True)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['temp'].hist()\nplt.xlabel('temp')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then normalize the 'customers' column using min-max scaling, then train a Linear Regression model to predict 'sales', then train a Random Forest Classifier to predict 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['customers_scaled'] = (df['customers'] - df['customers'].min()) / (df['customers'].max() - df['customers'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then create a new feature 'o3_ratio' as the ratio of 'o3' to 'pm2_5', then train a Random Forest Classifier to predict 'pm2_5', then handle missing values in 'pm10' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\ndf['o3_ratio'] = df['o3'] / df['pm2_5']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['pm10'].fillna(df['pm10'].median(), inplace=True)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then plot a histogram of 'LotArea', then clean text data in column 'YearBuilt' by removing punctuation and stopwords, then normalize the 'Neighborhood' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['LotArea'].hist()\nplt.xlabel('LotArea')\nplt.ylabel('Frequency')\nplt.show()\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['YearBuilt_clean'] = df['YearBuilt'].apply(clean)\ndf['Neighborhood_scaled'] = (df['Neighborhood'] - df['Neighborhood'].min()) / (df['Neighborhood'].max() - df['Neighborhood'].min())"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'temperature' and display top 10 words, then normalize the 'humidity' column using min-max scaling, then clean text data in column 'precipitation' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['temperature'])\nprint(vect.get_feature_names_out())\ndf['humidity_scaled'] = (df['humidity'] - df['humidity'].min()) / (df['humidity'].max() - df['humidity'].min())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['precipitation_clean'] = df['precipitation'].apply(clean)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'temperature', then create a new feature 'temperature_ratio' as the ratio of 'temperature' to 'temperature', then plot a histogram of 'wind_speed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['temperature_ratio'] = df['temperature'] / df['temperature']\ndf['wind_speed'].hist()\nplt.xlabel('wind_speed')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'device_id' and display top 10 words, then perform K-Means clustering with k=3 on numeric features, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['device_id'])\nprint(vect.get_feature_names_out())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then detect outliers in 'LotArea' using the IQR method, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Neighborhood'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nQ1 = df['LotArea'].quantile(0.25)\nQ3 = df['LotArea'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['LotArea'] < (Q1 - 1.5*IQR)) | (df['LotArea'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'sensor_data.csv' dataset into a Pandas DataFrame, then clean text data in column 'timestamp' by removing punctuation and stopwords, then evaluate the model performance using RMSE and R\u00b2 score, then compute TF-IDF features for column 'status' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sensor_data.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['timestamp_clean'] = df['timestamp'].apply(clean)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['status'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'isFraud', then display summary statistics of all numeric columns using df.describe(), then create a new feature 'oldbalanceOrg_ratio' as the ratio of 'oldbalanceOrg' to 'isFraud'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nprint(df.describe())\ndf['oldbalanceOrg_ratio'] = df['oldbalanceOrg'] / df['isFraud']"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then handle missing values in 'sepal_length' by imputing with median, then normalize the 'sepal_width' column using min-max scaling, then compute TF-IDF features for column 'petal_width' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\ndf['sepal_length'].fillna(df['sepal_length'].median(), inplace=True)\ndf['sepal_width_scaled'] = (df['sepal_width'] - df['sepal_width'].min()) / (df['sepal_width'].max() - df['sepal_width'].min())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['petal_width'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'SalePrice', then plot a histogram of 'Neighborhood', then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ndf['Neighborhood'].hist()\nplt.xlabel('Neighborhood')\nplt.ylabel('Frequency')\nplt.show()\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then handle missing values in 'Fare' by imputing with median, then normalize the 'Survived' column using min-max scaling, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf['Fare'].fillna(df['Fare'].median(), inplace=True)\ndf['Survived_scaled'] = (df['Survived'] - df['Survived'].min()) / (df['Survived'].max() - df['Survived'].min())\nprint(df.describe())"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then plot a histogram of 'Age', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['Age'].hist()\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['Fare'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then plot a histogram of 'review', then detect outliers in 'review' using the IQR method, then train a Random Forest Classifier to predict 'sentiment'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['review'].hist()\nplt.xlabel('review')\nplt.ylabel('Frequency')\nplt.show()\nQ1 = df['review'].quantile(0.25)\nQ3 = df['review'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['review'] < (Q1 - 1.5*IQR)) | (df['review'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'weather_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe(), then train a Random Forest Classifier to predict 'temperature'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('weather_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then detect outliers in 'Revenue' using the IQR method, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nQ1 = df['Revenue'].quantile(0.25)\nQ3 = df['Revenue'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Revenue'] < (Q1 - 1.5*IQR)) | (df['Revenue'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'y_ratio' as the ratio of 'y' to 'y', then clean text data in column 'age' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['education'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['y_ratio'] = df['y'] / df['y']\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['age_clean'] = df['age'].apply(clean)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'likes' and display top 10 words, then detect outliers in 'likes' using the IQR method, then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['likes'])\nprint(vect.get_feature_names_out())\nQ1 = df['likes'].quantile(0.25)\nQ3 = df['likes'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['likes'] < (Q1 - 1.5*IQR)) | (df['likes'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then handle missing values in 'y' by imputing with median, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['y'].fillna(df['y'].median(), inplace=True)\nprint(df.describe())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then train a Linear Regression model to predict 'text', then perform time-series forecasting using ARIMA to predict the next 12 periods.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nprint(df.describe())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['likes'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then detect outliers in 'Fare' using the IQR method, then display feature importances from the Random Forest model, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nQ1 = df['Fare'].quantile(0.25)\nQ3 = df['Fare'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['Fare'] < (Q1 - 1.5*IQR)) | (df['Fare'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then create a new feature 'customers_ratio' as the ratio of 'customers' to 'sales', then normalize the 'customers' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['sales'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['customers_ratio'] = df['customers'] / df['sales']\ndf['customers_scaled'] = (df['customers'] - df['customers'].min()) / (df['customers'].max() - df['customers'].min())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then normalize the 'Date' column using min-max scaling, then calculate the correlation matrix for numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Date_scaled'] = (df['Date'] - df['Date'].min()) / (df['Date'].max() - df['Date'].min())\ncorr = df.corr()\nprint(corr)\nprint(df.describe())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then detect outliers in 'amount' using the IQR method, then clean text data in column 'oldbalanceOrg' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nQ1 = df['amount'].quantile(0.25)\nQ3 = df['amount'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['amount'] < (Q1 - 1.5*IQR)) | (df['amount'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['oldbalanceOrg_clean'] = df['oldbalanceOrg'].apply(clean)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then detect outliers in 'ContractType' using the IQR method, then display feature importances from the Random Forest model, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nQ1 = df['ContractType'].quantile(0.25)\nQ3 = df['ContractType'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['ContractType'] < (Q1 - 1.5*IQR)) | (df['ContractType'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then normalize the 'user_id' column using min-max scaling, then train a Linear Regression model to predict 'text', then train a Random Forest Classifier to predict 'text'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\ndf['user_id_scaled'] = (df['user_id'] - df['user_id'].min()) / (df['user_id'].max() - df['user_id'].min())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then train a Random Forest Classifier to predict 'confirmed', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then normalize the 'Revenue' column using min-max scaling, then detect outliers in 'UnitsSold' using the IQR method, then compute TF-IDF features for column 'Revenue' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\ndf['Revenue_scaled'] = (df['Revenue'] - df['Revenue'].min()) / (df['Revenue'].max() - df['Revenue'].min())\nQ1 = df['UnitsSold'].quantile(0.25)\nQ3 = df['UnitsSold'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['UnitsSold'] < (Q1 - 1.5*IQR)) | (df['UnitsSold'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Revenue'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'sales_data.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'Region' and display top 10 words, then train a Random Forest Classifier to predict 'Revenue', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales_data.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Region'])\nprint(vect.get_feature_names_out())\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then train a Random Forest Classifier to predict 'consumption', then clean text data in column 'humidity' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['consumption'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['humidity_clean'] = df['humidity'].apply(clean)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then plot a histogram of 'consumption', then one-hot encode the categorical column 'temperature', then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\ndf['consumption'].hist()\nplt.xlabel('consumption')\nplt.ylabel('Frequency')\nplt.show()\ndf = pd.get_dummies(df, columns=['temperature'], prefix=['temperature'])\nprint(df.describe())"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'Churn', then calculate the correlation matrix for numeric features, then normalize the 'MonthlyCharges' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)\ndf['MonthlyCharges_scaled'] = (df['MonthlyCharges'] - df['MonthlyCharges'].min()) / (df['MonthlyCharges'].max() - df['MonthlyCharges'].min())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'consumption', then compute TF-IDF features for column 'pressure' and display top 10 words, then clean text data in column 'date' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['pressure'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['date_clean'] = df['date'].apply(clean)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'day_of_week' and display top 10 words, then perform K-Means clustering with k=3 on numeric features, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['day_of_week'])\nprint(vect.get_feature_names_out())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then one-hot encode the categorical column 'customer_id', then perform time-series forecasting using ARIMA to predict the next 12 periods, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\ndf = pd.get_dummies(df, columns=['customer_id'], prefix=['customer_id'])\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['product_id'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform K-Means clustering with k=3 on numeric features, then compute TF-IDF features for column 'Survived' and display top 10 words.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nprint(df.describe())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['Survived'])\nprint(vect.get_feature_names_out())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'rain_1h' and display top 10 words, then detect outliers in 'snow_1h' using the IQR method, then handle missing values in 'rain_1h' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['rain_1h'])\nprint(vect.get_feature_names_out())\nQ1 = df['snow_1h'].quantile(0.25)\nQ3 = df['snow_1h'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['snow_1h'] < (Q1 - 1.5*IQR)) | (df['snow_1h'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['rain_1h'].fillna(df['rain_1h'].median(), inplace=True)"}
{"instruction": "Load the 'store_sales.csv' dataset into a Pandas DataFrame, then create a new feature 'open_ratio' as the ratio of 'open' to 'sales', then detect outliers in 'sales' using the IQR method, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('store_sales.csv')\ndf['open_ratio'] = df['open'] / df['sales']\nQ1 = df['sales'].quantile(0.25)\nQ3 = df['sales'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sales'] < (Q1 - 1.5*IQR)) | (df['sales'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then calculate the correlation matrix for numeric features, then perform K-Means clustering with k=3 on numeric features, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ncorr = df.corr()\nprint(corr)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nprint(df.describe())"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then calculate the correlation matrix for numeric features, then train a Linear Regression model to predict 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ncorr = df.corr()\nprint(corr)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then create a new feature 'SalePrice_ratio' as the ratio of 'SalePrice' to 'SalePrice', then calculate the correlation matrix for numeric features, then handle missing values in 'LotArea' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['SalePrice_ratio'] = df['SalePrice'] / df['SalePrice']\ncorr = df.corr()\nprint(corr)\ndf['LotArea'].fillna(df['LotArea'].median(), inplace=True)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'deaths' and display top 10 words, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['deaths'])\nprint(vect.get_feature_names_out())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['deaths'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then normalize the 'pm2_5' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['pm2_5_scaled'] = (df['pm2_5'] - df['pm2_5'].min()) / (df['pm2_5'].max() - df['pm2_5'].min())"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then handle missing values in 'temp' by imputing with median, then train a Random Forest Classifier to predict 'traffic_volume', then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\ndf['temp'].fillna(df['temp'].median(), inplace=True)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then create a new feature 'Pclass_ratio' as the ratio of 'Pclass' to 'Survived', then evaluate the model performance using RMSE and R\u00b2 score.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\ndf['Pclass_ratio'] = df['Pclass'] / df['Survived']\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))"}
{"instruction": "Load the 'stock_prices.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then create a new feature 'High_ratio' as the ratio of 'High' to 'Close', then train a Random Forest Classifier to predict 'Close'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('stock_prices.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['Close'])\ny = df['Close']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf['High_ratio'] = df['High'] / df['Close']\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then clean text data in column 'text' by removing punctuation and stopwords, then detect outliers in 'likes' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nprint(df.describe())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['text_clean'] = df['text'].apply(clean)\nQ1 = df['likes'].quantile(0.25)\nQ3 = df['likes'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['likes'] < (Q1 - 1.5*IQR)) | (df['likes'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then perform K-Means clustering with k=3 on numeric features, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nprint(df.describe())\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['confirmed'])\ny = df['confirmed']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'traffic_volume.csv' dataset into a Pandas DataFrame, then compute TF-IDF features for column 'date_time' and display top 10 words, then perform time-series forecasting using ARIMA to predict the next 12 periods, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('traffic_volume.csv')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['date_time'])\nprint(vect.get_feature_names_out())\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['temp'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then clean text data in column 'pm2_5' by removing punctuation and stopwords, then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['pm2_5_clean'] = df['pm2_5'].apply(clean)\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['pm2_5'])\ny = df['pm2_5']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then normalize the 'age' column using min-max scaling, then display feature importances from the Random Forest model, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['age_scaled'] = (df['age'] - df['age'].min()) / (df['age'].max() - df['age'].min())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'air_quality.csv' dataset into a Pandas DataFrame, then clean text data in column 'pm10' by removing punctuation and stopwords, then handle missing values in 'so2' by imputing with median, then calculate the correlation matrix for numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('air_quality.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['pm10_clean'] = df['pm10'].apply(clean)\ndf['so2'].fillna(df['so2'].median(), inplace=True)\ncorr = df.corr()\nprint(corr)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then display feature importances from the Random Forest model, then plot a histogram of 'pressure'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['pressure'].hist()\nplt.xlabel('pressure')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then evaluate the model performance using RMSE and R\u00b2 score, then one-hot encode the categorical column 'user_id'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf = pd.get_dummies(df, columns=['user_id'], prefix=['user_id'])"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'SalePrice', then calculate the correlation matrix for numeric features, then normalize the 'SalePrice' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ncorr = df.corr()\nprint(corr)\ndf['SalePrice_scaled'] = (df['SalePrice'] - df['SalePrice'].min()) / (df['SalePrice'].max() - df['SalePrice'].min())"}
{"instruction": "Load the 'social_media.csv' dataset into a Pandas DataFrame, then train a Linear Regression model to predict 'text', then compute TF-IDF features for column 'shares' and display top 10 words, then clean text data in column 'likes' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('social_media.csv')\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['shares'])\nprint(vect.get_feature_names_out())\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['likes_clean'] = df['likes'].apply(clean)"}
{"instruction": "Load the 'customer_churn.csv' dataset into a Pandas DataFrame, then handle missing values in 'Churn' by imputing with median, then evaluate the model performance using RMSE and R\u00b2 score, then create a new feature 'Churn_ratio' as the ratio of 'Churn' to 'Churn'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('customer_churn.csv')\ndf['Churn'].fillna(df['Churn'].median(), inplace=True)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['Churn_ratio'] = df['Churn'] / df['Churn']"}
{"instruction": "Load the 'titanic.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'Survived', then plot a histogram of 'Survived', then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['Survived'].hist()\nplt.xlabel('Survived')\nplt.ylabel('Frequency')\nplt.show()\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then plot a histogram of 'marital', then compute TF-IDF features for column 'age' and display top 10 words, then handle missing values in 'age' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\ndf['marital'].hist()\nplt.xlabel('marital')\nplt.ylabel('Frequency')\nplt.show()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['age'])\nprint(vect.get_feature_names_out())\ndf['age'].fillna(df['age'].median(), inplace=True)"}
{"instruction": "Load the 'iris.csv' dataset into a Pandas DataFrame, then split the data into training and testing sets with an 80-20 split, then evaluate the model performance using RMSE and R\u00b2 score, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('iris.csv')\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['species'])\ny = df['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\nprint(df.describe())"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'y', then normalize the 'job' column using min-max scaling, then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ndf['job_scaled'] = (df['job'] - df['job'].min()) / (df['job'].max() - df['job'].min())\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['y'])\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then detect outliers in 'age' using the IQR method, then train a Random Forest Classifier to predict 'y', then clean text data in column 'education' by removing punctuation and stopwords.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nQ1 = df['age'].quantile(0.25)\nQ3 = df['age'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['age'] < (Q1 - 1.5*IQR)) | (df['age'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['education_clean'] = df['education'].apply(clean)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then display feature importances from the Random Forest model, then clean text data in column 'heart_rate' by removing punctuation and stopwords, then plot a histogram of 'time'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['heart_rate_clean'] = df['heart_rate'].apply(clean)\ndf['time'].hist()\nplt.xlabel('time')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'covid_cases.csv' dataset into a Pandas DataFrame, then evaluate the model performance using RMSE and R\u00b2 score, then plot a histogram of 'country', then detect outliers in 'confirmed' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('covid_cases.csv')\nfrom sklearn.metrics import mean_squared_error, r2_score\ny_pred = model.predict(X_test)\nprint('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\nprint('R\u00b2:', r2_score(y_test, y_pred))\ndf['country'].hist()\nplt.xlabel('country')\nplt.ylabel('Frequency')\nplt.show()\nQ1 = df['confirmed'].quantile(0.25)\nQ3 = df['confirmed'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['confirmed'] < (Q1 - 1.5*IQR)) | (df['confirmed'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'SalePrice', then display feature importances from the Random Forest model, then normalize the 'OverallQual' column using min-max scaling.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['OverallQual_scaled'] = (df['OverallQual'] - df['OverallQual'].min()) / (df['OverallQual'].max() - df['OverallQual'].min())"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then train a Random Forest Classifier to predict 'consumption', then perform time-series forecasting using ARIMA to predict the next 12 periods, then handle missing values in 'date' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['temperature'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\ndf['date'].fillna(df['date'].median(), inplace=True)"}
{"instruction": "Load the 'ecg_data.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then detect outliers in 'time' using the IQR method, then plot a histogram of 'heart_rate'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('ecg_data.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nQ1 = df['time'].quantile(0.25)\nQ3 = df['time'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['time'] < (Q1 - 1.5*IQR)) | (df['time'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['heart_rate'].hist()\nplt.xlabel('heart_rate')\nplt.ylabel('Frequency')\nplt.show()"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then perform K-Means clustering with k=3 on numeric features, then compute TF-IDF features for column 'total_amount' and display top 10 words, then train a Linear Regression model to predict 'total_amount'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=10)\nX_tfidf = vect.fit_transform(df['total_amount'])\nprint(vect.get_feature_names_out())\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"}
{"instruction": "Load the 'energy.csv' dataset into a Pandas DataFrame, then perform time-series forecasting using ARIMA to predict the next 12 periods, then split the data into training and testing sets with an 80-20 split, then display summary statistics of all numeric columns using df.describe().", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('energy.csv')\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(df['temperature'], order=(1,1,1))\nres = model.fit()\nprint(res.forecast(12))\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['consumption'])\ny = df['consumption']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(df.describe())"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then normalize the 'oldbalanceOrg' column using min-max scaling, then detect outliers in 'newbalanceOrig' using the IQR method, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['oldbalanceOrg_scaled'] = (df['oldbalanceOrg'] - df['oldbalanceOrg'].min()) / (df['oldbalanceOrg'].max() - df['oldbalanceOrg'].min())\nQ1 = df['newbalanceOrig'].quantile(0.25)\nQ3 = df['newbalanceOrig'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['newbalanceOrig'] < (Q1 - 1.5*IQR)) | (df['newbalanceOrig'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then handle missing values in 'YearBuilt' by imputing with median, then one-hot encode the categorical column 'Neighborhood', then split the data into training and testing sets with an 80-20 split.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\ndf['YearBuilt'].fillna(df['YearBuilt'].median(), inplace=True)\ndf = pd.get_dummies(df, columns=['Neighborhood'], prefix=['Neighborhood'])\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"}
{"instruction": "Load the 'credit_card.csv' dataset into a Pandas DataFrame, then create a new feature 'isFraud_ratio' as the ratio of 'isFraud' to 'isFraud', then handle missing values in 'isFraud' by imputing with median, then perform K-Means clustering with k=3 on numeric features.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('credit_card.csv')\ndf['isFraud_ratio'] = df['isFraud'] / df['isFraud']\ndf['isFraud'].fillna(df['isFraud'].median(), inplace=True)\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = km.fit_predict(df.select_dtypes(include='number'))"}
{"instruction": "Load the 'housing_prices.csv' dataset into a Pandas DataFrame, then detect outliers in 'YearBuilt' using the IQR method, then display feature importances from the Random Forest model, then train a Random Forest Classifier to predict 'SalePrice'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('housing_prices.csv')\nQ1 = df['YearBuilt'].quantile(0.25)\nQ3 = df['YearBuilt'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['YearBuilt'] < (Q1 - 1.5*IQR)) | (df['YearBuilt'] > (Q3 + 1.5*IQR))]\nprint(outliers)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)"}
{"instruction": "Load the 'movie_reviews.csv' dataset into a Pandas DataFrame, then create a new feature 'sentiment_ratio' as the ratio of 'sentiment' to 'sentiment', then display summary statistics of all numeric columns using df.describe(), then detect outliers in 'sentiment' using the IQR method.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('movie_reviews.csv')\ndf['sentiment_ratio'] = df['sentiment'] / df['sentiment']\nprint(df.describe())\nQ1 = df['sentiment'].quantile(0.25)\nQ3 = df['sentiment'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['sentiment'] < (Q1 - 1.5*IQR)) | (df['sentiment'] > (Q3 + 1.5*IQR))]\nprint(outliers)"}
{"instruction": "Load the 'bank_marketing.csv' dataset into a Pandas DataFrame, then clean text data in column 'y' by removing punctuation and stopwords, then display feature importances from the Random Forest model, then create a new feature 'age_ratio' as the ratio of 'age' to 'y'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('bank_marketing.csv')\nimport re\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\ndef clean(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return ' '.join([w for w in text.lower().split() if w not in stop])\ndf['y_clean'] = df['y'].apply(clean)\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))\ndf['age_ratio'] = df['age'] / df['y']"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then display summary statistics of all numeric columns using df.describe(), then plot a histogram of 'customer_id', then handle missing values in 'customer_id' by imputing with median.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nprint(df.describe())\ndf['customer_id'].hist()\nplt.xlabel('customer_id')\nplt.ylabel('Frequency')\nplt.show()\ndf['customer_id'].fillna(df['customer_id'].median(), inplace=True)"}
{"instruction": "Load the 'transactions.csv' dataset into a Pandas DataFrame, then detect outliers in 'quantity' using the IQR method, then normalize the 'quantity' column using min-max scaling, then display feature importances from the Random Forest model.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('transactions.csv')\nQ1 = df['quantity'].quantile(0.25)\nQ3 = df['quantity'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = df[(df['quantity'] < (Q1 - 1.5*IQR)) | (df['quantity'] > (Q3 + 1.5*IQR))]\nprint(outliers)\ndf['quantity_scaled'] = (df['quantity'] - df['quantity'].min()) / (df['quantity'].max() - df['quantity'].min())\nimportances = clf.feature_importances_\nprint(sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True))"}
