{"instruction": "Read CSV file 'time_series.csv' into DataFrame data, drop duplicates, fill missing values in 'quantity' with median, group by 'department' and plot average 'speed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('time_series.csv')\ndata = data.drop_duplicates()\ndata['quantity'] = data['quantity'].fillna(data['quantity'].median())\navg = data.groupby('department')['speed'].mean()\navg.plot(kind='bar')\nplt.title('Average speed by department')\nplt.show()"}
{"instruction": "Filter df for rows where 'category' contains 'A', then encode 'city' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf = df[df['category'].str.contains('A', na=False)]\ndf_ohe = pd.get_dummies(df, columns=['city'])\ntrain, test = train_test_split(df_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Concatenate multiple CSV files ['time_series.csv', 'logfile.log', 'data.xlsx'] into dataset, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['time_series.csv', 'logfile.log', 'data.xlsx'])\ndataset = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndataset.columns = dataset.columns.str.lower()\nnull_frac = dataset.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndataset.drop(columns=cols_to_drop, inplace=True)\ndataset.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read Excel file 'time_series.csv' sheet 'Report', concatenate sheets into df_data, pivot on 'region' and 'score', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('time_series.csv', sheet_name=None)\ndf_data = pd.concat(sheets.values(), ignore_index=True)\npivot = df_data.pivot_table(index='region', columns='score', values='score', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['logfile.log', 'time_series.csv', 'data.csv'] into dataset, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['logfile.log', 'time_series.csv', 'data.csv'])\ndataset = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndataset.columns = dataset.columns.str.lower()\nnull_frac = dataset.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndataset.drop(columns=cols_to_drop, inplace=True)\ndataset.to_csv('clean_data.csv', index=False)"}
{"instruction": "Load time series CSV 'sales.csv', set 'date' as index, resample weekly average of 'revenue', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('sales.csv', parse_dates=['date'], index_col='date')\nweekly = data_clean['revenue'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly revenue with Anomalies')\nplt.show()"}
{"instruction": "Read CSV file 'time_series.csv' into DataFrame sales_df, drop duplicates, fill missing values in 'department' with median, group by 'department' and plot average 'quantity'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('time_series.csv')\nsales_df = sales_df.drop_duplicates()\nsales_df['department'] = sales_df['department'].fillna(sales_df['department'].median())\navg = sales_df.groupby('department')['quantity'].mean()\navg.plot(kind='bar')\nplt.title('Average quantity by department')\nplt.show()"}
{"instruction": "Parse 'sale_date' in data_clean as datetime, extract year and month, compute monthly sum of 'speed', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean['sale_date'] = pd.to_datetime(data_clean['sale_date'])\ndata_clean['year_month'] = data_clean['sale_date'].dt.to_period('M')\nmonthly = data_clean.groupby('year_month')['speed'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of speed')\nplt.show()"}
{"instruction": "Load time series CSV 'sales.csv', set 'date' as index, resample weekly average of 'weight', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('sales.csv', parse_dates=['date'], index_col='date')\nweekly = dataset['weight'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly weight with Anomalies')\nplt.show()"}
{"instruction": "Load JSON file 'data.csv' into DataFrame df2, normalize nested records, filter rows where 'weight' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndf2 = pd.read_json('data.csv')\ndf2 = pd.json_normalize(df2.to_dict(orient='records'))\ndf2 = df2[df2['weight'] > 0]\ndf2.to_parquet('clean_data.csv')"}
{"instruction": "Read log file 'time_series.csv' with custom delimiter '|', parse into dataset, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('time_series.csv', sep='|', names=['timestamp','level','message'])\ndataset['timestamp'] = pd.to_datetime(dataset['timestamp'])\nerrors = dataset[dataset['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['logfile.log', 'data.xlsx', 'time_series.csv'] into df1, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['logfile.log', 'data.xlsx', 'time_series.csv'])\ndf1 = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf1.columns = df1.columns.str.lower()\nnull_frac = df1.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf1.drop(columns=cols_to_drop, inplace=True)\ndf1.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read log file 'input.json' with custom delimiter '|', parse into df1, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('input.json', sep='|', names=['timestamp','level','message'])\ndf1['timestamp'] = pd.to_datetime(df1['timestamp'])\nerrors = df1[df1['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load time series CSV 'time_series.csv', set 'sale_date' as index, resample weekly average of 'quantity', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('time_series.csv', parse_dates=['sale_date'], index_col='sale_date')\nweekly = dataset['quantity'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly quantity with Anomalies')\nplt.show()"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into data, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\ndata['timestamp'] = pd.to_datetime(data['timestamp'])\nerrors = data[data['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['input.json', 'data.xlsx', 'time_series.csv'] into df_sales, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['input.json', 'data.xlsx', 'time_series.csv'])\ndf_sales = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf_sales.columns = df_sales.columns.str.lower()\nnull_frac = df_sales.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf_sales.drop(columns=cols_to_drop, inplace=True)\ndf_sales.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read CSV file 'logfile.log' into DataFrame df2, drop duplicates, fill missing values in 'department' with median, group by 'class' and plot average 'count'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('logfile.log')\ndf2 = df2.drop_duplicates()\ndf2['department'] = df2['department'].fillna(df2['department'].median())\navg = df2.groupby('class')['count'].mean()\navg.plot(kind='bar')\nplt.title('Average count by class')\nplt.show()"}
{"instruction": "Load JSON file 'time_series.csv' into DataFrame df2, normalize nested records, filter rows where 'date' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndf2 = pd.read_json('time_series.csv')\ndf2 = pd.json_normalize(df2.to_dict(orient='records'))\ndf2 = df2[df2['date'] > 0]\ndf2.to_parquet('results.parquet')"}
{"instruction": "Load time series CSV 'time_series.csv', set 'date' as index, resample weekly average of 'value', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('time_series.csv', parse_dates=['date'], index_col='date')\nweekly = sales_df['value'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly value with Anomalies')\nplt.show()"}
{"instruction": "Load time series CSV 'data.csv', set 'order_date' as index, resample weekly average of 'height', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('data.csv', parse_dates=['order_date'], index_col='order_date')\nweekly = df1['height'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly height with Anomalies')\nplt.show()"}
{"instruction": "Filter df_data for rows where 'type' contains 'A', then encode 'city' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_data = df_data[df_data['type'].str.contains('A', na=False)]\ndf_data_ohe = pd.get_dummies(df_data, columns=['city'])\ntrain, test = train_test_split(df_data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Filter df_data for rows where 'value' contains 'A', then encode 'group' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_data = df_data[df_data['value'].str.contains('A', na=False)]\ndf_data_ohe = pd.get_dummies(df_data, columns=['group'])\ntrain, test = train_test_split(df_data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Load JSON file 'time_series.csv' into DataFrame df_data, normalize nested records, filter rows where 'quantity' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndf_data = pd.read_json('time_series.csv')\ndf_data = pd.json_normalize(df_data.to_dict(orient='records'))\ndf_data = df_data[df_data['quantity'] > 0]\ndf_data.to_parquet('results.parquet')"}
{"instruction": "Read log file 'sales.csv' with custom delimiter '|', parse into sales_df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('sales.csv', sep='|', names=['timestamp','level','message'])\nsales_df['timestamp'] = pd.to_datetime(sales_df['timestamp'])\nerrors = sales_df[sales_df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read Excel file 'data.xlsx' sheet 'Report', concatenate sheets into df, pivot on 'category' and 'value', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.xlsx', sheet_name=None)\ndf = pd.concat(sheets.values(), ignore_index=True)\npivot = df.pivot_table(index='category', columns='value', values='value', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Filter df_data for rows where 'duration' contains 'A', then encode 'region' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_data = df_data[df_data['duration'].str.contains('A', na=False)]\ndf_data_ohe = pd.get_dummies(df_data, columns=['region'])\ntrain, test = train_test_split(df_data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Load JSON file 'data.csv' into DataFrame data_clean, normalize nested records, filter rows where 'rating' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndata_clean = pd.read_json('data.csv')\ndata_clean = pd.json_normalize(data_clean.to_dict(orient='records'))\ndata_clean = data_clean[data_clean['rating'] > 0]\ndata_clean.to_parquet('results.parquet')"}
{"instruction": "Concatenate multiple CSV files ['sales.csv', 'time_series.csv', 'data.csv'] into data, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['sales.csv', 'time_series.csv', 'data.csv'])\ndata = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata.columns = data.columns.str.lower()\nnull_frac = data.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata.drop(columns=cols_to_drop, inplace=True)\ndata.to_csv('clean_data.csv', index=False)"}
{"instruction": "Load time series CSV 'data.csv', set 'timestamp' as index, resample weekly average of 'revenue', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('data.csv', parse_dates=['timestamp'], index_col='timestamp')\nweekly = df2['revenue'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly revenue with Anomalies')\nplt.show()"}
{"instruction": "Read Excel file 'data.xlsx' sheet 'Data', concatenate sheets into data_raw, pivot on 'category' and 'value', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.xlsx', sheet_name=None)\ndata_raw = pd.concat(sheets.values(), ignore_index=True)\npivot = data_raw.pivot_table(index='category', columns='value', values='value', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read CSV file 'time_series.csv' into DataFrame df1, drop duplicates, fill missing values in 'type' with median, group by 'status' and plot average 'salary'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('time_series.csv')\ndf1 = df1.drop_duplicates()\ndf1['type'] = df1['type'].fillna(df1['type'].median())\navg = df1.groupby('status')['salary'].mean()\navg.plot(kind='bar')\nplt.title('Average salary by status')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['data.xlsx', 'time_series.csv', 'logfile.log'] into data_clean, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.xlsx', 'time_series.csv', 'logfile.log'])\ndata_clean = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata_clean.columns = data_clean.columns.str.lower()\nnull_frac = data_clean.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata_clean.drop(columns=cols_to_drop, inplace=True)\ndata_clean.to_csv('clean_data.csv', index=False)"}
{"instruction": "Concatenate multiple CSV files ['data.xlsx', 'logfile.log', 'data.csv'] into df_data, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.xlsx', 'logfile.log', 'data.csv'])\ndf_data = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf_data.columns = df_data.columns.str.lower()\nnull_frac = df_data.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf_data.drop(columns=cols_to_drop, inplace=True)\ndf_data.to_csv('clean_data.csv', index=False)"}
{"instruction": "Load JSON file 'time_series.csv' into DataFrame data_clean, normalize nested records, filter rows where 'duration' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndata_clean = pd.read_json('time_series.csv')\ndata_clean = pd.json_normalize(data_clean.to_dict(orient='records'))\ndata_clean = data_clean[data_clean['duration'] > 0]\ndata_clean.to_parquet('output.parquet')"}
{"instruction": "Concatenate multiple CSV files ['logfile.log', 'data.csv', 'time_series.csv'] into data_clean, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['logfile.log', 'data.csv', 'time_series.csv'])\ndata_clean = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata_clean.columns = data_clean.columns.str.lower()\nnull_frac = data_clean.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata_clean.drop(columns=cols_to_drop, inplace=True)\ndata_clean.to_csv('clean_data.csv', index=False)"}
{"instruction": "Load time series CSV 'data.xlsx', set 'order_date' as index, resample weekly average of 'speed', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('data.xlsx', parse_dates=['order_date'], index_col='order_date')\nweekly = data_raw['speed'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly speed with Anomalies')\nplt.show()"}
{"instruction": "Parse 'timestamp' in df as datetime, extract year and month, compute monthly sum of 'income', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\ndf['year_month'] = df['timestamp'].dt.to_period('M')\nmonthly = df.groupby('year_month')['income'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of income')\nplt.show()"}
{"instruction": "Filter dataset for rows where 'sale_date' contains 'A', then encode 'class' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndataset = dataset[dataset['sale_date'].str.contains('A', na=False)]\ndataset_ohe = pd.get_dummies(dataset, columns=['class'])\ntrain, test = train_test_split(dataset_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read Excel file 'time_series.csv' sheet 'Data', concatenate sheets into data_raw, pivot on 'type' and 'revenue', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('time_series.csv', sheet_name=None)\ndata_raw = pd.concat(sheets.values(), ignore_index=True)\npivot = data_raw.pivot_table(index='type', columns='revenue', values='revenue', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Load time series CSV 'data.xlsx', set 'timestamp' as index, resample weekly average of 'count', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('data.xlsx', parse_dates=['timestamp'], index_col='timestamp')\nweekly = df_sales['count'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly count with Anomalies')\nplt.show()"}
{"instruction": "Read Excel file 'data.xlsx' sheet 'Data', concatenate sheets into data_clean, pivot on 'type' and 'quantity', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.xlsx', sheet_name=None)\ndata_clean = pd.concat(sheets.values(), ignore_index=True)\npivot = data_clean.pivot_table(index='type', columns='quantity', values='quantity', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into data_raw, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\ndata_raw['timestamp'] = pd.to_datetime(data_raw['timestamp'])\nerrors = data_raw[data_raw['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\nerrors = df[df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Parse 'order_date' in df_sales as datetime, extract year and month, compute monthly sum of 'age', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales['order_date'] = pd.to_datetime(df_sales['order_date'])\ndf_sales['year_month'] = df_sales['order_date'].dt.to_period('M')\nmonthly = df_sales.groupby('year_month')['age'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of age')\nplt.show()"}
{"instruction": "Read log file 'data.csv' with custom delimiter '|', parse into data_clean, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('data.csv', sep='|', names=['timestamp','level','message'])\ndata_clean['timestamp'] = pd.to_datetime(data_clean['timestamp'])\nerrors = data_clean[data_clean['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read log file 'data.csv' with custom delimiter '|', parse into data_raw, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('data.csv', sep='|', names=['timestamp','level','message'])\ndata_raw['timestamp'] = pd.to_datetime(data_raw['timestamp'])\nerrors = data_raw[data_raw['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Parse 'order_date' in dataset as datetime, extract year and month, compute monthly sum of 'rating', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset['order_date'] = pd.to_datetime(dataset['order_date'])\ndataset['year_month'] = dataset['order_date'].dt.to_period('M')\nmonthly = dataset.groupby('year_month')['rating'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of rating')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['time_series.csv', 'sales.csv', 'data.xlsx'] into data_clean, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['time_series.csv', 'sales.csv', 'data.xlsx'])\ndata_clean = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata_clean.columns = data_clean.columns.str.lower()\nnull_frac = data_clean.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata_clean.drop(columns=cols_to_drop, inplace=True)\ndata_clean.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read log file 'data.csv' with custom delimiter '|', parse into df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('data.csv', sep='|', names=['timestamp','level','message'])\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\nerrors = df[df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load JSON file 'input.json' into DataFrame df, normalize nested records, filter rows where 'group' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndf = pd.read_json('input.json')\ndf = pd.json_normalize(df.to_dict(orient='records'))\ndf = df[df['group'] > 0]\ndf.to_parquet('clean_data.csv')"}
{"instruction": "Concatenate multiple CSV files ['sales.csv', 'input.json', 'logfile.log'] into df1, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['sales.csv', 'input.json', 'logfile.log'])\ndf1 = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf1.columns = df1.columns.str.lower()\nnull_frac = df1.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf1.drop(columns=cols_to_drop, inplace=True)\ndf1.to_csv('clean_data.csv', index=False)"}
{"instruction": "Filter data_clean for rows where 'height' contains 'A', then encode 'category' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata_clean = data_clean[data_clean['height'].str.contains('A', na=False)]\ndata_clean_ohe = pd.get_dummies(data_clean, columns=['category'])\ntrain, test = train_test_split(data_clean_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Load time series CSV 'time_series.csv', set 'order_date' as index, resample weekly average of 'salary', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('time_series.csv', parse_dates=['order_date'], index_col='order_date')\nweekly = data_raw['salary'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly salary with Anomalies')\nplt.show()"}
{"instruction": "Load time series CSV 'sales.csv', set 'sale_date' as index, resample weekly average of 'income', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('sales.csv', parse_dates=['sale_date'], index_col='sale_date')\nweekly = df2['income'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly income with Anomalies')\nplt.show()"}
{"instruction": "Read CSV file 'input.json' into DataFrame df_data, drop duplicates, fill missing values in 'order_date' with median, group by 'city' and plot average 'age'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('input.json')\ndf_data = df_data.drop_duplicates()\ndf_data['order_date'] = df_data['order_date'].fillna(df_data['order_date'].median())\navg = df_data.groupby('city')['age'].mean()\navg.plot(kind='bar')\nplt.title('Average age by city')\nplt.show()"}
{"instruction": "Load JSON file 'data.csv' into DataFrame df1, normalize nested records, filter rows where 'score' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndf1 = pd.read_json('data.csv')\ndf1 = pd.json_normalize(df1.to_dict(orient='records'))\ndf1 = df1[df1['score'] > 0]\ndf1.to_parquet('results.parquet')"}
{"instruction": "Load JSON file 'time_series.csv' into DataFrame data_clean, normalize nested records, filter rows where 'duration' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndata_clean = pd.read_json('time_series.csv')\ndata_clean = pd.json_normalize(data_clean.to_dict(orient='records'))\ndata_clean = data_clean[data_clean['duration'] > 0]\ndata_clean.to_parquet('output.parquet')"}
{"instruction": "Parse 'sale_date' in data_clean as datetime, extract year and month, compute monthly sum of 'duration', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean['sale_date'] = pd.to_datetime(data_clean['sale_date'])\ndata_clean['year_month'] = data_clean['sale_date'].dt.to_period('M')\nmonthly = data_clean.groupby('year_month')['duration'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of duration')\nplt.show()"}
{"instruction": "Load JSON file 'time_series.csv' into DataFrame df1, normalize nested records, filter rows where 'group' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndf1 = pd.read_json('time_series.csv')\ndf1 = pd.json_normalize(df1.to_dict(orient='records'))\ndf1 = df1[df1['group'] > 0]\ndf1.to_parquet('results.parquet')"}
{"instruction": "Concatenate multiple CSV files ['input.json', 'logfile.log', 'time_series.csv'] into df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['input.json', 'logfile.log', 'time_series.csv'])\ndf = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf.columns = df.columns.str.lower()\nnull_frac = df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf.drop(columns=cols_to_drop, inplace=True)\ndf.to_csv('clean_data.csv', index=False)"}
{"instruction": "Filter df for rows where 'speed' contains 'A', then encode 'category' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf = df[df['speed'].str.contains('A', na=False)]\ndf_ohe = pd.get_dummies(df, columns=['category'])\ntrain, test = train_test_split(df_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Load time series CSV 'input.json', set 'timestamp' as index, resample weekly average of 'rating', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('input.json', parse_dates=['timestamp'], index_col='timestamp')\nweekly = df_data['rating'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly rating with Anomalies')\nplt.show()"}
{"instruction": "Load time series CSV 'data.xlsx', set 'date' as index, resample weekly average of 'salary', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('data.xlsx', parse_dates=['date'], index_col='date')\nweekly = df_sales['salary'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly salary with Anomalies')\nplt.show()"}
{"instruction": "Read CSV file 'data.xlsx' into DataFrame df_data, drop duplicates, fill missing values in 'region' with median, group by 'department' and plot average 'quantity'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('data.xlsx')\ndf_data = df_data.drop_duplicates()\ndf_data['region'] = df_data['region'].fillna(df_data['region'].median())\navg = df_data.groupby('department')['quantity'].mean()\navg.plot(kind='bar')\nplt.title('Average quantity by department')\nplt.show()"}
{"instruction": "Filter df1 for rows where 'status' contains 'A', then encode 'category' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf1 = df1[df1['status'].str.contains('A', na=False)]\ndf1_ohe = pd.get_dummies(df1, columns=['category'])\ntrain, test = train_test_split(df1_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Concatenate multiple CSV files ['data.csv', 'sales.csv', 'data.xlsx'] into df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.csv', 'sales.csv', 'data.xlsx'])\ndf = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf.columns = df.columns.str.lower()\nnull_frac = df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf.drop(columns=cols_to_drop, inplace=True)\ndf.to_csv('clean_data.csv', index=False)"}
{"instruction": "Parse 'timestamp' in dataset as datetime, extract year and month, compute monthly sum of 'salary', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset['timestamp'] = pd.to_datetime(dataset['timestamp'])\ndataset['year_month'] = dataset['timestamp'].dt.to_period('M')\nmonthly = dataset.groupby('year_month')['salary'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of salary')\nplt.show()"}
{"instruction": "Load JSON file 'sales.csv' into DataFrame df2, normalize nested records, filter rows where 'class' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndf2 = pd.read_json('sales.csv')\ndf2 = pd.json_normalize(df2.to_dict(orient='records'))\ndf2 = df2[df2['class'] > 0]\ndf2.to_parquet('clean_data.csv')"}
{"instruction": "Load JSON file 'data.csv' into DataFrame df1, normalize nested records, filter rows where 'count' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndf1 = pd.read_json('data.csv')\ndf1 = pd.json_normalize(df1.to_dict(orient='records'))\ndf1 = df1[df1['count'] > 0]\ndf1.to_parquet('clean_data.csv')"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into df1, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\ndf1['timestamp'] = pd.to_datetime(df1['timestamp'])\nerrors = df1[df1['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read log file 'data.csv' with custom delimiter '|', parse into sales_df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('data.csv', sep='|', names=['timestamp','level','message'])\nsales_df['timestamp'] = pd.to_datetime(sales_df['timestamp'])\nerrors = sales_df[sales_df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Filter df for rows where 'type' contains 'A', then encode 'department' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf = df[df['type'].str.contains('A', na=False)]\ndf_ohe = pd.get_dummies(df, columns=['department'])\ntrain, test = train_test_split(df_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read CSV file 'time_series.csv' into DataFrame sales_df, drop duplicates, fill missing values in 'order_date' with median, group by 'region' and plot average 'price'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('time_series.csv')\nsales_df = sales_df.drop_duplicates()\nsales_df['order_date'] = sales_df['order_date'].fillna(sales_df['order_date'].median())\navg = sales_df.groupby('region')['price'].mean()\navg.plot(kind='bar')\nplt.title('Average price by region')\nplt.show()"}
{"instruction": "Read CSV file 'time_series.csv' into DataFrame data_clean, drop duplicates, fill missing values in 'speed' with median, group by 'status' and plot average 'age'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('time_series.csv')\ndata_clean = data_clean.drop_duplicates()\ndata_clean['speed'] = data_clean['speed'].fillna(data_clean['speed'].median())\navg = data_clean.groupby('status')['age'].mean()\navg.plot(kind='bar')\nplt.title('Average age by status')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['input.json', 'data.xlsx', 'time_series.csv'] into df_data, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['input.json', 'data.xlsx', 'time_series.csv'])\ndf_data = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf_data.columns = df_data.columns.str.lower()\nnull_frac = df_data.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf_data.drop(columns=cols_to_drop, inplace=True)\ndf_data.to_csv('clean_data.csv', index=False)"}
{"instruction": "Filter df_data for rows where 'group' contains 'A', then encode 'class' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_data = df_data[df_data['group'].str.contains('A', na=False)]\ndf_data_ohe = pd.get_dummies(df_data, columns=['class'])\ntrain, test = train_test_split(df_data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read CSV file 'time_series.csv' into DataFrame df2, drop duplicates, fill missing values in 'weight' with median, group by 'category' and plot average 'age'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('time_series.csv')\ndf2 = df2.drop_duplicates()\ndf2['weight'] = df2['weight'].fillna(df2['weight'].median())\navg = df2.groupby('category')['age'].mean()\navg.plot(kind='bar')\nplt.title('Average age by category')\nplt.show()"}
{"instruction": "Read log file 'input.json' with custom delimiter '|', parse into dataset, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('input.json', sep='|', names=['timestamp','level','message'])\ndataset['timestamp'] = pd.to_datetime(dataset['timestamp'])\nerrors = dataset[dataset['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read log file 'data.csv' with custom delimiter '|', parse into df_data, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('data.csv', sep='|', names=['timestamp','level','message'])\ndf_data['timestamp'] = pd.to_datetime(df_data['timestamp'])\nerrors = df_data[df_data['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load JSON file 'logfile.log' into DataFrame sales_df, normalize nested records, filter rows where 'rating' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\nsales_df = pd.read_json('logfile.log')\nsales_df = pd.json_normalize(sales_df.to_dict(orient='records'))\nsales_df = sales_df[sales_df['rating'] > 0]\nsales_df.to_parquet('results.parquet')"}
{"instruction": "Read log file 'time_series.csv' with custom delimiter '|', parse into data_raw, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('time_series.csv', sep='|', names=['timestamp','level','message'])\ndata_raw['timestamp'] = pd.to_datetime(data_raw['timestamp'])\nerrors = data_raw[data_raw['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read CSV file 'input.json' into DataFrame df1, drop duplicates, fill missing values in 'speed' with median, group by 'group' and plot average 'value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('input.json')\ndf1 = df1.drop_duplicates()\ndf1['speed'] = df1['speed'].fillna(df1['speed'].median())\navg = df1.groupby('group')['value'].mean()\navg.plot(kind='bar')\nplt.title('Average value by group')\nplt.show()"}
{"instruction": "Filter df for rows where 'rating' contains 'A', then encode 'department' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf = df[df['rating'].str.contains('A', na=False)]\ndf_ohe = pd.get_dummies(df, columns=['department'])\ntrain, test = train_test_split(df_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Load time series CSV 'time_series.csv', set 'sale_date' as index, resample weekly average of 'price', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('time_series.csv', parse_dates=['sale_date'], index_col='sale_date')\nweekly = dataset['price'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly price with Anomalies')\nplt.show()"}
{"instruction": "Load JSON file 'time_series.csv' into DataFrame df_sales, normalize nested records, filter rows where 'speed' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndf_sales = pd.read_json('time_series.csv')\ndf_sales = pd.json_normalize(df_sales.to_dict(orient='records'))\ndf_sales = df_sales[df_sales['speed'] > 0]\ndf_sales.to_parquet('clean_data.csv')"}
{"instruction": "Read CSV file 'logfile.log' into DataFrame df_sales, drop duplicates, fill missing values in 'count' with median, group by 'department' and plot average 'speed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('logfile.log')\ndf_sales = df_sales.drop_duplicates()\ndf_sales['count'] = df_sales['count'].fillna(df_sales['count'].median())\navg = df_sales.groupby('department')['speed'].mean()\navg.plot(kind='bar')\nplt.title('Average speed by department')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['time_series.csv', 'input.json', 'logfile.log'] into data_clean, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['time_series.csv', 'input.json', 'logfile.log'])\ndata_clean = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata_clean.columns = data_clean.columns.str.lower()\nnull_frac = data_clean.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata_clean.drop(columns=cols_to_drop, inplace=True)\ndata_clean.to_csv('clean_data.csv', index=False)"}
{"instruction": "Parse 'date' in df1 as datetime, extract year and month, compute monthly sum of 'weight', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1['date'] = pd.to_datetime(df1['date'])\ndf1['year_month'] = df1['date'].dt.to_period('M')\nmonthly = df1.groupby('year_month')['weight'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of weight')\nplt.show()"}
{"instruction": "Parse 'sale_date' in data_clean as datetime, extract year and month, compute monthly sum of 'speed', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean['sale_date'] = pd.to_datetime(data_clean['sale_date'])\ndata_clean['year_month'] = data_clean['sale_date'].dt.to_period('M')\nmonthly = data_clean.groupby('year_month')['speed'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of speed')\nplt.show()"}
{"instruction": "Parse 'date' in sales_df as datetime, extract year and month, compute monthly sum of 'height', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df['date'] = pd.to_datetime(sales_df['date'])\nsales_df['year_month'] = sales_df['date'].dt.to_period('M')\nmonthly = sales_df.groupby('year_month')['height'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of height')\nplt.show()"}
{"instruction": "Read log file 'data.csv' with custom delimiter '|', parse into data, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('data.csv', sep='|', names=['timestamp','level','message'])\ndata['timestamp'] = pd.to_datetime(data['timestamp'])\nerrors = data[data['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Filter data_raw for rows where 'date' contains 'A', then encode 'status' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata_raw = data_raw[data_raw['date'].str.contains('A', na=False)]\ndata_raw_ohe = pd.get_dummies(data_raw, columns=['status'])\ntrain, test = train_test_split(data_raw_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Concatenate multiple CSV files ['input.json', 'data.csv', 'logfile.log'] into sales_df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['input.json', 'data.csv', 'logfile.log'])\nsales_df = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\nsales_df.columns = sales_df.columns.str.lower()\nnull_frac = sales_df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\nsales_df.drop(columns=cols_to_drop, inplace=True)\nsales_df.to_csv('clean_data.csv', index=False)"}
{"instruction": "Load time series CSV 'logfile.log', set 'order_date' as index, resample weekly average of 'speed', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('logfile.log', parse_dates=['order_date'], index_col='order_date')\nweekly = df['speed'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly speed with Anomalies')\nplt.show()"}
{"instruction": "Load time series CSV 'sales.csv', set 'timestamp' as index, resample weekly average of 'duration', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('sales.csv', parse_dates=['timestamp'], index_col='timestamp')\nweekly = df2['duration'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly duration with Anomalies')\nplt.show()"}
{"instruction": "Load JSON file 'logfile.log' into DataFrame dataset, normalize nested records, filter rows where 'duration' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndataset = pd.read_json('logfile.log')\ndataset = pd.json_normalize(dataset.to_dict(orient='records'))\ndataset = dataset[dataset['duration'] > 0]\ndataset.to_parquet('output.parquet')"}
{"instruction": "Read CSV file 'logfile.log' into DataFrame df2, drop duplicates, fill missing values in 'class' with median, group by 'category' and plot average 'rating'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('logfile.log')\ndf2 = df2.drop_duplicates()\ndf2['class'] = df2['class'].fillna(df2['class'].median())\navg = df2.groupby('category')['rating'].mean()\navg.plot(kind='bar')\nplt.title('Average rating by category')\nplt.show()"}
{"instruction": "Load time series CSV 'logfile.log', set 'timestamp' as index, resample weekly average of 'value', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('logfile.log', parse_dates=['timestamp'], index_col='timestamp')\nweekly = df1['value'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly value with Anomalies')\nplt.show()"}
{"instruction": "Read CSV file 'time_series.csv' into DataFrame dataset, drop duplicates, fill missing values in 'rating' with median, group by 'status' and plot average 'value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('time_series.csv')\ndataset = dataset.drop_duplicates()\ndataset['rating'] = dataset['rating'].fillna(dataset['rating'].median())\navg = dataset.groupby('status')['value'].mean()\navg.plot(kind='bar')\nplt.title('Average value by status')\nplt.show()"}
{"instruction": "Parse 'sale_date' in df as datetime, extract year and month, compute monthly sum of 'rating', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf['sale_date'] = pd.to_datetime(df['sale_date'])\ndf['year_month'] = df['sale_date'].dt.to_period('M')\nmonthly = df.groupby('year_month')['rating'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of rating')\nplt.show()"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into df_data, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\ndf_data['timestamp'] = pd.to_datetime(df_data['timestamp'])\nerrors = df_data[df_data['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load JSON file 'time_series.csv' into DataFrame df1, normalize nested records, filter rows where 'sale_date' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndf1 = pd.read_json('time_series.csv')\ndf1 = pd.json_normalize(df1.to_dict(orient='records'))\ndf1 = df1[df1['sale_date'] > 0]\ndf1.to_parquet('results.parquet')"}
{"instruction": "Read log file 'input.json' with custom delimiter '|', parse into data_raw, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('input.json', sep='|', names=['timestamp','level','message'])\ndata_raw['timestamp'] = pd.to_datetime(data_raw['timestamp'])\nerrors = data_raw[data_raw['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Parse 'timestamp' in df_data as datetime, extract year and month, compute monthly sum of 'quantity', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data['timestamp'] = pd.to_datetime(df_data['timestamp'])\ndf_data['year_month'] = df_data['timestamp'].dt.to_period('M')\nmonthly = df_data.groupby('year_month')['quantity'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of quantity')\nplt.show()"}
{"instruction": "Load time series CSV 'data.csv', set 'sale_date' as index, resample weekly average of 'salary', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('data.csv', parse_dates=['sale_date'], index_col='sale_date')\nweekly = data_clean['salary'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly salary with Anomalies')\nplt.show()"}
{"instruction": "Read Excel file 'data.xlsx' sheet 'Report', concatenate sheets into df1, pivot on 'status' and 'age', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.xlsx', sheet_name=None)\ndf1 = pd.concat(sheets.values(), ignore_index=True)\npivot = df1.pivot_table(index='status', columns='age', values='age', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['time_series.csv', 'input.json', 'sales.csv'] into df2, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['time_series.csv', 'input.json', 'sales.csv'])\ndf2 = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf2.columns = df2.columns.str.lower()\nnull_frac = df2.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf2.drop(columns=cols_to_drop, inplace=True)\ndf2.to_csv('clean_data.csv', index=False)"}
{"instruction": "Parse 'date' in data_clean as datetime, extract year and month, compute monthly sum of 'revenue', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean['date'] = pd.to_datetime(data_clean['date'])\ndata_clean['year_month'] = data_clean['date'].dt.to_period('M')\nmonthly = data_clean.groupby('year_month')['revenue'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of revenue')\nplt.show()"}
{"instruction": "Read CSV file 'data.xlsx' into DataFrame sales_df, drop duplicates, fill missing values in 'city' with median, group by 'class' and plot average 'score'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('data.xlsx')\nsales_df = sales_df.drop_duplicates()\nsales_df['city'] = sales_df['city'].fillna(sales_df['city'].median())\navg = sales_df.groupby('class')['score'].mean()\navg.plot(kind='bar')\nplt.title('Average score by class')\nplt.show()"}
{"instruction": "Load JSON file 'logfile.log' into DataFrame data_clean, normalize nested records, filter rows where 'status' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndata_clean = pd.read_json('logfile.log')\ndata_clean = pd.json_normalize(data_clean.to_dict(orient='records'))\ndata_clean = data_clean[data_clean['status'] > 0]\ndata_clean.to_parquet('output.parquet')"}
{"instruction": "Read CSV file 'input.json' into DataFrame data, drop duplicates, fill missing values in 'department' with median, group by 'type' and plot average 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('input.json')\ndata = data.drop_duplicates()\ndata['department'] = data['department'].fillna(data['department'].median())\navg = data.groupby('type')['sales'].mean()\navg.plot(kind='bar')\nplt.title('Average sales by type')\nplt.show()"}
{"instruction": "Read Excel file 'logfile.log' sheet 'Report', concatenate sheets into df_sales, pivot on 'city' and 'count', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('logfile.log', sheet_name=None)\ndf_sales = pd.concat(sheets.values(), ignore_index=True)\npivot = df_sales.pivot_table(index='city', columns='count', values='count', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Parse 'timestamp' in df1 as datetime, extract year and month, compute monthly sum of 'height', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1['timestamp'] = pd.to_datetime(df1['timestamp'])\ndf1['year_month'] = df1['timestamp'].dt.to_period('M')\nmonthly = df1.groupby('year_month')['height'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of height')\nplt.show()"}
{"instruction": "Parse 'sale_date' in df_sales as datetime, extract year and month, compute monthly sum of 'age', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales['sale_date'] = pd.to_datetime(df_sales['sale_date'])\ndf_sales['year_month'] = df_sales['sale_date'].dt.to_period('M')\nmonthly = df_sales.groupby('year_month')['age'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of age')\nplt.show()"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into df_sales, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\ndf_sales['timestamp'] = pd.to_datetime(df_sales['timestamp'])\nerrors = df_sales[df_sales['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read log file 'data.csv' with custom delimiter '|', parse into df2, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('data.csv', sep='|', names=['timestamp','level','message'])\ndf2['timestamp'] = pd.to_datetime(df2['timestamp'])\nerrors = df2[df2['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read Excel file 'sales.csv' sheet 'Data', concatenate sheets into df, pivot on 'region' and 'income', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('sales.csv', sheet_name=None)\ndf = pd.concat(sheets.values(), ignore_index=True)\npivot = df.pivot_table(index='region', columns='income', values='income', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Parse 'sale_date' in sales_df as datetime, extract year and month, compute monthly sum of 'price', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df['sale_date'] = pd.to_datetime(sales_df['sale_date'])\nsales_df['year_month'] = sales_df['sale_date'].dt.to_period('M')\nmonthly = sales_df.groupby('year_month')['price'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of price')\nplt.show()"}
{"instruction": "Read Excel file 'data.xlsx' sheet 'Report', concatenate sheets into df_sales, pivot on 'status' and 'score', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.xlsx', sheet_name=None)\ndf_sales = pd.concat(sheets.values(), ignore_index=True)\npivot = df_sales.pivot_table(index='status', columns='score', values='score', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read log file 'data.csv' with custom delimiter '|', parse into df1, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('data.csv', sep='|', names=['timestamp','level','message'])\ndf1['timestamp'] = pd.to_datetime(df1['timestamp'])\nerrors = df1[df1['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read log file 'time_series.csv' with custom delimiter '|', parse into sales_df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('time_series.csv', sep='|', names=['timestamp','level','message'])\nsales_df['timestamp'] = pd.to_datetime(sales_df['timestamp'])\nerrors = sales_df[sales_df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load JSON file 'time_series.csv' into DataFrame df2, normalize nested records, filter rows where 'sale_date' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndf2 = pd.read_json('time_series.csv')\ndf2 = pd.json_normalize(df2.to_dict(orient='records'))\ndf2 = df2[df2['sale_date'] > 0]\ndf2.to_parquet('results.parquet')"}
{"instruction": "Load time series CSV 'input.json', set 'sale_date' as index, resample weekly average of 'age', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('input.json', parse_dates=['sale_date'], index_col='sale_date')\nweekly = data_clean['age'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly age with Anomalies')\nplt.show()"}
{"instruction": "Load time series CSV 'sales.csv', set 'order_date' as index, resample weekly average of 'price', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('sales.csv', parse_dates=['order_date'], index_col='order_date')\nweekly = dataset['price'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly price with Anomalies')\nplt.show()"}
{"instruction": "Read log file 'sales.csv' with custom delimiter '|', parse into df_sales, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('sales.csv', sep='|', names=['timestamp','level','message'])\ndf_sales['timestamp'] = pd.to_datetime(df_sales['timestamp'])\nerrors = df_sales[df_sales['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Filter sales_df for rows where 'category' contains 'A', then encode 'status' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nsales_df = sales_df[sales_df['category'].str.contains('A', na=False)]\nsales_df_ohe = pd.get_dummies(sales_df, columns=['status'])\ntrain, test = train_test_split(sales_df_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Concatenate multiple CSV files ['time_series.csv', 'logfile.log', 'data.csv'] into data_clean, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['time_series.csv', 'logfile.log', 'data.csv'])\ndata_clean = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata_clean.columns = data_clean.columns.str.lower()\nnull_frac = data_clean.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata_clean.drop(columns=cols_to_drop, inplace=True)\ndata_clean.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read CSV file 'data.xlsx' into DataFrame df2, drop duplicates, fill missing values in 'salary' with median, group by 'type' and plot average 'count'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('data.xlsx')\ndf2 = df2.drop_duplicates()\ndf2['salary'] = df2['salary'].fillna(df2['salary'].median())\navg = df2.groupby('type')['count'].mean()\navg.plot(kind='bar')\nplt.title('Average count by type')\nplt.show()"}
{"instruction": "Filter df2 for rows where 'date' contains 'A', then encode 'city' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf2 = df2[df2['date'].str.contains('A', na=False)]\ndf2_ohe = pd.get_dummies(df2, columns=['city'])\ntrain, test = train_test_split(df2_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read Excel file 'logfile.log' sheet 'Sheet1', concatenate sheets into data_clean, pivot on 'status' and 'rating', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('logfile.log', sheet_name=None)\ndata_clean = pd.concat(sheets.values(), ignore_index=True)\npivot = data_clean.pivot_table(index='status', columns='rating', values='rating', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Load time series CSV 'input.json', set 'date' as index, resample weekly average of 'quantity', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('input.json', parse_dates=['date'], index_col='date')\nweekly = sales_df['quantity'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly quantity with Anomalies')\nplt.show()"}
{"instruction": "Parse 'sale_date' in df1 as datetime, extract year and month, compute monthly sum of 'weight', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1['sale_date'] = pd.to_datetime(df1['sale_date'])\ndf1['year_month'] = df1['sale_date'].dt.to_period('M')\nmonthly = df1.groupby('year_month')['weight'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of weight')\nplt.show()"}
{"instruction": "Parse 'order_date' in df_data as datetime, extract year and month, compute monthly sum of 'rating', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data['order_date'] = pd.to_datetime(df_data['order_date'])\ndf_data['year_month'] = df_data['order_date'].dt.to_period('M')\nmonthly = df_data.groupby('year_month')['rating'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of rating')\nplt.show()"}
{"instruction": "Parse 'date' in df_sales as datetime, extract year and month, compute monthly sum of 'duration', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales['date'] = pd.to_datetime(df_sales['date'])\ndf_sales['year_month'] = df_sales['date'].dt.to_period('M')\nmonthly = df_sales.groupby('year_month')['duration'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of duration')\nplt.show()"}
{"instruction": "Parse 'timestamp' in df_data as datetime, extract year and month, compute monthly sum of 'age', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data['timestamp'] = pd.to_datetime(df_data['timestamp'])\ndf_data['year_month'] = df_data['timestamp'].dt.to_period('M')\nmonthly = df_data.groupby('year_month')['age'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of age')\nplt.show()"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into df1, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\ndf1['timestamp'] = pd.to_datetime(df1['timestamp'])\nerrors = df1[df1['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read log file 'input.json' with custom delimiter '|', parse into df1, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('input.json', sep='|', names=['timestamp','level','message'])\ndf1['timestamp'] = pd.to_datetime(df1['timestamp'])\nerrors = df1[df1['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['input.json', 'sales.csv', 'time_series.csv'] into data_clean, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['input.json', 'sales.csv', 'time_series.csv'])\ndata_clean = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata_clean.columns = data_clean.columns.str.lower()\nnull_frac = data_clean.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata_clean.drop(columns=cols_to_drop, inplace=True)\ndata_clean.to_csv('clean_data.csv', index=False)"}
{"instruction": "Filter data for rows where 'value' contains 'A', then encode 'department' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata = data[data['value'].str.contains('A', na=False)]\ndata_ohe = pd.get_dummies(data, columns=['department'])\ntrain, test = train_test_split(data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Load JSON file 'input.json' into DataFrame df1, normalize nested records, filter rows where 'sale_date' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndf1 = pd.read_json('input.json')\ndf1 = pd.json_normalize(df1.to_dict(orient='records'))\ndf1 = df1[df1['sale_date'] > 0]\ndf1.to_parquet('results.parquet')"}
{"instruction": "Concatenate multiple CSV files ['logfile.log', 'time_series.csv', 'input.json'] into df_sales, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['logfile.log', 'time_series.csv', 'input.json'])\ndf_sales = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf_sales.columns = df_sales.columns.str.lower()\nnull_frac = df_sales.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf_sales.drop(columns=cols_to_drop, inplace=True)\ndf_sales.to_csv('clean_data.csv', index=False)"}
{"instruction": "Filter data_raw for rows where 'class' contains 'A', then encode 'class' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata_raw = data_raw[data_raw['class'].str.contains('A', na=False)]\ndata_raw_ohe = pd.get_dummies(data_raw, columns=['class'])\ntrain, test = train_test_split(data_raw_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read CSV file 'sales.csv' into DataFrame df1, drop duplicates, fill missing values in 'income' with median, group by 'group' and plot average 'salary'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('sales.csv')\ndf1 = df1.drop_duplicates()\ndf1['income'] = df1['income'].fillna(df1['income'].median())\navg = df1.groupby('group')['salary'].mean()\navg.plot(kind='bar')\nplt.title('Average salary by group')\nplt.show()"}
{"instruction": "Filter dataset for rows where 'count' contains 'A', then encode 'department' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndataset = dataset[dataset['count'].str.contains('A', na=False)]\ndataset_ohe = pd.get_dummies(dataset, columns=['department'])\ntrain, test = train_test_split(dataset_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read log file 'logfile.log' with custom delimiter '|', parse into df2, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('logfile.log', sep='|', names=['timestamp','level','message'])\ndf2['timestamp'] = pd.to_datetime(df2['timestamp'])\nerrors = df2[df2['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into df2, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\ndf2['timestamp'] = pd.to_datetime(df2['timestamp'])\nerrors = df2[df2['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['input.json', 'time_series.csv', 'sales.csv'] into data_clean, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['input.json', 'time_series.csv', 'sales.csv'])\ndata_clean = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata_clean.columns = data_clean.columns.str.lower()\nnull_frac = data_clean.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata_clean.drop(columns=cols_to_drop, inplace=True)\ndata_clean.to_csv('clean_data.csv', index=False)"}
{"instruction": "Filter data for rows where 'revenue' contains 'A', then encode 'category' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata = data[data['revenue'].str.contains('A', na=False)]\ndata_ohe = pd.get_dummies(data, columns=['category'])\ntrain, test = train_test_split(data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read log file 'data.csv' with custom delimiter '|', parse into data, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('data.csv', sep='|', names=['timestamp','level','message'])\ndata['timestamp'] = pd.to_datetime(data['timestamp'])\nerrors = data[data['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load time series CSV 'data.xlsx', set 'timestamp' as index, resample weekly average of 'revenue', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('data.xlsx', parse_dates=['timestamp'], index_col='timestamp')\nweekly = df2['revenue'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly revenue with Anomalies')\nplt.show()"}
{"instruction": "Read CSV file 'logfile.log' into DataFrame df, drop duplicates, fill missing values in 'timestamp' with median, group by 'region' and plot average 'revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('logfile.log')\ndf = df.drop_duplicates()\ndf['timestamp'] = df['timestamp'].fillna(df['timestamp'].median())\navg = df.groupby('region')['revenue'].mean()\navg.plot(kind='bar')\nplt.title('Average revenue by region')\nplt.show()"}
{"instruction": "Read CSV file 'time_series.csv' into DataFrame df, drop duplicates, fill missing values in 'value' with median, group by 'class' and plot average 'age'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('time_series.csv')\ndf = df.drop_duplicates()\ndf['value'] = df['value'].fillna(df['value'].median())\navg = df.groupby('class')['age'].mean()\navg.plot(kind='bar')\nplt.title('Average age by class')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['sales.csv', 'data.xlsx', 'input.json'] into df_data, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['sales.csv', 'data.xlsx', 'input.json'])\ndf_data = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf_data.columns = df_data.columns.str.lower()\nnull_frac = df_data.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf_data.drop(columns=cols_to_drop, inplace=True)\ndf_data.to_csv('clean_data.csv', index=False)"}
{"instruction": "Concatenate multiple CSV files ['data.csv', 'input.json', 'logfile.log'] into df1, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.csv', 'input.json', 'logfile.log'])\ndf1 = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf1.columns = df1.columns.str.lower()\nnull_frac = df1.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf1.drop(columns=cols_to_drop, inplace=True)\ndf1.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read CSV file 'time_series.csv' into DataFrame data_clean, drop duplicates, fill missing values in 'price' with median, group by 'category' and plot average 'price'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('time_series.csv')\ndata_clean = data_clean.drop_duplicates()\ndata_clean['price'] = data_clean['price'].fillna(data_clean['price'].median())\navg = data_clean.groupby('category')['price'].mean()\navg.plot(kind='bar')\nplt.title('Average price by category')\nplt.show()"}
{"instruction": "Read Excel file 'input.json' sheet 'Data', concatenate sheets into dataset, pivot on 'group' and 'count', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('input.json', sheet_name=None)\ndataset = pd.concat(sheets.values(), ignore_index=True)\npivot = dataset.pivot_table(index='group', columns='count', values='count', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read log file 'sales.csv' with custom delimiter '|', parse into df_sales, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('sales.csv', sep='|', names=['timestamp','level','message'])\ndf_sales['timestamp'] = pd.to_datetime(df_sales['timestamp'])\nerrors = df_sales[df_sales['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read log file 'logfile.log' with custom delimiter '|', parse into df1, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('logfile.log', sep='|', names=['timestamp','level','message'])\ndf1['timestamp'] = pd.to_datetime(df1['timestamp'])\nerrors = df1[df1['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Filter df_data for rows where 'class' contains 'A', then encode 'status' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_data = df_data[df_data['class'].str.contains('A', na=False)]\ndf_data_ohe = pd.get_dummies(df_data, columns=['status'])\ntrain, test = train_test_split(df_data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Load time series CSV 'data.xlsx', set 'date' as index, resample weekly average of 'score', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('data.xlsx', parse_dates=['date'], index_col='date')\nweekly = sales_df['score'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly score with Anomalies')\nplt.show()"}
{"instruction": "Load time series CSV 'time_series.csv', set 'timestamp' as index, resample weekly average of 'count', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('time_series.csv', parse_dates=['timestamp'], index_col='timestamp')\nweekly = df_data['count'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly count with Anomalies')\nplt.show()"}
{"instruction": "Load time series CSV 'input.json', set 'order_date' as index, resample weekly average of 'rating', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('input.json', parse_dates=['order_date'], index_col='order_date')\nweekly = sales_df['rating'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly rating with Anomalies')\nplt.show()"}
{"instruction": "Filter data for rows where 'revenue' contains 'A', then encode 'department' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata = data[data['revenue'].str.contains('A', na=False)]\ndata_ohe = pd.get_dummies(data, columns=['department'])\ntrain, test = train_test_split(data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Filter df for rows where 'order_date' contains 'A', then encode 'region' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf = df[df['order_date'].str.contains('A', na=False)]\ndf_ohe = pd.get_dummies(df, columns=['region'])\ntrain, test = train_test_split(df_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Load time series CSV 'time_series.csv', set 'timestamp' as index, resample weekly average of 'weight', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('time_series.csv', parse_dates=['timestamp'], index_col='timestamp')\nweekly = data_clean['weight'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly weight with Anomalies')\nplt.show()"}
{"instruction": "Parse 'timestamp' in df_data as datetime, extract year and month, compute monthly sum of 'quantity', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data['timestamp'] = pd.to_datetime(df_data['timestamp'])\ndf_data['year_month'] = df_data['timestamp'].dt.to_period('M')\nmonthly = df_data.groupby('year_month')['quantity'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of quantity')\nplt.show()"}
{"instruction": "Filter df1 for rows where 'department' contains 'A', then encode 'group' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf1 = df1[df1['department'].str.contains('A', na=False)]\ndf1_ohe = pd.get_dummies(df1, columns=['group'])\ntrain, test = train_test_split(df1_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Load JSON file 'time_series.csv' into DataFrame data, normalize nested records, filter rows where 'salary' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndata = pd.read_json('time_series.csv')\ndata = pd.json_normalize(data.to_dict(orient='records'))\ndata = data[data['salary'] > 0]\ndata.to_parquet('output.parquet')"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into data, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\ndata['timestamp'] = pd.to_datetime(data['timestamp'])\nerrors = data[data['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load time series CSV 'sales.csv', set 'sale_date' as index, resample weekly average of 'revenue', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('sales.csv', parse_dates=['sale_date'], index_col='sale_date')\nweekly = sales_df['revenue'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly revenue with Anomalies')\nplt.show()"}
{"instruction": "Parse 'order_date' in df_sales as datetime, extract year and month, compute monthly sum of 'weight', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales['order_date'] = pd.to_datetime(df_sales['order_date'])\ndf_sales['year_month'] = df_sales['order_date'].dt.to_period('M')\nmonthly = df_sales.groupby('year_month')['weight'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of weight')\nplt.show()"}
{"instruction": "Load JSON file 'data.xlsx' into DataFrame data, normalize nested records, filter rows where 'department' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndata = pd.read_json('data.xlsx')\ndata = pd.json_normalize(data.to_dict(orient='records'))\ndata = data[data['department'] > 0]\ndata.to_parquet('results.parquet')"}
{"instruction": "Parse 'order_date' in df as datetime, extract year and month, compute monthly sum of 'value', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf['order_date'] = pd.to_datetime(df['order_date'])\ndf['year_month'] = df['order_date'].dt.to_period('M')\nmonthly = df.groupby('year_month')['value'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of value')\nplt.show()"}
{"instruction": "Filter data for rows where 'category' contains 'A', then encode 'status' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata = data[data['category'].str.contains('A', na=False)]\ndata_ohe = pd.get_dummies(data, columns=['status'])\ntrain, test = train_test_split(data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Concatenate multiple CSV files ['sales.csv', 'input.json', 'logfile.log'] into df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['sales.csv', 'input.json', 'logfile.log'])\ndf = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf.columns = df.columns.str.lower()\nnull_frac = df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf.drop(columns=cols_to_drop, inplace=True)\ndf.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read Excel file 'data.xlsx' sheet 'Sheet1', concatenate sheets into data, pivot on 'department' and 'value', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.xlsx', sheet_name=None)\ndata = pd.concat(sheets.values(), ignore_index=True)\npivot = data.pivot_table(index='department', columns='value', values='value', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read CSV file 'data.xlsx' into DataFrame df, drop duplicates, fill missing values in 'date' with median, group by 'type' and plot average 'duration'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('data.xlsx')\ndf = df.drop_duplicates()\ndf['date'] = df['date'].fillna(df['date'].median())\navg = df.groupby('type')['duration'].mean()\navg.plot(kind='bar')\nplt.title('Average duration by type')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['input.json', 'time_series.csv', 'data.xlsx'] into df_data, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['input.json', 'time_series.csv', 'data.xlsx'])\ndf_data = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf_data.columns = df_data.columns.str.lower()\nnull_frac = df_data.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf_data.drop(columns=cols_to_drop, inplace=True)\ndf_data.to_csv('clean_data.csv', index=False)"}
{"instruction": "Parse 'sale_date' in dataset as datetime, extract year and month, compute monthly sum of 'salary', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset['sale_date'] = pd.to_datetime(dataset['sale_date'])\ndataset['year_month'] = dataset['sale_date'].dt.to_period('M')\nmonthly = dataset.groupby('year_month')['salary'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of salary')\nplt.show()"}
{"instruction": "Read Excel file 'data.xlsx' sheet 'Report', concatenate sheets into data_raw, pivot on 'status' and 'speed', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.xlsx', sheet_name=None)\ndata_raw = pd.concat(sheets.values(), ignore_index=True)\npivot = data_raw.pivot_table(index='status', columns='speed', values='speed', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read Excel file 'data.xlsx' sheet 'Sheet1', concatenate sheets into df2, pivot on 'category' and 'quantity', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.xlsx', sheet_name=None)\ndf2 = pd.concat(sheets.values(), ignore_index=True)\npivot = df2.pivot_table(index='category', columns='quantity', values='quantity', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['input.json', 'data.csv', 'data.xlsx'] into df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['input.json', 'data.csv', 'data.xlsx'])\ndf = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf.columns = df.columns.str.lower()\nnull_frac = df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf.drop(columns=cols_to_drop, inplace=True)\ndf.to_csv('clean_data.csv', index=False)"}
{"instruction": "Filter dataset for rows where 'revenue' contains 'A', then encode 'group' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndataset = dataset[dataset['revenue'].str.contains('A', na=False)]\ndataset_ohe = pd.get_dummies(dataset, columns=['group'])\ntrain, test = train_test_split(dataset_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Parse 'date' in dataset as datetime, extract year and month, compute monthly sum of 'duration', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset['date'] = pd.to_datetime(dataset['date'])\ndataset['year_month'] = dataset['date'].dt.to_period('M')\nmonthly = dataset.groupby('year_month')['duration'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of duration')\nplt.show()"}
{"instruction": "Read CSV file 'logfile.log' into DataFrame df, drop duplicates, fill missing values in 'rating' with median, group by 'city' and plot average 'quantity'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('logfile.log')\ndf = df.drop_duplicates()\ndf['rating'] = df['rating'].fillna(df['rating'].median())\navg = df.groupby('city')['quantity'].mean()\navg.plot(kind='bar')\nplt.title('Average quantity by city')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['logfile.log', 'data.csv', 'input.json'] into data_raw, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['logfile.log', 'data.csv', 'input.json'])\ndata_raw = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata_raw.columns = data_raw.columns.str.lower()\nnull_frac = data_raw.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata_raw.drop(columns=cols_to_drop, inplace=True)\ndata_raw.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read CSV file 'data.xlsx' into DataFrame df2, drop duplicates, fill missing values in 'class' with median, group by 'department' and plot average 'duration'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('data.xlsx')\ndf2 = df2.drop_duplicates()\ndf2['class'] = df2['class'].fillna(df2['class'].median())\navg = df2.groupby('department')['duration'].mean()\navg.plot(kind='bar')\nplt.title('Average duration by department')\nplt.show()"}
{"instruction": "Read CSV file 'time_series.csv' into DataFrame df_data, drop duplicates, fill missing values in 'duration' with median, group by 'region' and plot average 'count'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('time_series.csv')\ndf_data = df_data.drop_duplicates()\ndf_data['duration'] = df_data['duration'].fillna(df_data['duration'].median())\navg = df_data.groupby('region')['count'].mean()\navg.plot(kind='bar')\nplt.title('Average count by region')\nplt.show()"}
{"instruction": "Read CSV file 'logfile.log' into DataFrame dataset, drop duplicates, fill missing values in 'quantity' with median, group by 'status' and plot average 'rating'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('logfile.log')\ndataset = dataset.drop_duplicates()\ndataset['quantity'] = dataset['quantity'].fillna(dataset['quantity'].median())\navg = dataset.groupby('status')['rating'].mean()\navg.plot(kind='bar')\nplt.title('Average rating by status')\nplt.show()"}
{"instruction": "Read CSV file 'sales.csv' into DataFrame df, drop duplicates, fill missing values in 'revenue' with median, group by 'category' and plot average 'rating'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales.csv')\ndf = df.drop_duplicates()\ndf['revenue'] = df['revenue'].fillna(df['revenue'].median())\navg = df.groupby('category')['rating'].mean()\navg.plot(kind='bar')\nplt.title('Average rating by category')\nplt.show()"}
{"instruction": "Read log file 'logfile.log' with custom delimiter '|', parse into data_clean, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('logfile.log', sep='|', names=['timestamp','level','message'])\ndata_clean['timestamp'] = pd.to_datetime(data_clean['timestamp'])\nerrors = data_clean[data_clean['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read Excel file 'data.xlsx' sheet 'Data', concatenate sheets into df_sales, pivot on 'status' and 'sales', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.xlsx', sheet_name=None)\ndf_sales = pd.concat(sheets.values(), ignore_index=True)\npivot = df_sales.pivot_table(index='status', columns='sales', values='sales', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Filter df for rows where 'city' contains 'A', then encode 'group' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf = df[df['city'].str.contains('A', na=False)]\ndf_ohe = pd.get_dummies(df, columns=['group'])\ntrain, test = train_test_split(df_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Parse 'sale_date' in df_sales as datetime, extract year and month, compute monthly sum of 'age', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales['sale_date'] = pd.to_datetime(df_sales['sale_date'])\ndf_sales['year_month'] = df_sales['sale_date'].dt.to_period('M')\nmonthly = df_sales.groupby('year_month')['age'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of age')\nplt.show()"}
{"instruction": "Read Excel file 'sales.csv' sheet 'Data', concatenate sheets into data_clean, pivot on 'region' and 'score', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('sales.csv', sheet_name=None)\ndata_clean = pd.concat(sheets.values(), ignore_index=True)\npivot = data_clean.pivot_table(index='region', columns='score', values='score', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read Excel file 'logfile.log' sheet 'Sheet1', concatenate sheets into df2, pivot on 'status' and 'score', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('logfile.log', sheet_name=None)\ndf2 = pd.concat(sheets.values(), ignore_index=True)\npivot = df2.pivot_table(index='status', columns='score', values='score', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Load JSON file 'data.csv' into DataFrame df1, normalize nested records, filter rows where 'score' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndf1 = pd.read_json('data.csv')\ndf1 = pd.json_normalize(df1.to_dict(orient='records'))\ndf1 = df1[df1['score'] > 0]\ndf1.to_parquet('results.parquet')"}
{"instruction": "Parse 'order_date' in sales_df as datetime, extract year and month, compute monthly sum of 'salary', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df['order_date'] = pd.to_datetime(sales_df['order_date'])\nsales_df['year_month'] = sales_df['order_date'].dt.to_period('M')\nmonthly = sales_df.groupby('year_month')['salary'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of salary')\nplt.show()"}
{"instruction": "Read log file 'input.json' with custom delimiter '|', parse into df1, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('input.json', sep='|', names=['timestamp','level','message'])\ndf1['timestamp'] = pd.to_datetime(df1['timestamp'])\nerrors = df1[df1['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['input.json', 'time_series.csv', 'data.csv'] into df_data, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['input.json', 'time_series.csv', 'data.csv'])\ndf_data = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf_data.columns = df_data.columns.str.lower()\nnull_frac = df_data.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf_data.drop(columns=cols_to_drop, inplace=True)\ndf_data.to_csv('clean_data.csv', index=False)"}
{"instruction": "Filter data_clean for rows where 'value' contains 'A', then encode 'department' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata_clean = data_clean[data_clean['value'].str.contains('A', na=False)]\ndata_clean_ohe = pd.get_dummies(data_clean, columns=['department'])\ntrain, test = train_test_split(data_clean_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read log file 'sales.csv' with custom delimiter '|', parse into df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales.csv', sep='|', names=['timestamp','level','message'])\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\nerrors = df[df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read log file 'time_series.csv' with custom delimiter '|', parse into df1, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('time_series.csv', sep='|', names=['timestamp','level','message'])\ndf1['timestamp'] = pd.to_datetime(df1['timestamp'])\nerrors = df1[df1['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read CSV file 'time_series.csv' into DataFrame data_clean, drop duplicates, fill missing values in 'region' with median, group by 'group' and plot average 'speed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('time_series.csv')\ndata_clean = data_clean.drop_duplicates()\ndata_clean['region'] = data_clean['region'].fillna(data_clean['region'].median())\navg = data_clean.groupby('group')['speed'].mean()\navg.plot(kind='bar')\nplt.title('Average speed by group')\nplt.show()"}
{"instruction": "Read CSV file 'data.xlsx' into DataFrame df2, drop duplicates, fill missing values in 'value' with median, group by 'group' and plot average 'salary'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('data.xlsx')\ndf2 = df2.drop_duplicates()\ndf2['value'] = df2['value'].fillna(df2['value'].median())\navg = df2.groupby('group')['salary'].mean()\navg.plot(kind='bar')\nplt.title('Average salary by group')\nplt.show()"}
{"instruction": "Filter data_clean for rows where 'income' contains 'A', then encode 'type' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata_clean = data_clean[data_clean['income'].str.contains('A', na=False)]\ndata_clean_ohe = pd.get_dummies(data_clean, columns=['type'])\ntrain, test = train_test_split(data_clean_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Load JSON file 'input.json' into DataFrame df1, normalize nested records, filter rows where 'city' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndf1 = pd.read_json('input.json')\ndf1 = pd.json_normalize(df1.to_dict(orient='records'))\ndf1 = df1[df1['city'] > 0]\ndf1.to_parquet('clean_data.csv')"}
{"instruction": "Load time series CSV 'input.json', set 'sale_date' as index, resample weekly average of 'duration', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('input.json', parse_dates=['sale_date'], index_col='sale_date')\nweekly = data_clean['duration'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly duration with Anomalies')\nplt.show()"}
{"instruction": "Filter data_raw for rows where 'income' contains 'A', then encode 'department' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata_raw = data_raw[data_raw['income'].str.contains('A', na=False)]\ndata_raw_ohe = pd.get_dummies(data_raw, columns=['department'])\ntrain, test = train_test_split(data_raw_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Load JSON file 'input.json' into DataFrame data_clean, normalize nested records, filter rows where 'count' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndata_clean = pd.read_json('input.json')\ndata_clean = pd.json_normalize(data_clean.to_dict(orient='records'))\ndata_clean = data_clean[data_clean['count'] > 0]\ndata_clean.to_parquet('output.parquet')"}
{"instruction": "Read CSV file 'sales.csv' into DataFrame df_sales, drop duplicates, fill missing values in 'income' with median, group by 'region' and plot average 'revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('sales.csv')\ndf_sales = df_sales.drop_duplicates()\ndf_sales['income'] = df_sales['income'].fillna(df_sales['income'].median())\navg = df_sales.groupby('region')['revenue'].mean()\navg.plot(kind='bar')\nplt.title('Average revenue by region')\nplt.show()"}
{"instruction": "Load time series CSV 'data.xlsx', set 'timestamp' as index, resample weekly average of 'value', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('data.xlsx', parse_dates=['timestamp'], index_col='timestamp')\nweekly = sales_df['value'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly value with Anomalies')\nplt.show()"}
{"instruction": "Load time series CSV 'data.csv', set 'sale_date' as index, resample weekly average of 'score', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('data.csv', parse_dates=['sale_date'], index_col='sale_date')\nweekly = data_raw['score'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly score with Anomalies')\nplt.show()"}
{"instruction": "Load time series CSV 'logfile.log', set 'order_date' as index, resample weekly average of 'height', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('logfile.log', parse_dates=['order_date'], index_col='order_date')\nweekly = data_raw['height'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly height with Anomalies')\nplt.show()"}
{"instruction": "Read CSV file 'sales.csv' into DataFrame df, drop duplicates, fill missing values in 'class' with median, group by 'status' and plot average 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales.csv')\ndf = df.drop_duplicates()\ndf['class'] = df['class'].fillna(df['class'].median())\navg = df.groupby('status')['sales'].mean()\navg.plot(kind='bar')\nplt.title('Average sales by status')\nplt.show()"}
{"instruction": "Read log file 'sales.csv' with custom delimiter '|', parse into df_data, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('sales.csv', sep='|', names=['timestamp','level','message'])\ndf_data['timestamp'] = pd.to_datetime(df_data['timestamp'])\nerrors = df_data[df_data['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load JSON file 'input.json' into DataFrame df, normalize nested records, filter rows where 'income' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndf = pd.read_json('input.json')\ndf = pd.json_normalize(df.to_dict(orient='records'))\ndf = df[df['income'] > 0]\ndf.to_parquet('output.parquet')"}
{"instruction": "Load JSON file 'input.json' into DataFrame data_raw, normalize nested records, filter rows where 'timestamp' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndata_raw = pd.read_json('input.json')\ndata_raw = pd.json_normalize(data_raw.to_dict(orient='records'))\ndata_raw = data_raw[data_raw['timestamp'] > 0]\ndata_raw.to_parquet('clean_data.csv')"}
{"instruction": "Read Excel file 'logfile.log' sheet 'Data', concatenate sheets into df2, pivot on 'type' and 'age', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('logfile.log', sheet_name=None)\ndf2 = pd.concat(sheets.values(), ignore_index=True)\npivot = df2.pivot_table(index='type', columns='age', values='age', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into df_sales, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\ndf_sales['timestamp'] = pd.to_datetime(df_sales['timestamp'])\nerrors = df_sales[df_sales['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['logfile.log', 'data.xlsx', 'data.csv'] into df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['logfile.log', 'data.xlsx', 'data.csv'])\ndf = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf.columns = df.columns.str.lower()\nnull_frac = df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf.drop(columns=cols_to_drop, inplace=True)\ndf.to_csv('clean_data.csv', index=False)"}
{"instruction": "Parse 'sale_date' in dataset as datetime, extract year and month, compute monthly sum of 'value', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset['sale_date'] = pd.to_datetime(dataset['sale_date'])\ndataset['year_month'] = dataset['sale_date'].dt.to_period('M')\nmonthly = dataset.groupby('year_month')['value'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of value')\nplt.show()"}
{"instruction": "Read log file 'logfile.log' with custom delimiter '|', parse into sales_df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('logfile.log', sep='|', names=['timestamp','level','message'])\nsales_df['timestamp'] = pd.to_datetime(sales_df['timestamp'])\nerrors = sales_df[sales_df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read Excel file 'input.json' sheet 'Sheet1', concatenate sheets into df_data, pivot on 'category' and 'age', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('input.json', sheet_name=None)\ndf_data = pd.concat(sheets.values(), ignore_index=True)\npivot = df_data.pivot_table(index='category', columns='age', values='age', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read CSV file 'logfile.log' into DataFrame sales_df, drop duplicates, fill missing values in 'region' with median, group by 'category' and plot average 'score'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('logfile.log')\nsales_df = sales_df.drop_duplicates()\nsales_df['region'] = sales_df['region'].fillna(sales_df['region'].median())\navg = sales_df.groupby('category')['score'].mean()\navg.plot(kind='bar')\nplt.title('Average score by category')\nplt.show()"}
{"instruction": "Read log file 'sales.csv' with custom delimiter '|', parse into df2, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('sales.csv', sep='|', names=['timestamp','level','message'])\ndf2['timestamp'] = pd.to_datetime(df2['timestamp'])\nerrors = df2[df2['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load time series CSV 'sales.csv', set 'sale_date' as index, resample weekly average of 'quantity', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('sales.csv', parse_dates=['sale_date'], index_col='sale_date')\nweekly = data['quantity'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly quantity with Anomalies')\nplt.show()"}
{"instruction": "Load time series CSV 'logfile.log', set 'sale_date' as index, resample weekly average of 'value', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('logfile.log', parse_dates=['sale_date'], index_col='sale_date')\nweekly = sales_df['value'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly value with Anomalies')\nplt.show()"}
{"instruction": "Filter data_clean for rows where 'income' contains 'A', then encode 'group' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata_clean = data_clean[data_clean['income'].str.contains('A', na=False)]\ndata_clean_ohe = pd.get_dummies(data_clean, columns=['group'])\ntrain, test = train_test_split(data_clean_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Load time series CSV 'input.json', set 'timestamp' as index, resample weekly average of 'count', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('input.json', parse_dates=['timestamp'], index_col='timestamp')\nweekly = df1['count'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly count with Anomalies')\nplt.show()"}
{"instruction": "Read CSV file 'data.xlsx' into DataFrame df1, drop duplicates, fill missing values in 'duration' with median, group by 'status' and plot average 'age'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('data.xlsx')\ndf1 = df1.drop_duplicates()\ndf1['duration'] = df1['duration'].fillna(df1['duration'].median())\navg = df1.groupby('status')['age'].mean()\navg.plot(kind='bar')\nplt.title('Average age by status')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['data.csv', 'logfile.log', 'sales.csv'] into df_data, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.csv', 'logfile.log', 'sales.csv'])\ndf_data = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf_data.columns = df_data.columns.str.lower()\nnull_frac = df_data.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf_data.drop(columns=cols_to_drop, inplace=True)\ndf_data.to_csv('clean_data.csv', index=False)"}
{"instruction": "Concatenate multiple CSV files ['input.json', 'data.xlsx', 'data.csv'] into data_raw, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['input.json', 'data.xlsx', 'data.csv'])\ndata_raw = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata_raw.columns = data_raw.columns.str.lower()\nnull_frac = data_raw.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata_raw.drop(columns=cols_to_drop, inplace=True)\ndata_raw.to_csv('clean_data.csv', index=False)"}
{"instruction": "Concatenate multiple CSV files ['input.json', 'time_series.csv', 'data.xlsx'] into df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['input.json', 'time_series.csv', 'data.xlsx'])\ndf = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf.columns = df.columns.str.lower()\nnull_frac = df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf.drop(columns=cols_to_drop, inplace=True)\ndf.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read Excel file 'input.json' sheet 'Data', concatenate sheets into df1, pivot on 'type' and 'price', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('input.json', sheet_name=None)\ndf1 = pd.concat(sheets.values(), ignore_index=True)\npivot = df1.pivot_table(index='type', columns='price', values='price', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read CSV file 'data.csv' into DataFrame df2, drop duplicates, fill missing values in 'height' with median, group by 'department' and plot average 'rating'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('data.csv')\ndf2 = df2.drop_duplicates()\ndf2['height'] = df2['height'].fillna(df2['height'].median())\navg = df2.groupby('department')['rating'].mean()\navg.plot(kind='bar')\nplt.title('Average rating by department')\nplt.show()"}
{"instruction": "Parse 'sale_date' in df_data as datetime, extract year and month, compute monthly sum of 'count', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data['sale_date'] = pd.to_datetime(df_data['sale_date'])\ndf_data['year_month'] = df_data['sale_date'].dt.to_period('M')\nmonthly = df_data.groupby('year_month')['count'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of count')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['logfile.log', 'time_series.csv', 'data.xlsx'] into data_raw, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['logfile.log', 'time_series.csv', 'data.xlsx'])\ndata_raw = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata_raw.columns = data_raw.columns.str.lower()\nnull_frac = data_raw.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata_raw.drop(columns=cols_to_drop, inplace=True)\ndata_raw.to_csv('clean_data.csv', index=False)"}
{"instruction": "Filter dataset for rows where 'sales' contains 'A', then encode 'type' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndataset = dataset[dataset['sales'].str.contains('A', na=False)]\ndataset_ohe = pd.get_dummies(dataset, columns=['type'])\ntrain, test = train_test_split(dataset_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Concatenate multiple CSV files ['input.json', 'logfile.log', 'sales.csv'] into df2, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['input.json', 'logfile.log', 'sales.csv'])\ndf2 = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf2.columns = df2.columns.str.lower()\nnull_frac = df2.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf2.drop(columns=cols_to_drop, inplace=True)\ndf2.to_csv('clean_data.csv', index=False)"}
{"instruction": "Concatenate multiple CSV files ['time_series.csv', 'data.csv', 'sales.csv'] into data, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['time_series.csv', 'data.csv', 'sales.csv'])\ndata = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata.columns = data.columns.str.lower()\nnull_frac = data.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata.drop(columns=cols_to_drop, inplace=True)\ndata.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read CSV file 'data.csv' into DataFrame df1, drop duplicates, fill missing values in 'category' with median, group by 'region' and plot average 'weight'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('data.csv')\ndf1 = df1.drop_duplicates()\ndf1['category'] = df1['category'].fillna(df1['category'].median())\navg = df1.groupby('region')['weight'].mean()\navg.plot(kind='bar')\nplt.title('Average weight by region')\nplt.show()"}
{"instruction": "Load time series CSV 'logfile.log', set 'timestamp' as index, resample weekly average of 'score', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('logfile.log', parse_dates=['timestamp'], index_col='timestamp')\nweekly = data['score'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly score with Anomalies')\nplt.show()"}
{"instruction": "Read CSV file 'input.json' into DataFrame df, drop duplicates, fill missing values in 'revenue' with median, group by 'region' and plot average 'weight'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('input.json')\ndf = df.drop_duplicates()\ndf['revenue'] = df['revenue'].fillna(df['revenue'].median())\navg = df.groupby('region')['weight'].mean()\navg.plot(kind='bar')\nplt.title('Average weight by region')\nplt.show()"}
{"instruction": "Parse 'sale_date' in df as datetime, extract year and month, compute monthly sum of 'duration', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf['sale_date'] = pd.to_datetime(df['sale_date'])\ndf['year_month'] = df['sale_date'].dt.to_period('M')\nmonthly = df.groupby('year_month')['duration'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of duration')\nplt.show()"}
{"instruction": "Parse 'order_date' in df as datetime, extract year and month, compute monthly sum of 'value', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf['order_date'] = pd.to_datetime(df['order_date'])\ndf['year_month'] = df['order_date'].dt.to_period('M')\nmonthly = df.groupby('year_month')['value'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of value')\nplt.show()"}
{"instruction": "Read CSV file 'data.csv' into DataFrame df, drop duplicates, fill missing values in 'speed' with median, group by 'city' and plot average 'revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('data.csv')\ndf = df.drop_duplicates()\ndf['speed'] = df['speed'].fillna(df['speed'].median())\navg = df.groupby('city')['revenue'].mean()\navg.plot(kind='bar')\nplt.title('Average revenue by city')\nplt.show()"}
{"instruction": "Parse 'timestamp' in df2 as datetime, extract year and month, compute monthly sum of 'score', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2['timestamp'] = pd.to_datetime(df2['timestamp'])\ndf2['year_month'] = df2['timestamp'].dt.to_period('M')\nmonthly = df2.groupby('year_month')['score'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of score')\nplt.show()"}
{"instruction": "Load JSON file 'data.xlsx' into DataFrame df_data, normalize nested records, filter rows where 'type' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndf_data = pd.read_json('data.xlsx')\ndf_data = pd.json_normalize(df_data.to_dict(orient='records'))\ndf_data = df_data[df_data['type'] > 0]\ndf_data.to_parquet('output.parquet')"}
{"instruction": "Load JSON file 'input.json' into DataFrame data_raw, normalize nested records, filter rows where 'status' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndata_raw = pd.read_json('input.json')\ndata_raw = pd.json_normalize(data_raw.to_dict(orient='records'))\ndata_raw = data_raw[data_raw['status'] > 0]\ndata_raw.to_parquet('clean_data.csv')"}
{"instruction": "Filter df1 for rows where 'income' contains 'A', then encode 'region' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf1 = df1[df1['income'].str.contains('A', na=False)]\ndf1_ohe = pd.get_dummies(df1, columns=['region'])\ntrain, test = train_test_split(df1_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Concatenate multiple CSV files ['input.json', 'time_series.csv', 'data.csv'] into data_raw, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['input.json', 'time_series.csv', 'data.csv'])\ndata_raw = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata_raw.columns = data_raw.columns.str.lower()\nnull_frac = data_raw.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata_raw.drop(columns=cols_to_drop, inplace=True)\ndata_raw.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read CSV file 'input.json' into DataFrame df2, drop duplicates, fill missing values in 'price' with median, group by 'class' and plot average 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('input.json')\ndf2 = df2.drop_duplicates()\ndf2['price'] = df2['price'].fillna(df2['price'].median())\navg = df2.groupby('class')['sales'].mean()\navg.plot(kind='bar')\nplt.title('Average sales by class')\nplt.show()"}
{"instruction": "Read CSV file 'input.json' into DataFrame data_raw, drop duplicates, fill missing values in 'income' with median, group by 'region' and plot average 'price'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('input.json')\ndata_raw = data_raw.drop_duplicates()\ndata_raw['income'] = data_raw['income'].fillna(data_raw['income'].median())\navg = data_raw.groupby('region')['price'].mean()\navg.plot(kind='bar')\nplt.title('Average price by region')\nplt.show()"}
{"instruction": "Read log file 'data.csv' with custom delimiter '|', parse into df2, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('data.csv', sep='|', names=['timestamp','level','message'])\ndf2['timestamp'] = pd.to_datetime(df2['timestamp'])\nerrors = df2[df2['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read CSV file 'logfile.log' into DataFrame data_raw, drop duplicates, fill missing values in 'timestamp' with median, group by 'status' and plot average 'duration'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('logfile.log')\ndata_raw = data_raw.drop_duplicates()\ndata_raw['timestamp'] = data_raw['timestamp'].fillna(data_raw['timestamp'].median())\navg = data_raw.groupby('status')['duration'].mean()\navg.plot(kind='bar')\nplt.title('Average duration by status')\nplt.show()"}
{"instruction": "Parse 'sale_date' in dataset as datetime, extract year and month, compute monthly sum of 'sales', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset['sale_date'] = pd.to_datetime(dataset['sale_date'])\ndataset['year_month'] = dataset['sale_date'].dt.to_period('M')\nmonthly = dataset.groupby('year_month')['sales'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of sales')\nplt.show()"}
{"instruction": "Read log file 'logfile.log' with custom delimiter '|', parse into data_clean, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('logfile.log', sep='|', names=['timestamp','level','message'])\ndata_clean['timestamp'] = pd.to_datetime(data_clean['timestamp'])\nerrors = data_clean[data_clean['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Filter data for rows where 'order_date' contains 'A', then encode 'group' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata = data[data['order_date'].str.contains('A', na=False)]\ndata_ohe = pd.get_dummies(data, columns=['group'])\ntrain, test = train_test_split(data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Load JSON file 'sales.csv' into DataFrame sales_df, normalize nested records, filter rows where 'city' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\nsales_df = pd.read_json('sales.csv')\nsales_df = pd.json_normalize(sales_df.to_dict(orient='records'))\nsales_df = sales_df[sales_df['city'] > 0]\nsales_df.to_parquet('results.parquet')"}
{"instruction": "Load time series CSV 'input.json', set 'date' as index, resample weekly average of 'age', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('input.json', parse_dates=['date'], index_col='date')\nweekly = df_data['age'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly age with Anomalies')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['sales.csv', 'logfile.log', 'input.json'] into df_sales, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['sales.csv', 'logfile.log', 'input.json'])\ndf_sales = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf_sales.columns = df_sales.columns.str.lower()\nnull_frac = df_sales.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf_sales.drop(columns=cols_to_drop, inplace=True)\ndf_sales.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read CSV file 'data.csv' into DataFrame data, drop duplicates, fill missing values in 'score' with median, group by 'type' and plot average 'height'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('data.csv')\ndata = data.drop_duplicates()\ndata['score'] = data['score'].fillna(data['score'].median())\navg = data.groupby('type')['height'].mean()\navg.plot(kind='bar')\nplt.title('Average height by type')\nplt.show()"}
{"instruction": "Read log file 'sales.csv' with custom delimiter '|', parse into df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales.csv', sep='|', names=['timestamp','level','message'])\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\nerrors = df[df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read Excel file 'time_series.csv' sheet 'Report', concatenate sheets into df_data, pivot on 'group' and 'rating', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('time_series.csv', sheet_name=None)\ndf_data = pd.concat(sheets.values(), ignore_index=True)\npivot = df_data.pivot_table(index='group', columns='rating', values='rating', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read Excel file 'logfile.log' sheet 'Sheet1', concatenate sheets into sales_df, pivot on 'status' and 'duration', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('logfile.log', sheet_name=None)\nsales_df = pd.concat(sheets.values(), ignore_index=True)\npivot = sales_df.pivot_table(index='status', columns='duration', values='duration', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read Excel file 'logfile.log' sheet 'Data', concatenate sheets into df, pivot on 'group' and 'count', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('logfile.log', sheet_name=None)\ndf = pd.concat(sheets.values(), ignore_index=True)\npivot = df.pivot_table(index='group', columns='count', values='count', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Parse 'sale_date' in dataset as datetime, extract year and month, compute monthly sum of 'score', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset['sale_date'] = pd.to_datetime(dataset['sale_date'])\ndataset['year_month'] = dataset['sale_date'].dt.to_period('M')\nmonthly = dataset.groupby('year_month')['score'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of score')\nplt.show()"}
{"instruction": "Read CSV file 'logfile.log' into DataFrame df_sales, drop duplicates, fill missing values in 'age' with median, group by 'city' and plot average 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('logfile.log')\ndf_sales = df_sales.drop_duplicates()\ndf_sales['age'] = df_sales['age'].fillna(df_sales['age'].median())\navg = df_sales.groupby('city')['sales'].mean()\navg.plot(kind='bar')\nplt.title('Average sales by city')\nplt.show()"}
{"instruction": "Load JSON file 'data.xlsx' into DataFrame df2, normalize nested records, filter rows where 'salary' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndf2 = pd.read_json('data.xlsx')\ndf2 = pd.json_normalize(df2.to_dict(orient='records'))\ndf2 = df2[df2['salary'] > 0]\ndf2.to_parquet('clean_data.csv')"}
{"instruction": "Concatenate multiple CSV files ['data.xlsx', 'sales.csv', 'data.csv'] into df2, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.xlsx', 'sales.csv', 'data.csv'])\ndf2 = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf2.columns = df2.columns.str.lower()\nnull_frac = df2.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf2.drop(columns=cols_to_drop, inplace=True)\ndf2.to_csv('clean_data.csv', index=False)"}
{"instruction": "Load time series CSV 'input.json', set 'date' as index, resample weekly average of 'weight', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('input.json', parse_dates=['date'], index_col='date')\nweekly = data_clean['weight'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly weight with Anomalies')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['input.json', 'logfile.log', 'data.csv'] into df1, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['input.json', 'logfile.log', 'data.csv'])\ndf1 = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf1.columns = df1.columns.str.lower()\nnull_frac = df1.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf1.drop(columns=cols_to_drop, inplace=True)\ndf1.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read Excel file 'data.csv' sheet 'Data', concatenate sheets into df2, pivot on 'city' and 'sales', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.csv', sheet_name=None)\ndf2 = pd.concat(sheets.values(), ignore_index=True)\npivot = df2.pivot_table(index='city', columns='sales', values='sales', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read log file 'input.json' with custom delimiter '|', parse into data_raw, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('input.json', sep='|', names=['timestamp','level','message'])\ndata_raw['timestamp'] = pd.to_datetime(data_raw['timestamp'])\nerrors = data_raw[data_raw['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Parse 'date' in df_data as datetime, extract year and month, compute monthly sum of 'rating', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data['date'] = pd.to_datetime(df_data['date'])\ndf_data['year_month'] = df_data['date'].dt.to_period('M')\nmonthly = df_data.groupby('year_month')['rating'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of rating')\nplt.show()"}
{"instruction": "Load JSON file 'data.xlsx' into DataFrame dataset, normalize nested records, filter rows where 'count' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndataset = pd.read_json('data.xlsx')\ndataset = pd.json_normalize(dataset.to_dict(orient='records'))\ndataset = dataset[dataset['count'] > 0]\ndataset.to_parquet('clean_data.csv')"}
{"instruction": "Parse 'timestamp' in data_raw as datetime, extract year and month, compute monthly sum of 'age', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw['timestamp'] = pd.to_datetime(data_raw['timestamp'])\ndata_raw['year_month'] = data_raw['timestamp'].dt.to_period('M')\nmonthly = data_raw.groupby('year_month')['age'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of age')\nplt.show()"}
{"instruction": "Load time series CSV 'input.json', set 'date' as index, resample weekly average of 'count', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('input.json', parse_dates=['date'], index_col='date')\nweekly = data_raw['count'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly count with Anomalies')\nplt.show()"}
{"instruction": "Parse 'sale_date' in data_raw as datetime, extract year and month, compute monthly sum of 'income', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw['sale_date'] = pd.to_datetime(data_raw['sale_date'])\ndata_raw['year_month'] = data_raw['sale_date'].dt.to_period('M')\nmonthly = data_raw.groupby('year_month')['income'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of income')\nplt.show()"}
{"instruction": "Read Excel file 'sales.csv' sheet 'Data', concatenate sheets into df_data, pivot on 'type' and 'value', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('sales.csv', sheet_name=None)\ndf_data = pd.concat(sheets.values(), ignore_index=True)\npivot = df_data.pivot_table(index='type', columns='value', values='value', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read log file 'sales.csv' with custom delimiter '|', parse into df2, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('sales.csv', sep='|', names=['timestamp','level','message'])\ndf2['timestamp'] = pd.to_datetime(df2['timestamp'])\nerrors = df2[df2['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read Excel file 'time_series.csv' sheet 'Report', concatenate sheets into sales_df, pivot on 'type' and 'revenue', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('time_series.csv', sheet_name=None)\nsales_df = pd.concat(sheets.values(), ignore_index=True)\npivot = sales_df.pivot_table(index='type', columns='revenue', values='revenue', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Load JSON file 'input.json' into DataFrame df_sales, normalize nested records, filter rows where 'category' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndf_sales = pd.read_json('input.json')\ndf_sales = pd.json_normalize(df_sales.to_dict(orient='records'))\ndf_sales = df_sales[df_sales['category'] > 0]\ndf_sales.to_parquet('output.parquet')"}
{"instruction": "Parse 'sale_date' in df1 as datetime, extract year and month, compute monthly sum of 'revenue', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1['sale_date'] = pd.to_datetime(df1['sale_date'])\ndf1['year_month'] = df1['sale_date'].dt.to_period('M')\nmonthly = df1.groupby('year_month')['revenue'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of revenue')\nplt.show()"}
{"instruction": "Load time series CSV 'logfile.log', set 'sale_date' as index, resample weekly average of 'speed', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('logfile.log', parse_dates=['sale_date'], index_col='sale_date')\nweekly = data['speed'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly speed with Anomalies')\nplt.show()"}
{"instruction": "Parse 'sale_date' in data_clean as datetime, extract year and month, compute monthly sum of 'count', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean['sale_date'] = pd.to_datetime(data_clean['sale_date'])\ndata_clean['year_month'] = data_clean['sale_date'].dt.to_period('M')\nmonthly = data_clean.groupby('year_month')['count'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of count')\nplt.show()"}
{"instruction": "Read CSV file 'sales.csv' into DataFrame data_raw, drop duplicates, fill missing values in 'order_date' with median, group by 'category' and plot average 'age'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('sales.csv')\ndata_raw = data_raw.drop_duplicates()\ndata_raw['order_date'] = data_raw['order_date'].fillna(data_raw['order_date'].median())\navg = data_raw.groupby('category')['age'].mean()\navg.plot(kind='bar')\nplt.title('Average age by category')\nplt.show()"}
{"instruction": "Filter sales_df for rows where 'region' contains 'A', then encode 'class' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nsales_df = sales_df[sales_df['region'].str.contains('A', na=False)]\nsales_df_ohe = pd.get_dummies(sales_df, columns=['class'])\ntrain, test = train_test_split(sales_df_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read CSV file 'input.json' into DataFrame sales_df, drop duplicates, fill missing values in 'city' with median, group by 'category' and plot average 'value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('input.json')\nsales_df = sales_df.drop_duplicates()\nsales_df['city'] = sales_df['city'].fillna(sales_df['city'].median())\navg = sales_df.groupby('category')['value'].mean()\navg.plot(kind='bar')\nplt.title('Average value by category')\nplt.show()"}
{"instruction": "Read CSV file 'time_series.csv' into DataFrame df, drop duplicates, fill missing values in 'salary' with median, group by 'department' and plot average 'age'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('time_series.csv')\ndf = df.drop_duplicates()\ndf['salary'] = df['salary'].fillna(df['salary'].median())\navg = df.groupby('department')['age'].mean()\navg.plot(kind='bar')\nplt.title('Average age by department')\nplt.show()"}
{"instruction": "Load JSON file 'data.csv' into DataFrame data, normalize nested records, filter rows where 'class' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndata = pd.read_json('data.csv')\ndata = pd.json_normalize(data.to_dict(orient='records'))\ndata = data[data['class'] > 0]\ndata.to_parquet('results.parquet')"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into sales_df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\nsales_df['timestamp'] = pd.to_datetime(sales_df['timestamp'])\nerrors = sales_df[sales_df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load time series CSV 'sales.csv', set 'date' as index, resample weekly average of 'height', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('sales.csv', parse_dates=['date'], index_col='date')\nweekly = data_raw['height'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly height with Anomalies')\nplt.show()"}
{"instruction": "Parse 'date' in df_sales as datetime, extract year and month, compute monthly sum of 'weight', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales['date'] = pd.to_datetime(df_sales['date'])\ndf_sales['year_month'] = df_sales['date'].dt.to_period('M')\nmonthly = df_sales.groupby('year_month')['weight'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of weight')\nplt.show()"}
{"instruction": "Load JSON file 'input.json' into DataFrame sales_df, normalize nested records, filter rows where 'timestamp' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\nsales_df = pd.read_json('input.json')\nsales_df = pd.json_normalize(sales_df.to_dict(orient='records'))\nsales_df = sales_df[sales_df['timestamp'] > 0]\nsales_df.to_parquet('results.parquet')"}
{"instruction": "Filter df2 for rows where 'department' contains 'A', then encode 'region' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf2 = df2[df2['department'].str.contains('A', na=False)]\ndf2_ohe = pd.get_dummies(df2, columns=['region'])\ntrain, test = train_test_split(df2_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Load JSON file 'sales.csv' into DataFrame data, normalize nested records, filter rows where 'order_date' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndata = pd.read_json('sales.csv')\ndata = pd.json_normalize(data.to_dict(orient='records'))\ndata = data[data['order_date'] > 0]\ndata.to_parquet('clean_data.csv')"}
{"instruction": "Read CSV file 'data.xlsx' into DataFrame data, drop duplicates, fill missing values in 'class' with median, group by 'type' and plot average 'height'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('data.xlsx')\ndata = data.drop_duplicates()\ndata['class'] = data['class'].fillna(data['class'].median())\navg = data.groupby('type')['height'].mean()\navg.plot(kind='bar')\nplt.title('Average height by type')\nplt.show()"}
{"instruction": "Load time series CSV 'data.xlsx', set 'timestamp' as index, resample weekly average of 'age', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('data.xlsx', parse_dates=['timestamp'], index_col='timestamp')\nweekly = df2['age'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly age with Anomalies')\nplt.show()"}
{"instruction": "Load time series CSV 'input.json', set 'sale_date' as index, resample weekly average of 'rating', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('input.json', parse_dates=['sale_date'], index_col='sale_date')\nweekly = data_clean['rating'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly rating with Anomalies')\nplt.show()"}
{"instruction": "Read CSV file 'input.json' into DataFrame df2, drop duplicates, fill missing values in 'department' with median, group by 'class' and plot average 'salary'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('input.json')\ndf2 = df2.drop_duplicates()\ndf2['department'] = df2['department'].fillna(df2['department'].median())\navg = df2.groupby('class')['salary'].mean()\navg.plot(kind='bar')\nplt.title('Average salary by class')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['sales.csv', 'data.xlsx', 'time_series.csv'] into df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['sales.csv', 'data.xlsx', 'time_series.csv'])\ndf = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf.columns = df.columns.str.lower()\nnull_frac = df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf.drop(columns=cols_to_drop, inplace=True)\ndf.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read log file 'input.json' with custom delimiter '|', parse into df2, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('input.json', sep='|', names=['timestamp','level','message'])\ndf2['timestamp'] = pd.to_datetime(df2['timestamp'])\nerrors = df2[df2['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read Excel file 'data.csv' sheet 'Report', concatenate sheets into df2, pivot on 'region' and 'salary', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.csv', sheet_name=None)\ndf2 = pd.concat(sheets.values(), ignore_index=True)\npivot = df2.pivot_table(index='region', columns='salary', values='salary', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read CSV file 'data.xlsx' into DataFrame df, drop duplicates, fill missing values in 'date' with median, group by 'group' and plot average 'price'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('data.xlsx')\ndf = df.drop_duplicates()\ndf['date'] = df['date'].fillna(df['date'].median())\navg = df.groupby('group')['price'].mean()\navg.plot(kind='bar')\nplt.title('Average price by group')\nplt.show()"}
{"instruction": "Read CSV file 'data.xlsx' into DataFrame data_clean, drop duplicates, fill missing values in 'region' with median, group by 'group' and plot average 'rating'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('data.xlsx')\ndata_clean = data_clean.drop_duplicates()\ndata_clean['region'] = data_clean['region'].fillna(data_clean['region'].median())\navg = data_clean.groupby('group')['rating'].mean()\navg.plot(kind='bar')\nplt.title('Average rating by group')\nplt.show()"}
{"instruction": "Parse 'date' in df as datetime, extract year and month, compute monthly sum of 'rating', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf['date'] = pd.to_datetime(df['date'])\ndf['year_month'] = df['date'].dt.to_period('M')\nmonthly = df.groupby('year_month')['rating'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of rating')\nplt.show()"}
{"instruction": "Load JSON file 'input.json' into DataFrame data, normalize nested records, filter rows where 'score' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndata = pd.read_json('input.json')\ndata = pd.json_normalize(data.to_dict(orient='records'))\ndata = data[data['score'] > 0]\ndata.to_parquet('clean_data.csv')"}
{"instruction": "Load JSON file 'input.json' into DataFrame df_data, normalize nested records, filter rows where 'order_date' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndf_data = pd.read_json('input.json')\ndf_data = pd.json_normalize(df_data.to_dict(orient='records'))\ndf_data = df_data[df_data['order_date'] > 0]\ndf_data.to_parquet('clean_data.csv')"}
{"instruction": "Read Excel file 'input.json' sheet 'Data', concatenate sheets into df, pivot on 'region' and 'income', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('input.json', sheet_name=None)\ndf = pd.concat(sheets.values(), ignore_index=True)\npivot = df.pivot_table(index='region', columns='income', values='income', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read log file 'sales.csv' with custom delimiter '|', parse into data_clean, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('sales.csv', sep='|', names=['timestamp','level','message'])\ndata_clean['timestamp'] = pd.to_datetime(data_clean['timestamp'])\nerrors = data_clean[data_clean['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read log file 'input.json' with custom delimiter '|', parse into data, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('input.json', sep='|', names=['timestamp','level','message'])\ndata['timestamp'] = pd.to_datetime(data['timestamp'])\nerrors = data[data['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load JSON file 'data.csv' into DataFrame data, normalize nested records, filter rows where 'category' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndata = pd.read_json('data.csv')\ndata = pd.json_normalize(data.to_dict(orient='records'))\ndata = data[data['category'] > 0]\ndata.to_parquet('results.parquet')"}
{"instruction": "Concatenate multiple CSV files ['logfile.log', 'data.csv', 'input.json'] into sales_df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['logfile.log', 'data.csv', 'input.json'])\nsales_df = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\nsales_df.columns = sales_df.columns.str.lower()\nnull_frac = sales_df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\nsales_df.drop(columns=cols_to_drop, inplace=True)\nsales_df.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read log file 'time_series.csv' with custom delimiter '|', parse into dataset, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('time_series.csv', sep='|', names=['timestamp','level','message'])\ndataset['timestamp'] = pd.to_datetime(dataset['timestamp'])\nerrors = dataset[dataset['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Parse 'sale_date' in data as datetime, extract year and month, compute monthly sum of 'height', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata['sale_date'] = pd.to_datetime(data['sale_date'])\ndata['year_month'] = data['sale_date'].dt.to_period('M')\nmonthly = data.groupby('year_month')['height'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of height')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['sales.csv', 'data.csv', 'input.json'] into sales_df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['sales.csv', 'data.csv', 'input.json'])\nsales_df = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\nsales_df.columns = sales_df.columns.str.lower()\nnull_frac = sales_df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\nsales_df.drop(columns=cols_to_drop, inplace=True)\nsales_df.to_csv('clean_data.csv', index=False)"}
{"instruction": "Filter df_sales for rows where 'department' contains 'A', then encode 'category' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_sales = df_sales[df_sales['department'].str.contains('A', na=False)]\ndf_sales_ohe = pd.get_dummies(df_sales, columns=['category'])\ntrain, test = train_test_split(df_sales_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read log file 'input.json' with custom delimiter '|', parse into df_sales, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('input.json', sep='|', names=['timestamp','level','message'])\ndf_sales['timestamp'] = pd.to_datetime(df_sales['timestamp'])\nerrors = df_sales[df_sales['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\nerrors = df[df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load JSON file 'time_series.csv' into DataFrame data_raw, normalize nested records, filter rows where 'city' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndata_raw = pd.read_json('time_series.csv')\ndata_raw = pd.json_normalize(data_raw.to_dict(orient='records'))\ndata_raw = data_raw[data_raw['city'] > 0]\ndata_raw.to_parquet('output.parquet')"}
{"instruction": "Load time series CSV 'data.csv', set 'date' as index, resample weekly average of 'revenue', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('data.csv', parse_dates=['date'], index_col='date')\nweekly = df['revenue'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly revenue with Anomalies')\nplt.show()"}
{"instruction": "Read CSV file 'data.csv' into DataFrame data_raw, drop duplicates, fill missing values in 'class' with median, group by 'department' and plot average 'salary'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('data.csv')\ndata_raw = data_raw.drop_duplicates()\ndata_raw['class'] = data_raw['class'].fillna(data_raw['class'].median())\navg = data_raw.groupby('department')['salary'].mean()\navg.plot(kind='bar')\nplt.title('Average salary by department')\nplt.show()"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into df1, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\ndf1['timestamp'] = pd.to_datetime(df1['timestamp'])\nerrors = df1[df1['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Filter data_clean for rows where 'price' contains 'A', then encode 'department' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata_clean = data_clean[data_clean['price'].str.contains('A', na=False)]\ndata_clean_ohe = pd.get_dummies(data_clean, columns=['department'])\ntrain, test = train_test_split(data_clean_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read log file 'sales.csv' with custom delimiter '|', parse into data_clean, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('sales.csv', sep='|', names=['timestamp','level','message'])\ndata_clean['timestamp'] = pd.to_datetime(data_clean['timestamp'])\nerrors = data_clean[data_clean['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Filter sales_df for rows where 'group' contains 'A', then encode 'status' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nsales_df = sales_df[sales_df['group'].str.contains('A', na=False)]\nsales_df_ohe = pd.get_dummies(sales_df, columns=['status'])\ntrain, test = train_test_split(sales_df_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Parse 'order_date' in df1 as datetime, extract year and month, compute monthly sum of 'speed', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1['order_date'] = pd.to_datetime(df1['order_date'])\ndf1['year_month'] = df1['order_date'].dt.to_period('M')\nmonthly = df1.groupby('year_month')['speed'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of speed')\nplt.show()"}
{"instruction": "Read Excel file 'logfile.log' sheet 'Data', concatenate sheets into sales_df, pivot on 'region' and 'score', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('logfile.log', sheet_name=None)\nsales_df = pd.concat(sheets.values(), ignore_index=True)\npivot = sales_df.pivot_table(index='region', columns='score', values='score', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Parse 'date' in dataset as datetime, extract year and month, compute monthly sum of 'sales', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset['date'] = pd.to_datetime(dataset['date'])\ndataset['year_month'] = dataset['date'].dt.to_period('M')\nmonthly = dataset.groupby('year_month')['sales'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of sales')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['logfile.log', 'sales.csv', 'data.csv'] into sales_df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['logfile.log', 'sales.csv', 'data.csv'])\nsales_df = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\nsales_df.columns = sales_df.columns.str.lower()\nnull_frac = sales_df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\nsales_df.drop(columns=cols_to_drop, inplace=True)\nsales_df.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read log file 'data.csv' with custom delimiter '|', parse into sales_df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('data.csv', sep='|', names=['timestamp','level','message'])\nsales_df['timestamp'] = pd.to_datetime(sales_df['timestamp'])\nerrors = sales_df[sales_df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load JSON file 'input.json' into DataFrame data_raw, normalize nested records, filter rows where 'count' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndata_raw = pd.read_json('input.json')\ndata_raw = pd.json_normalize(data_raw.to_dict(orient='records'))\ndata_raw = data_raw[data_raw['count'] > 0]\ndata_raw.to_parquet('clean_data.csv')"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\nerrors = df[df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load JSON file 'data.csv' into DataFrame df_sales, normalize nested records, filter rows where 'class' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndf_sales = pd.read_json('data.csv')\ndf_sales = pd.json_normalize(df_sales.to_dict(orient='records'))\ndf_sales = df_sales[df_sales['class'] > 0]\ndf_sales.to_parquet('output.parquet')"}
{"instruction": "Read CSV file 'sales.csv' into DataFrame data_clean, drop duplicates, fill missing values in 'class' with median, group by 'category' and plot average 'income'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('sales.csv')\ndata_clean = data_clean.drop_duplicates()\ndata_clean['class'] = data_clean['class'].fillna(data_clean['class'].median())\navg = data_clean.groupby('category')['income'].mean()\navg.plot(kind='bar')\nplt.title('Average income by category')\nplt.show()"}
{"instruction": "Read CSV file 'input.json' into DataFrame df, drop duplicates, fill missing values in 'age' with median, group by 'department' and plot average 'score'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('input.json')\ndf = df.drop_duplicates()\ndf['age'] = df['age'].fillna(df['age'].median())\navg = df.groupby('department')['score'].mean()\navg.plot(kind='bar')\nplt.title('Average score by department')\nplt.show()"}
{"instruction": "Filter df_sales for rows where 'sales' contains 'A', then encode 'status' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_sales = df_sales[df_sales['sales'].str.contains('A', na=False)]\ndf_sales_ohe = pd.get_dummies(df_sales, columns=['status'])\ntrain, test = train_test_split(df_sales_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Filter sales_df for rows where 'sale_date' contains 'A', then encode 'type' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nsales_df = sales_df[sales_df['sale_date'].str.contains('A', na=False)]\nsales_df_ohe = pd.get_dummies(sales_df, columns=['type'])\ntrain, test = train_test_split(sales_df_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read Excel file 'data.csv' sheet 'Report', concatenate sheets into sales_df, pivot on 'class' and 'sales', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.csv', sheet_name=None)\nsales_df = pd.concat(sheets.values(), ignore_index=True)\npivot = sales_df.pivot_table(index='class', columns='sales', values='sales', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Load JSON file 'logfile.log' into DataFrame df_data, normalize nested records, filter rows where 'rating' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndf_data = pd.read_json('logfile.log')\ndf_data = pd.json_normalize(df_data.to_dict(orient='records'))\ndf_data = df_data[df_data['rating'] > 0]\ndf_data.to_parquet('clean_data.csv')"}
{"instruction": "Read CSV file 'data.xlsx' into DataFrame data_raw, drop duplicates, fill missing values in 'category' with median, group by 'class' and plot average 'rating'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('data.xlsx')\ndata_raw = data_raw.drop_duplicates()\ndata_raw['category'] = data_raw['category'].fillna(data_raw['category'].median())\navg = data_raw.groupby('class')['rating'].mean()\navg.plot(kind='bar')\nplt.title('Average rating by class')\nplt.show()"}
{"instruction": "Read CSV file 'input.json' into DataFrame df_data, drop duplicates, fill missing values in 'order_date' with median, group by 'group' and plot average 'speed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('input.json')\ndf_data = df_data.drop_duplicates()\ndf_data['order_date'] = df_data['order_date'].fillna(df_data['order_date'].median())\navg = df_data.groupby('group')['speed'].mean()\navg.plot(kind='bar')\nplt.title('Average speed by group')\nplt.show()"}
{"instruction": "Filter df for rows where 'score' contains 'A', then encode 'department' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf = df[df['score'].str.contains('A', na=False)]\ndf_ohe = pd.get_dummies(df, columns=['department'])\ntrain, test = train_test_split(df_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Parse 'order_date' in df as datetime, extract year and month, compute monthly sum of 'value', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf['order_date'] = pd.to_datetime(df['order_date'])\ndf['year_month'] = df['order_date'].dt.to_period('M')\nmonthly = df.groupby('year_month')['value'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of value')\nplt.show()"}
{"instruction": "Load time series CSV 'time_series.csv', set 'sale_date' as index, resample weekly average of 'score', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('time_series.csv', parse_dates=['sale_date'], index_col='sale_date')\nweekly = df_sales['score'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly score with Anomalies')\nplt.show()"}
{"instruction": "Load time series CSV 'input.json', set 'timestamp' as index, resample weekly average of 'duration', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('input.json', parse_dates=['timestamp'], index_col='timestamp')\nweekly = dataset['duration'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly duration with Anomalies')\nplt.show()"}
{"instruction": "Load JSON file 'time_series.csv' into DataFrame data_raw, normalize nested records, filter rows where 'type' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndata_raw = pd.read_json('time_series.csv')\ndata_raw = pd.json_normalize(data_raw.to_dict(orient='records'))\ndata_raw = data_raw[data_raw['type'] > 0]\ndata_raw.to_parquet('output.parquet')"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into data_clean, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\ndata_clean['timestamp'] = pd.to_datetime(data_clean['timestamp'])\nerrors = data_clean[data_clean['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read Excel file 'data.csv' sheet 'Report', concatenate sheets into data_raw, pivot on 'class' and 'price', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.csv', sheet_name=None)\ndata_raw = pd.concat(sheets.values(), ignore_index=True)\npivot = data_raw.pivot_table(index='class', columns='price', values='price', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Load JSON file 'time_series.csv' into DataFrame dataset, normalize nested records, filter rows where 'height' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndataset = pd.read_json('time_series.csv')\ndataset = pd.json_normalize(dataset.to_dict(orient='records'))\ndataset = dataset[dataset['height'] > 0]\ndataset.to_parquet('clean_data.csv')"}
{"instruction": "Concatenate multiple CSV files ['input.json', 'time_series.csv', 'data.csv'] into df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['input.json', 'time_series.csv', 'data.csv'])\ndf = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf.columns = df.columns.str.lower()\nnull_frac = df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf.drop(columns=cols_to_drop, inplace=True)\ndf.to_csv('clean_data.csv', index=False)"}
{"instruction": "Filter sales_df for rows where 'order_date' contains 'A', then encode 'status' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nsales_df = sales_df[sales_df['order_date'].str.contains('A', na=False)]\nsales_df_ohe = pd.get_dummies(sales_df, columns=['status'])\ntrain, test = train_test_split(sales_df_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Filter data for rows where 'city' contains 'A', then encode 'status' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata = data[data['city'].str.contains('A', na=False)]\ndata_ohe = pd.get_dummies(data, columns=['status'])\ntrain, test = train_test_split(data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Concatenate multiple CSV files ['time_series.csv', 'input.json', 'data.csv'] into sales_df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['time_series.csv', 'input.json', 'data.csv'])\nsales_df = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\nsales_df.columns = sales_df.columns.str.lower()\nnull_frac = sales_df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\nsales_df.drop(columns=cols_to_drop, inplace=True)\nsales_df.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read log file 'logfile.log' with custom delimiter '|', parse into data_raw, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('logfile.log', sep='|', names=['timestamp','level','message'])\ndata_raw['timestamp'] = pd.to_datetime(data_raw['timestamp'])\nerrors = data_raw[data_raw['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Parse 'timestamp' in df2 as datetime, extract year and month, compute monthly sum of 'quantity', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2['timestamp'] = pd.to_datetime(df2['timestamp'])\ndf2['year_month'] = df2['timestamp'].dt.to_period('M')\nmonthly = df2.groupby('year_month')['quantity'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of quantity')\nplt.show()"}
{"instruction": "Read CSV file 'data.xlsx' into DataFrame df_data, drop duplicates, fill missing values in 'speed' with median, group by 'city' and plot average 'income'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('data.xlsx')\ndf_data = df_data.drop_duplicates()\ndf_data['speed'] = df_data['speed'].fillna(df_data['speed'].median())\navg = df_data.groupby('city')['income'].mean()\navg.plot(kind='bar')\nplt.title('Average income by city')\nplt.show()"}
{"instruction": "Load time series CSV 'input.json', set 'timestamp' as index, resample weekly average of 'sales', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('input.json', parse_dates=['timestamp'], index_col='timestamp')\nweekly = data_raw['sales'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly sales with Anomalies')\nplt.show()"}
{"instruction": "Read CSV file 'time_series.csv' into DataFrame data_raw, drop duplicates, fill missing values in 'height' with median, group by 'status' and plot average 'income'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('time_series.csv')\ndata_raw = data_raw.drop_duplicates()\ndata_raw['height'] = data_raw['height'].fillna(data_raw['height'].median())\navg = data_raw.groupby('status')['income'].mean()\navg.plot(kind='bar')\nplt.title('Average income by status')\nplt.show()"}
{"instruction": "Read CSV file 'input.json' into DataFrame data_clean, drop duplicates, fill missing values in 'score' with median, group by 'type' and plot average 'score'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('input.json')\ndata_clean = data_clean.drop_duplicates()\ndata_clean['score'] = data_clean['score'].fillna(data_clean['score'].median())\navg = data_clean.groupby('type')['score'].mean()\navg.plot(kind='bar')\nplt.title('Average score by type')\nplt.show()"}
{"instruction": "Read CSV file 'time_series.csv' into DataFrame df, drop duplicates, fill missing values in 'timestamp' with median, group by 'type' and plot average 'count'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('time_series.csv')\ndf = df.drop_duplicates()\ndf['timestamp'] = df['timestamp'].fillna(df['timestamp'].median())\navg = df.groupby('type')['count'].mean()\navg.plot(kind='bar')\nplt.title('Average count by type')\nplt.show()"}
{"instruction": "Parse 'timestamp' in dataset as datetime, extract year and month, compute monthly sum of 'speed', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset['timestamp'] = pd.to_datetime(dataset['timestamp'])\ndataset['year_month'] = dataset['timestamp'].dt.to_period('M')\nmonthly = dataset.groupby('year_month')['speed'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of speed')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['data.csv', 'logfile.log', 'sales.csv'] into df2, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.csv', 'logfile.log', 'sales.csv'])\ndf2 = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf2.columns = df2.columns.str.lower()\nnull_frac = df2.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf2.drop(columns=cols_to_drop, inplace=True)\ndf2.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read Excel file 'time_series.csv' sheet 'Report', concatenate sheets into df_sales, pivot on 'status' and 'speed', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('time_series.csv', sheet_name=None)\ndf_sales = pd.concat(sheets.values(), ignore_index=True)\npivot = df_sales.pivot_table(index='status', columns='speed', values='speed', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read Excel file 'input.json' sheet 'Data', concatenate sheets into data, pivot on 'group' and 'count', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('input.json', sheet_name=None)\ndata = pd.concat(sheets.values(), ignore_index=True)\npivot = data.pivot_table(index='group', columns='count', values='count', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Load JSON file 'input.json' into DataFrame df2, normalize nested records, filter rows where 'rating' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndf2 = pd.read_json('input.json')\ndf2 = pd.json_normalize(df2.to_dict(orient='records'))\ndf2 = df2[df2['rating'] > 0]\ndf2.to_parquet('results.parquet')"}
{"instruction": "Load JSON file 'logfile.log' into DataFrame df, normalize nested records, filter rows where 'height' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndf = pd.read_json('logfile.log')\ndf = pd.json_normalize(df.to_dict(orient='records'))\ndf = df[df['height'] > 0]\ndf.to_parquet('results.parquet')"}
{"instruction": "Concatenate multiple CSV files ['data.csv', 'input.json', 'logfile.log'] into df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.csv', 'input.json', 'logfile.log'])\ndf = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf.columns = df.columns.str.lower()\nnull_frac = df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf.drop(columns=cols_to_drop, inplace=True)\ndf.to_csv('clean_data.csv', index=False)"}
{"instruction": "Concatenate multiple CSV files ['data.xlsx', 'input.json', 'data.csv'] into dataset, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.xlsx', 'input.json', 'data.csv'])\ndataset = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndataset.columns = dataset.columns.str.lower()\nnull_frac = dataset.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndataset.drop(columns=cols_to_drop, inplace=True)\ndataset.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read log file 'input.json' with custom delimiter '|', parse into sales_df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('input.json', sep='|', names=['timestamp','level','message'])\nsales_df['timestamp'] = pd.to_datetime(sales_df['timestamp'])\nerrors = sales_df[sales_df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['data.csv', 'input.json', 'data.xlsx'] into df1, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.csv', 'input.json', 'data.xlsx'])\ndf1 = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf1.columns = df1.columns.str.lower()\nnull_frac = df1.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf1.drop(columns=cols_to_drop, inplace=True)\ndf1.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read log file 'data.csv' with custom delimiter '|', parse into df1, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('data.csv', sep='|', names=['timestamp','level','message'])\ndf1['timestamp'] = pd.to_datetime(df1['timestamp'])\nerrors = df1[df1['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Filter df2 for rows where 'height' contains 'A', then encode 'city' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf2 = df2[df2['height'].str.contains('A', na=False)]\ndf2_ohe = pd.get_dummies(df2, columns=['city'])\ntrain, test = train_test_split(df2_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Concatenate multiple CSV files ['time_series.csv', 'logfile.log', 'sales.csv'] into df_data, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['time_series.csv', 'logfile.log', 'sales.csv'])\ndf_data = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf_data.columns = df_data.columns.str.lower()\nnull_frac = df_data.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf_data.drop(columns=cols_to_drop, inplace=True)\ndf_data.to_csv('clean_data.csv', index=False)"}
{"instruction": "Concatenate multiple CSV files ['data.xlsx', 'sales.csv', 'time_series.csv'] into data_clean, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.xlsx', 'sales.csv', 'time_series.csv'])\ndata_clean = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata_clean.columns = data_clean.columns.str.lower()\nnull_frac = data_clean.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata_clean.drop(columns=cols_to_drop, inplace=True)\ndata_clean.to_csv('clean_data.csv', index=False)"}
{"instruction": "Parse 'date' in dataset as datetime, extract year and month, compute monthly sum of 'quantity', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset['date'] = pd.to_datetime(dataset['date'])\ndataset['year_month'] = dataset['date'].dt.to_period('M')\nmonthly = dataset.groupby('year_month')['quantity'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of quantity')\nplt.show()"}
{"instruction": "Load JSON file 'logfile.log' into DataFrame df, normalize nested records, filter rows where 'revenue' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndf = pd.read_json('logfile.log')\ndf = pd.json_normalize(df.to_dict(orient='records'))\ndf = df[df['revenue'] > 0]\ndf.to_parquet('clean_data.csv')"}
{"instruction": "Load time series CSV 'data.xlsx', set 'order_date' as index, resample weekly average of 'age', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('data.xlsx', parse_dates=['order_date'], index_col='order_date')\nweekly = data_clean['age'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly age with Anomalies')\nplt.show()"}
{"instruction": "Read log file 'logfile.log' with custom delimiter '|', parse into data, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('logfile.log', sep='|', names=['timestamp','level','message'])\ndata['timestamp'] = pd.to_datetime(data['timestamp'])\nerrors = data[data['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load time series CSV 'sales.csv', set 'timestamp' as index, resample weekly average of 'count', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('sales.csv', parse_dates=['timestamp'], index_col='timestamp')\nweekly = df1['count'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly count with Anomalies')\nplt.show()"}
{"instruction": "Parse 'date' in df as datetime, extract year and month, compute monthly sum of 'score', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf['date'] = pd.to_datetime(df['date'])\ndf['year_month'] = df['date'].dt.to_period('M')\nmonthly = df.groupby('year_month')['score'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of score')\nplt.show()"}
{"instruction": "Parse 'timestamp' in df_data as datetime, extract year and month, compute monthly sum of 'height', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data['timestamp'] = pd.to_datetime(df_data['timestamp'])\ndf_data['year_month'] = df_data['timestamp'].dt.to_period('M')\nmonthly = df_data.groupby('year_month')['height'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of height')\nplt.show()"}
{"instruction": "Read CSV file 'sales.csv' into DataFrame df_sales, drop duplicates, fill missing values in 'city' with median, group by 'class' and plot average 'weight'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('sales.csv')\ndf_sales = df_sales.drop_duplicates()\ndf_sales['city'] = df_sales['city'].fillna(df_sales['city'].median())\navg = df_sales.groupby('class')['weight'].mean()\navg.plot(kind='bar')\nplt.title('Average weight by class')\nplt.show()"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into df_sales, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\ndf_sales['timestamp'] = pd.to_datetime(df_sales['timestamp'])\nerrors = df_sales[df_sales['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Parse 'timestamp' in data as datetime, extract year and month, compute monthly sum of 'value', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata['timestamp'] = pd.to_datetime(data['timestamp'])\ndata['year_month'] = data['timestamp'].dt.to_period('M')\nmonthly = data.groupby('year_month')['value'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of value')\nplt.show()"}
{"instruction": "Load time series CSV 'input.json', set 'timestamp' as index, resample weekly average of 'value', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('input.json', parse_dates=['timestamp'], index_col='timestamp')\nweekly = df['value'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly value with Anomalies')\nplt.show()"}
{"instruction": "Read CSV file 'input.json' into DataFrame data_clean, drop duplicates, fill missing values in 'rating' with median, group by 'department' and plot average 'salary'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('input.json')\ndata_clean = data_clean.drop_duplicates()\ndata_clean['rating'] = data_clean['rating'].fillna(data_clean['rating'].median())\navg = data_clean.groupby('department')['salary'].mean()\navg.plot(kind='bar')\nplt.title('Average salary by department')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['data.csv', 'sales.csv', 'input.json'] into df_sales, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.csv', 'sales.csv', 'input.json'])\ndf_sales = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf_sales.columns = df_sales.columns.str.lower()\nnull_frac = df_sales.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf_sales.drop(columns=cols_to_drop, inplace=True)\ndf_sales.to_csv('clean_data.csv', index=False)"}
{"instruction": "Parse 'order_date' in df1 as datetime, extract year and month, compute monthly sum of 'salary', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1['order_date'] = pd.to_datetime(df1['order_date'])\ndf1['year_month'] = df1['order_date'].dt.to_period('M')\nmonthly = df1.groupby('year_month')['salary'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of salary')\nplt.show()"}
{"instruction": "Parse 'sale_date' in df_sales as datetime, extract year and month, compute monthly sum of 'price', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales['sale_date'] = pd.to_datetime(df_sales['sale_date'])\ndf_sales['year_month'] = df_sales['sale_date'].dt.to_period('M')\nmonthly = df_sales.groupby('year_month')['price'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of price')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['data.xlsx', 'input.json', 'sales.csv'] into sales_df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.xlsx', 'input.json', 'sales.csv'])\nsales_df = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\nsales_df.columns = sales_df.columns.str.lower()\nnull_frac = sales_df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\nsales_df.drop(columns=cols_to_drop, inplace=True)\nsales_df.to_csv('clean_data.csv', index=False)"}
{"instruction": "Parse 'date' in sales_df as datetime, extract year and month, compute monthly sum of 'count', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df['date'] = pd.to_datetime(sales_df['date'])\nsales_df['year_month'] = sales_df['date'].dt.to_period('M')\nmonthly = sales_df.groupby('year_month')['count'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of count')\nplt.show()"}
{"instruction": "Load JSON file 'time_series.csv' into DataFrame df_data, normalize nested records, filter rows where 'date' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndf_data = pd.read_json('time_series.csv')\ndf_data = pd.json_normalize(df_data.to_dict(orient='records'))\ndf_data = df_data[df_data['date'] > 0]\ndf_data.to_parquet('results.parquet')"}
{"instruction": "Parse 'sale_date' in df as datetime, extract year and month, compute monthly sum of 'count', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf['sale_date'] = pd.to_datetime(df['sale_date'])\ndf['year_month'] = df['sale_date'].dt.to_period('M')\nmonthly = df.groupby('year_month')['count'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of count')\nplt.show()"}
{"instruction": "Filter data_raw for rows where 'sales' contains 'A', then encode 'region' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata_raw = data_raw[data_raw['sales'].str.contains('A', na=False)]\ndata_raw_ohe = pd.get_dummies(data_raw, columns=['region'])\ntrain, test = train_test_split(data_raw_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read Excel file 'logfile.log' sheet 'Sheet1', concatenate sheets into df1, pivot on 'department' and 'sales', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('logfile.log', sheet_name=None)\ndf1 = pd.concat(sheets.values(), ignore_index=True)\npivot = df1.pivot_table(index='department', columns='sales', values='sales', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['data.csv', 'data.xlsx', 'input.json'] into data_clean, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.csv', 'data.xlsx', 'input.json'])\ndata_clean = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata_clean.columns = data_clean.columns.str.lower()\nnull_frac = data_clean.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata_clean.drop(columns=cols_to_drop, inplace=True)\ndata_clean.to_csv('clean_data.csv', index=False)"}
{"instruction": "Concatenate multiple CSV files ['time_series.csv', 'logfile.log', 'input.json'] into df1, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['time_series.csv', 'logfile.log', 'input.json'])\ndf1 = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf1.columns = df1.columns.str.lower()\nnull_frac = df1.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf1.drop(columns=cols_to_drop, inplace=True)\ndf1.to_csv('clean_data.csv', index=False)"}
{"instruction": "Concatenate multiple CSV files ['sales.csv', 'data.csv', 'logfile.log'] into dataset, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['sales.csv', 'data.csv', 'logfile.log'])\ndataset = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndataset.columns = dataset.columns.str.lower()\nnull_frac = dataset.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndataset.drop(columns=cols_to_drop, inplace=True)\ndataset.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read Excel file 'data.csv' sheet 'Report', concatenate sheets into data_clean, pivot on 'type' and 'value', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.csv', sheet_name=None)\ndata_clean = pd.concat(sheets.values(), ignore_index=True)\npivot = data_clean.pivot_table(index='type', columns='value', values='value', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Parse 'order_date' in dataset as datetime, extract year and month, compute monthly sum of 'income', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset['order_date'] = pd.to_datetime(dataset['order_date'])\ndataset['year_month'] = dataset['order_date'].dt.to_period('M')\nmonthly = dataset.groupby('year_month')['income'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of income')\nplt.show()"}
{"instruction": "Read Excel file 'data.xlsx' sheet 'Report', concatenate sheets into df2, pivot on 'group' and 'price', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.xlsx', sheet_name=None)\ndf2 = pd.concat(sheets.values(), ignore_index=True)\npivot = df2.pivot_table(index='group', columns='price', values='price', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Load time series CSV 'data.csv', set 'order_date' as index, resample weekly average of 'age', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('data.csv', parse_dates=['order_date'], index_col='order_date')\nweekly = dataset['age'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly age with Anomalies')\nplt.show()"}
{"instruction": "Load JSON file 'sales.csv' into DataFrame df2, normalize nested records, filter rows where 'sales' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndf2 = pd.read_json('sales.csv')\ndf2 = pd.json_normalize(df2.to_dict(orient='records'))\ndf2 = df2[df2['sales'] > 0]\ndf2.to_parquet('results.parquet')"}
{"instruction": "Read CSV file 'input.json' into DataFrame data_raw, drop duplicates, fill missing values in 'weight' with median, group by 'type' and plot average 'quantity'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('input.json')\ndata_raw = data_raw.drop_duplicates()\ndata_raw['weight'] = data_raw['weight'].fillna(data_raw['weight'].median())\navg = data_raw.groupby('type')['quantity'].mean()\navg.plot(kind='bar')\nplt.title('Average quantity by type')\nplt.show()"}
{"instruction": "Load JSON file 'logfile.log' into DataFrame df2, normalize nested records, filter rows where 'class' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndf2 = pd.read_json('logfile.log')\ndf2 = pd.json_normalize(df2.to_dict(orient='records'))\ndf2 = df2[df2['class'] > 0]\ndf2.to_parquet('output.parquet')"}
{"instruction": "Load time series CSV 'logfile.log', set 'timestamp' as index, resample weekly average of 'sales', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('logfile.log', parse_dates=['timestamp'], index_col='timestamp')\nweekly = dataset['sales'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly sales with Anomalies')\nplt.show()"}
{"instruction": "Load time series CSV 'time_series.csv', set 'sale_date' as index, resample weekly average of 'count', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('time_series.csv', parse_dates=['sale_date'], index_col='sale_date')\nweekly = df2['count'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly count with Anomalies')\nplt.show()"}
{"instruction": "Read CSV file 'logfile.log' into DataFrame df_sales, drop duplicates, fill missing values in 'rating' with median, group by 'group' and plot average 'weight'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('logfile.log')\ndf_sales = df_sales.drop_duplicates()\ndf_sales['rating'] = df_sales['rating'].fillna(df_sales['rating'].median())\navg = df_sales.groupby('group')['weight'].mean()\navg.plot(kind='bar')\nplt.title('Average weight by group')\nplt.show()"}
{"instruction": "Load JSON file 'data.xlsx' into DataFrame data, normalize nested records, filter rows where 'weight' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndata = pd.read_json('data.xlsx')\ndata = pd.json_normalize(data.to_dict(orient='records'))\ndata = data[data['weight'] > 0]\ndata.to_parquet('output.parquet')"}
{"instruction": "Parse 'order_date' in data as datetime, extract year and month, compute monthly sum of 'salary', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata['order_date'] = pd.to_datetime(data['order_date'])\ndata['year_month'] = data['order_date'].dt.to_period('M')\nmonthly = data.groupby('year_month')['salary'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of salary')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['sales.csv', 'logfile.log', 'input.json'] into data, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['sales.csv', 'logfile.log', 'input.json'])\ndata = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata.columns = data.columns.str.lower()\nnull_frac = data.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata.drop(columns=cols_to_drop, inplace=True)\ndata.to_csv('clean_data.csv', index=False)"}
{"instruction": "Concatenate multiple CSV files ['sales.csv', 'input.json', 'data.xlsx'] into dataset, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['sales.csv', 'input.json', 'data.xlsx'])\ndataset = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndataset.columns = dataset.columns.str.lower()\nnull_frac = dataset.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndataset.drop(columns=cols_to_drop, inplace=True)\ndataset.to_csv('clean_data.csv', index=False)"}
{"instruction": "Parse 'timestamp' in df_data as datetime, extract year and month, compute monthly sum of 'count', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data['timestamp'] = pd.to_datetime(df_data['timestamp'])\ndf_data['year_month'] = df_data['timestamp'].dt.to_period('M')\nmonthly = df_data.groupby('year_month')['count'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of count')\nplt.show()"}
{"instruction": "Load time series CSV 'input.json', set 'date' as index, resample weekly average of 'income', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('input.json', parse_dates=['date'], index_col='date')\nweekly = df1['income'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly income with Anomalies')\nplt.show()"}
{"instruction": "Load JSON file 'data.xlsx' into DataFrame data_raw, normalize nested records, filter rows where 'salary' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndata_raw = pd.read_json('data.xlsx')\ndata_raw = pd.json_normalize(data_raw.to_dict(orient='records'))\ndata_raw = data_raw[data_raw['salary'] > 0]\ndata_raw.to_parquet('results.parquet')"}
{"instruction": "Parse 'order_date' in sales_df as datetime, extract year and month, compute monthly sum of 'revenue', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df['order_date'] = pd.to_datetime(sales_df['order_date'])\nsales_df['year_month'] = sales_df['order_date'].dt.to_period('M')\nmonthly = sales_df.groupby('year_month')['revenue'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of revenue')\nplt.show()"}
{"instruction": "Load time series CSV 'data.xlsx', set 'timestamp' as index, resample weekly average of 'speed', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('data.xlsx', parse_dates=['timestamp'], index_col='timestamp')\nweekly = df1['speed'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly speed with Anomalies')\nplt.show()"}
{"instruction": "Filter sales_df for rows where 'age' contains 'A', then encode 'department' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nsales_df = sales_df[sales_df['age'].str.contains('A', na=False)]\nsales_df_ohe = pd.get_dummies(sales_df, columns=['department'])\ntrain, test = train_test_split(sales_df_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read log file 'sales.csv' with custom delimiter '|', parse into sales_df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('sales.csv', sep='|', names=['timestamp','level','message'])\nsales_df['timestamp'] = pd.to_datetime(sales_df['timestamp'])\nerrors = sales_df[sales_df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load time series CSV 'data.csv', set 'date' as index, resample weekly average of 'age', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('data.csv', parse_dates=['date'], index_col='date')\nweekly = df['age'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly age with Anomalies')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['sales.csv', 'data.csv', 'logfile.log'] into df1, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['sales.csv', 'data.csv', 'logfile.log'])\ndf1 = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf1.columns = df1.columns.str.lower()\nnull_frac = df1.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf1.drop(columns=cols_to_drop, inplace=True)\ndf1.to_csv('clean_data.csv', index=False)"}
{"instruction": "Parse 'sale_date' in sales_df as datetime, extract year and month, compute monthly sum of 'price', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df['sale_date'] = pd.to_datetime(sales_df['sale_date'])\nsales_df['year_month'] = sales_df['sale_date'].dt.to_period('M')\nmonthly = sales_df.groupby('year_month')['price'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of price')\nplt.show()"}
{"instruction": "Read CSV file 'input.json' into DataFrame dataset, drop duplicates, fill missing values in 'salary' with median, group by 'type' and plot average 'revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('input.json')\ndataset = dataset.drop_duplicates()\ndataset['salary'] = dataset['salary'].fillna(dataset['salary'].median())\navg = dataset.groupby('type')['revenue'].mean()\navg.plot(kind='bar')\nplt.title('Average revenue by type')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['sales.csv', 'data.xlsx', 'input.json'] into df1, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['sales.csv', 'data.xlsx', 'input.json'])\ndf1 = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf1.columns = df1.columns.str.lower()\nnull_frac = df1.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf1.drop(columns=cols_to_drop, inplace=True)\ndf1.to_csv('clean_data.csv', index=False)"}
{"instruction": "Load time series CSV 'input.json', set 'date' as index, resample weekly average of 'sales', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('input.json', parse_dates=['date'], index_col='date')\nweekly = df_data['sales'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly sales with Anomalies')\nplt.show()"}
{"instruction": "Read Excel file 'input.json' sheet 'Data', concatenate sheets into df_sales, pivot on 'category' and 'sales', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('input.json', sheet_name=None)\ndf_sales = pd.concat(sheets.values(), ignore_index=True)\npivot = df_sales.pivot_table(index='category', columns='sales', values='sales', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read log file 'data.csv' with custom delimiter '|', parse into sales_df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('data.csv', sep='|', names=['timestamp','level','message'])\nsales_df['timestamp'] = pd.to_datetime(sales_df['timestamp'])\nerrors = sales_df[sales_df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Parse 'sale_date' in data_raw as datetime, extract year and month, compute monthly sum of 'salary', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw['sale_date'] = pd.to_datetime(data_raw['sale_date'])\ndata_raw['year_month'] = data_raw['sale_date'].dt.to_period('M')\nmonthly = data_raw.groupby('year_month')['salary'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of salary')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['input.json', 'data.xlsx', 'logfile.log'] into df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['input.json', 'data.xlsx', 'logfile.log'])\ndf = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf.columns = df.columns.str.lower()\nnull_frac = df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf.drop(columns=cols_to_drop, inplace=True)\ndf.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read CSV file 'logfile.log' into DataFrame data_raw, drop duplicates, fill missing values in 'status' with median, group by 'department' and plot average 'revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('logfile.log')\ndata_raw = data_raw.drop_duplicates()\ndata_raw['status'] = data_raw['status'].fillna(data_raw['status'].median())\navg = data_raw.groupby('department')['revenue'].mean()\navg.plot(kind='bar')\nplt.title('Average revenue by department')\nplt.show()"}
{"instruction": "Read Excel file 'data.xlsx' sheet 'Report', concatenate sheets into data, pivot on 'type' and 'salary', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.xlsx', sheet_name=None)\ndata = pd.concat(sheets.values(), ignore_index=True)\npivot = data.pivot_table(index='type', columns='salary', values='salary', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Load JSON file 'data.csv' into DataFrame sales_df, normalize nested records, filter rows where 'class' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\nsales_df = pd.read_json('data.csv')\nsales_df = pd.json_normalize(sales_df.to_dict(orient='records'))\nsales_df = sales_df[sales_df['class'] > 0]\nsales_df.to_parquet('clean_data.csv')"}
{"instruction": "Load JSON file 'data.xlsx' into DataFrame df_sales, normalize nested records, filter rows where 'age' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndf_sales = pd.read_json('data.xlsx')\ndf_sales = pd.json_normalize(df_sales.to_dict(orient='records'))\ndf_sales = df_sales[df_sales['age'] > 0]\ndf_sales.to_parquet('clean_data.csv')"}
{"instruction": "Load time series CSV 'data.csv', set 'date' as index, resample weekly average of 'sales', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('data.csv', parse_dates=['date'], index_col='date')\nweekly = data['sales'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly sales with Anomalies')\nplt.show()"}
{"instruction": "Parse 'timestamp' in data as datetime, extract year and month, compute monthly sum of 'rating', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata['timestamp'] = pd.to_datetime(data['timestamp'])\ndata['year_month'] = data['timestamp'].dt.to_period('M')\nmonthly = data.groupby('year_month')['rating'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of rating')\nplt.show()"}
{"instruction": "Read log file 'sales.csv' with custom delimiter '|', parse into sales_df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('sales.csv', sep='|', names=['timestamp','level','message'])\nsales_df['timestamp'] = pd.to_datetime(sales_df['timestamp'])\nerrors = sales_df[sales_df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Parse 'order_date' in df2 as datetime, extract year and month, compute monthly sum of 'quantity', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2['order_date'] = pd.to_datetime(df2['order_date'])\ndf2['year_month'] = df2['order_date'].dt.to_period('M')\nmonthly = df2.groupby('year_month')['quantity'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of quantity')\nplt.show()"}
{"instruction": "Filter df1 for rows where 'class' contains 'A', then encode 'type' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf1 = df1[df1['class'].str.contains('A', na=False)]\ndf1_ohe = pd.get_dummies(df1, columns=['type'])\ntrain, test = train_test_split(df1_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read log file 'data.csv' with custom delimiter '|', parse into df1, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('data.csv', sep='|', names=['timestamp','level','message'])\ndf1['timestamp'] = pd.to_datetime(df1['timestamp'])\nerrors = df1[df1['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read log file 'data.csv' with custom delimiter '|', parse into data_clean, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('data.csv', sep='|', names=['timestamp','level','message'])\ndata_clean['timestamp'] = pd.to_datetime(data_clean['timestamp'])\nerrors = data_clean[data_clean['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load JSON file 'data.xlsx' into DataFrame df_data, normalize nested records, filter rows where 'score' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndf_data = pd.read_json('data.xlsx')\ndf_data = pd.json_normalize(df_data.to_dict(orient='records'))\ndf_data = df_data[df_data['score'] > 0]\ndf_data.to_parquet('clean_data.csv')"}
{"instruction": "Load time series CSV 'data.csv', set 'timestamp' as index, resample weekly average of 'weight', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('data.csv', parse_dates=['timestamp'], index_col='timestamp')\nweekly = data_raw['weight'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly weight with Anomalies')\nplt.show()"}
{"instruction": "Read log file 'data.csv' with custom delimiter '|', parse into data_clean, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('data.csv', sep='|', names=['timestamp','level','message'])\ndata_clean['timestamp'] = pd.to_datetime(data_clean['timestamp'])\nerrors = data_clean[data_clean['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load JSON file 'time_series.csv' into DataFrame data, normalize nested records, filter rows where 'income' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndata = pd.read_json('time_series.csv')\ndata = pd.json_normalize(data.to_dict(orient='records'))\ndata = data[data['income'] > 0]\ndata.to_parquet('output.parquet')"}
{"instruction": "Read Excel file 'time_series.csv' sheet 'Sheet1', concatenate sheets into df_data, pivot on 'region' and 'age', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('time_series.csv', sheet_name=None)\ndf_data = pd.concat(sheets.values(), ignore_index=True)\npivot = df_data.pivot_table(index='region', columns='age', values='age', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read Excel file 'data.csv' sheet 'Report', concatenate sheets into df_data, pivot on 'status' and 'height', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.csv', sheet_name=None)\ndf_data = pd.concat(sheets.values(), ignore_index=True)\npivot = df_data.pivot_table(index='status', columns='height', values='height', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Filter dataset for rows where 'salary' contains 'A', then encode 'department' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndataset = dataset[dataset['salary'].str.contains('A', na=False)]\ndataset_ohe = pd.get_dummies(dataset, columns=['department'])\ntrain, test = train_test_split(dataset_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read log file 'input.json' with custom delimiter '|', parse into sales_df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('input.json', sep='|', names=['timestamp','level','message'])\nsales_df['timestamp'] = pd.to_datetime(sales_df['timestamp'])\nerrors = sales_df[sales_df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load JSON file 'input.json' into DataFrame data, normalize nested records, filter rows where 'score' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndata = pd.read_json('input.json')\ndata = pd.json_normalize(data.to_dict(orient='records'))\ndata = data[data['score'] > 0]\ndata.to_parquet('clean_data.csv')"}
{"instruction": "Load JSON file 'logfile.log' into DataFrame data_clean, normalize nested records, filter rows where 'order_date' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndata_clean = pd.read_json('logfile.log')\ndata_clean = pd.json_normalize(data_clean.to_dict(orient='records'))\ndata_clean = data_clean[data_clean['order_date'] > 0]\ndata_clean.to_parquet('clean_data.csv')"}
{"instruction": "Parse 'timestamp' in data_raw as datetime, extract year and month, compute monthly sum of 'revenue', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw['timestamp'] = pd.to_datetime(data_raw['timestamp'])\ndata_raw['year_month'] = data_raw['timestamp'].dt.to_period('M')\nmonthly = data_raw.groupby('year_month')['revenue'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of revenue')\nplt.show()"}
{"instruction": "Parse 'order_date' in df_sales as datetime, extract year and month, compute monthly sum of 'value', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales['order_date'] = pd.to_datetime(df_sales['order_date'])\ndf_sales['year_month'] = df_sales['order_date'].dt.to_period('M')\nmonthly = df_sales.groupby('year_month')['value'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of value')\nplt.show()"}
{"instruction": "Load time series CSV 'logfile.log', set 'date' as index, resample weekly average of 'height', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('logfile.log', parse_dates=['date'], index_col='date')\nweekly = sales_df['height'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly height with Anomalies')\nplt.show()"}
{"instruction": "Filter data_raw for rows where 'revenue' contains 'A', then encode 'class' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata_raw = data_raw[data_raw['revenue'].str.contains('A', na=False)]\ndata_raw_ohe = pd.get_dummies(data_raw, columns=['class'])\ntrain, test = train_test_split(data_raw_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read log file 'sales.csv' with custom delimiter '|', parse into df_sales, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('sales.csv', sep='|', names=['timestamp','level','message'])\ndf_sales['timestamp'] = pd.to_datetime(df_sales['timestamp'])\nerrors = df_sales[df_sales['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into sales_df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\nsales_df['timestamp'] = pd.to_datetime(sales_df['timestamp'])\nerrors = sales_df[sales_df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['sales.csv', 'time_series.csv', 'data.xlsx'] into df_sales, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['sales.csv', 'time_series.csv', 'data.xlsx'])\ndf_sales = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf_sales.columns = df_sales.columns.str.lower()\nnull_frac = df_sales.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf_sales.drop(columns=cols_to_drop, inplace=True)\ndf_sales.to_csv('clean_data.csv', index=False)"}
{"instruction": "Load JSON file 'time_series.csv' into DataFrame df2, normalize nested records, filter rows where 'category' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndf2 = pd.read_json('time_series.csv')\ndf2 = pd.json_normalize(df2.to_dict(orient='records'))\ndf2 = df2[df2['category'] > 0]\ndf2.to_parquet('clean_data.csv')"}
{"instruction": "Read log file 'logfile.log' with custom delimiter '|', parse into data_clean, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('logfile.log', sep='|', names=['timestamp','level','message'])\ndata_clean['timestamp'] = pd.to_datetime(data_clean['timestamp'])\nerrors = data_clean[data_clean['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read CSV file 'time_series.csv' into DataFrame df1, drop duplicates, fill missing values in 'region' with median, group by 'group' and plot average 'value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('time_series.csv')\ndf1 = df1.drop_duplicates()\ndf1['region'] = df1['region'].fillna(df1['region'].median())\navg = df1.groupby('group')['value'].mean()\navg.plot(kind='bar')\nplt.title('Average value by group')\nplt.show()"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into data_raw, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\ndata_raw['timestamp'] = pd.to_datetime(data_raw['timestamp'])\nerrors = data_raw[data_raw['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load JSON file 'input.json' into DataFrame dataset, normalize nested records, filter rows where 'category' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndataset = pd.read_json('input.json')\ndataset = pd.json_normalize(dataset.to_dict(orient='records'))\ndataset = dataset[dataset['category'] > 0]\ndataset.to_parquet('results.parquet')"}
{"instruction": "Filter df2 for rows where 'value' contains 'A', then encode 'class' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf2 = df2[df2['value'].str.contains('A', na=False)]\ndf2_ohe = pd.get_dummies(df2, columns=['class'])\ntrain, test = train_test_split(df2_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read CSV file 'input.json' into DataFrame data_clean, drop duplicates, fill missing values in 'height' with median, group by 'type' and plot average 'rating'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('input.json')\ndata_clean = data_clean.drop_duplicates()\ndata_clean['height'] = data_clean['height'].fillna(data_clean['height'].median())\navg = data_clean.groupby('type')['rating'].mean()\navg.plot(kind='bar')\nplt.title('Average rating by type')\nplt.show()"}
{"instruction": "Parse 'sale_date' in data as datetime, extract year and month, compute monthly sum of 'sales', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata['sale_date'] = pd.to_datetime(data['sale_date'])\ndata['year_month'] = data['sale_date'].dt.to_period('M')\nmonthly = data.groupby('year_month')['sales'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of sales')\nplt.show()"}
{"instruction": "Parse 'sale_date' in data_raw as datetime, extract year and month, compute monthly sum of 'duration', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw['sale_date'] = pd.to_datetime(data_raw['sale_date'])\ndata_raw['year_month'] = data_raw['sale_date'].dt.to_period('M')\nmonthly = data_raw.groupby('year_month')['duration'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of duration')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['data.csv', 'data.xlsx', 'time_series.csv'] into df2, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.csv', 'data.xlsx', 'time_series.csv'])\ndf2 = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf2.columns = df2.columns.str.lower()\nnull_frac = df2.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf2.drop(columns=cols_to_drop, inplace=True)\ndf2.to_csv('clean_data.csv', index=False)"}
{"instruction": "Concatenate multiple CSV files ['data.csv', 'logfile.log', 'sales.csv'] into df2, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.csv', 'logfile.log', 'sales.csv'])\ndf2 = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf2.columns = df2.columns.str.lower()\nnull_frac = df2.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf2.drop(columns=cols_to_drop, inplace=True)\ndf2.to_csv('clean_data.csv', index=False)"}
{"instruction": "Concatenate multiple CSV files ['data.xlsx', 'time_series.csv', 'logfile.log'] into sales_df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.xlsx', 'time_series.csv', 'logfile.log'])\nsales_df = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\nsales_df.columns = sales_df.columns.str.lower()\nnull_frac = sales_df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\nsales_df.drop(columns=cols_to_drop, inplace=True)\nsales_df.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read CSV file 'data.xlsx' into DataFrame df1, drop duplicates, fill missing values in 'score' with median, group by 'group' and plot average 'duration'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('data.xlsx')\ndf1 = df1.drop_duplicates()\ndf1['score'] = df1['score'].fillna(df1['score'].median())\navg = df1.groupby('group')['duration'].mean()\navg.plot(kind='bar')\nplt.title('Average duration by group')\nplt.show()"}
{"instruction": "Read CSV file 'data.csv' into DataFrame df, drop duplicates, fill missing values in 'price' with median, group by 'city' and plot average 'age'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('data.csv')\ndf = df.drop_duplicates()\ndf['price'] = df['price'].fillna(df['price'].median())\navg = df.groupby('city')['age'].mean()\navg.plot(kind='bar')\nplt.title('Average age by city')\nplt.show()"}
{"instruction": "Load JSON file 'data.csv' into DataFrame df1, normalize nested records, filter rows where 'count' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndf1 = pd.read_json('data.csv')\ndf1 = pd.json_normalize(df1.to_dict(orient='records'))\ndf1 = df1[df1['count'] > 0]\ndf1.to_parquet('clean_data.csv')"}
{"instruction": "Read log file 'logfile.log' with custom delimiter '|', parse into df_data, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('logfile.log', sep='|', names=['timestamp','level','message'])\ndf_data['timestamp'] = pd.to_datetime(df_data['timestamp'])\nerrors = df_data[df_data['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Filter df_sales for rows where 'count' contains 'A', then encode 'city' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_sales = df_sales[df_sales['count'].str.contains('A', na=False)]\ndf_sales_ohe = pd.get_dummies(df_sales, columns=['city'])\ntrain, test = train_test_split(df_sales_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Load JSON file 'sales.csv' into DataFrame df1, normalize nested records, filter rows where 'order_date' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndf1 = pd.read_json('sales.csv')\ndf1 = pd.json_normalize(df1.to_dict(orient='records'))\ndf1 = df1[df1['order_date'] > 0]\ndf1.to_parquet('clean_data.csv')"}
{"instruction": "Load time series CSV 'data.xlsx', set 'sale_date' as index, resample weekly average of 'rating', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('data.xlsx', parse_dates=['sale_date'], index_col='sale_date')\nweekly = dataset['rating'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly rating with Anomalies')\nplt.show()"}
{"instruction": "Load JSON file 'sales.csv' into DataFrame dataset, normalize nested records, filter rows where 'department' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndataset = pd.read_json('sales.csv')\ndataset = pd.json_normalize(dataset.to_dict(orient='records'))\ndataset = dataset[dataset['department'] > 0]\ndataset.to_parquet('results.parquet')"}
{"instruction": "Load time series CSV 'data.csv', set 'sale_date' as index, resample weekly average of 'count', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('data.csv', parse_dates=['sale_date'], index_col='sale_date')\nweekly = sales_df['count'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly count with Anomalies')\nplt.show()"}
{"instruction": "Filter data_clean for rows where 'salary' contains 'A', then encode 'class' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata_clean = data_clean[data_clean['salary'].str.contains('A', na=False)]\ndata_clean_ohe = pd.get_dummies(data_clean, columns=['class'])\ntrain, test = train_test_split(data_clean_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Filter data_clean for rows where 'income' contains 'A', then encode 'type' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata_clean = data_clean[data_clean['income'].str.contains('A', na=False)]\ndata_clean_ohe = pd.get_dummies(data_clean, columns=['type'])\ntrain, test = train_test_split(data_clean_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Concatenate multiple CSV files ['data.xlsx', 'logfile.log', 'time_series.csv'] into df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.xlsx', 'logfile.log', 'time_series.csv'])\ndf = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf.columns = df.columns.str.lower()\nnull_frac = df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf.drop(columns=cols_to_drop, inplace=True)\ndf.to_csv('clean_data.csv', index=False)"}
{"instruction": "Filter df1 for rows where 'height' contains 'A', then encode 'region' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf1 = df1[df1['height'].str.contains('A', na=False)]\ndf1_ohe = pd.get_dummies(df1, columns=['region'])\ntrain, test = train_test_split(df1_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Parse 'date' in data_clean as datetime, extract year and month, compute monthly sum of 'quantity', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean['date'] = pd.to_datetime(data_clean['date'])\ndata_clean['year_month'] = data_clean['date'].dt.to_period('M')\nmonthly = data_clean.groupby('year_month')['quantity'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of quantity')\nplt.show()"}
{"instruction": "Read Excel file 'input.json' sheet 'Data', concatenate sheets into sales_df, pivot on 'status' and 'price', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('input.json', sheet_name=None)\nsales_df = pd.concat(sheets.values(), ignore_index=True)\npivot = sales_df.pivot_table(index='status', columns='price', values='price', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['sales.csv', 'time_series.csv', 'data.xlsx'] into data, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['sales.csv', 'time_series.csv', 'data.xlsx'])\ndata = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata.columns = data.columns.str.lower()\nnull_frac = data.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata.drop(columns=cols_to_drop, inplace=True)\ndata.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read CSV file 'time_series.csv' into DataFrame dataset, drop duplicates, fill missing values in 'height' with median, group by 'department' and plot average 'weight'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('time_series.csv')\ndataset = dataset.drop_duplicates()\ndataset['height'] = dataset['height'].fillna(dataset['height'].median())\navg = dataset.groupby('department')['weight'].mean()\navg.plot(kind='bar')\nplt.title('Average weight by department')\nplt.show()"}
{"instruction": "Parse 'order_date' in df2 as datetime, extract year and month, compute monthly sum of 'height', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2['order_date'] = pd.to_datetime(df2['order_date'])\ndf2['year_month'] = df2['order_date'].dt.to_period('M')\nmonthly = df2.groupby('year_month')['height'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of height')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['data.xlsx', 'input.json', 'time_series.csv'] into df_data, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.xlsx', 'input.json', 'time_series.csv'])\ndf_data = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf_data.columns = df_data.columns.str.lower()\nnull_frac = df_data.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf_data.drop(columns=cols_to_drop, inplace=True)\ndf_data.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read Excel file 'logfile.log' sheet 'Sheet1', concatenate sheets into data_clean, pivot on 'category' and 'count', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('logfile.log', sheet_name=None)\ndata_clean = pd.concat(sheets.values(), ignore_index=True)\npivot = data_clean.pivot_table(index='category', columns='count', values='count', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['input.json', 'time_series.csv', 'sales.csv'] into data_clean, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['input.json', 'time_series.csv', 'sales.csv'])\ndata_clean = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata_clean.columns = data_clean.columns.str.lower()\nnull_frac = data_clean.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata_clean.drop(columns=cols_to_drop, inplace=True)\ndata_clean.to_csv('clean_data.csv', index=False)"}
{"instruction": "Parse 'timestamp' in data_raw as datetime, extract year and month, compute monthly sum of 'count', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw['timestamp'] = pd.to_datetime(data_raw['timestamp'])\ndata_raw['year_month'] = data_raw['timestamp'].dt.to_period('M')\nmonthly = data_raw.groupby('year_month')['count'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of count')\nplt.show()"}
{"instruction": "Read CSV file 'sales.csv' into DataFrame data_raw, drop duplicates, fill missing values in 'height' with median, group by 'class' and plot average 'weight'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('sales.csv')\ndata_raw = data_raw.drop_duplicates()\ndata_raw['height'] = data_raw['height'].fillna(data_raw['height'].median())\navg = data_raw.groupby('class')['weight'].mean()\navg.plot(kind='bar')\nplt.title('Average weight by class')\nplt.show()"}
{"instruction": "Parse 'order_date' in sales_df as datetime, extract year and month, compute monthly sum of 'quantity', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df['order_date'] = pd.to_datetime(sales_df['order_date'])\nsales_df['year_month'] = sales_df['order_date'].dt.to_period('M')\nmonthly = sales_df.groupby('year_month')['quantity'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of quantity')\nplt.show()"}
{"instruction": "Load time series CSV 'logfile.log', set 'order_date' as index, resample weekly average of 'quantity', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('logfile.log', parse_dates=['order_date'], index_col='order_date')\nweekly = df_data['quantity'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly quantity with Anomalies')\nplt.show()"}
{"instruction": "Read CSV file 'data.xlsx' into DataFrame df_sales, drop duplicates, fill missing values in 'rating' with median, group by 'department' and plot average 'value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('data.xlsx')\ndf_sales = df_sales.drop_duplicates()\ndf_sales['rating'] = df_sales['rating'].fillna(df_sales['rating'].median())\navg = df_sales.groupby('department')['value'].mean()\navg.plot(kind='bar')\nplt.title('Average value by department')\nplt.show()"}
{"instruction": "Load time series CSV 'time_series.csv', set 'order_date' as index, resample weekly average of 'revenue', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('time_series.csv', parse_dates=['order_date'], index_col='order_date')\nweekly = data['revenue'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly revenue with Anomalies')\nplt.show()"}
{"instruction": "Read log file 'sales.csv' with custom delimiter '|', parse into data_clean, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('sales.csv', sep='|', names=['timestamp','level','message'])\ndata_clean['timestamp'] = pd.to_datetime(data_clean['timestamp'])\nerrors = data_clean[data_clean['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read log file 'time_series.csv' with custom delimiter '|', parse into dataset, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('time_series.csv', sep='|', names=['timestamp','level','message'])\ndataset['timestamp'] = pd.to_datetime(dataset['timestamp'])\nerrors = dataset[dataset['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['time_series.csv', 'data.xlsx', 'input.json'] into df_sales, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['time_series.csv', 'data.xlsx', 'input.json'])\ndf_sales = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf_sales.columns = df_sales.columns.str.lower()\nnull_frac = df_sales.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf_sales.drop(columns=cols_to_drop, inplace=True)\ndf_sales.to_csv('clean_data.csv', index=False)"}
{"instruction": "Filter data_raw for rows where 'class' contains 'A', then encode 'class' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata_raw = data_raw[data_raw['class'].str.contains('A', na=False)]\ndata_raw_ohe = pd.get_dummies(data_raw, columns=['class'])\ntrain, test = train_test_split(data_raw_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read CSV file 'sales.csv' into DataFrame df1, drop duplicates, fill missing values in 'quantity' with median, group by 'region' and plot average 'height'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('sales.csv')\ndf1 = df1.drop_duplicates()\ndf1['quantity'] = df1['quantity'].fillna(df1['quantity'].median())\navg = df1.groupby('region')['height'].mean()\navg.plot(kind='bar')\nplt.title('Average height by region')\nplt.show()"}
{"instruction": "Read CSV file 'data.csv' into DataFrame df2, drop duplicates, fill missing values in 'price' with median, group by 'department' and plot average 'height'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('data.csv')\ndf2 = df2.drop_duplicates()\ndf2['price'] = df2['price'].fillna(df2['price'].median())\navg = df2.groupby('department')['height'].mean()\navg.plot(kind='bar')\nplt.title('Average height by department')\nplt.show()"}
{"instruction": "Read Excel file 'data.xlsx' sheet 'Report', concatenate sheets into df2, pivot on 'type' and 'revenue', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.xlsx', sheet_name=None)\ndf2 = pd.concat(sheets.values(), ignore_index=True)\npivot = df2.pivot_table(index='type', columns='revenue', values='revenue', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Filter df2 for rows where 'duration' contains 'A', then encode 'region' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf2 = df2[df2['duration'].str.contains('A', na=False)]\ndf2_ohe = pd.get_dummies(df2, columns=['region'])\ntrain, test = train_test_split(df2_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read Excel file 'input.json' sheet 'Data', concatenate sheets into data_clean, pivot on 'region' and 'salary', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('input.json', sheet_name=None)\ndata_clean = pd.concat(sheets.values(), ignore_index=True)\npivot = data_clean.pivot_table(index='region', columns='salary', values='salary', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Load JSON file 'data.csv' into DataFrame dataset, normalize nested records, filter rows where 'speed' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndataset = pd.read_json('data.csv')\ndataset = pd.json_normalize(dataset.to_dict(orient='records'))\ndataset = dataset[dataset['speed'] > 0]\ndataset.to_parquet('output.parquet')"}
{"instruction": "Load time series CSV 'time_series.csv', set 'timestamp' as index, resample weekly average of 'score', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('time_series.csv', parse_dates=['timestamp'], index_col='timestamp')\nweekly = sales_df['score'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly score with Anomalies')\nplt.show()"}
{"instruction": "Load time series CSV 'input.json', set 'sale_date' as index, resample weekly average of 'value', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('input.json', parse_dates=['sale_date'], index_col='sale_date')\nweekly = df_data['value'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly value with Anomalies')\nplt.show()"}
{"instruction": "Load time series CSV 'sales.csv', set 'date' as index, resample weekly average of 'weight', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('sales.csv', parse_dates=['date'], index_col='date')\nweekly = data_raw['weight'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly weight with Anomalies')\nplt.show()"}
{"instruction": "Load time series CSV 'data.xlsx', set 'sale_date' as index, resample weekly average of 'weight', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('data.xlsx', parse_dates=['sale_date'], index_col='sale_date')\nweekly = data_clean['weight'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly weight with Anomalies')\nplt.show()"}
{"instruction": "Load time series CSV 'data.xlsx', set 'timestamp' as index, resample weekly average of 'height', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('data.xlsx', parse_dates=['timestamp'], index_col='timestamp')\nweekly = df_sales['height'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly height with Anomalies')\nplt.show()"}
{"instruction": "Filter df_data for rows where 'class' contains 'A', then encode 'class' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_data = df_data[df_data['class'].str.contains('A', na=False)]\ndf_data_ohe = pd.get_dummies(df_data, columns=['class'])\ntrain, test = train_test_split(df_data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Filter dataset for rows where 'group' contains 'A', then encode 'department' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndataset = dataset[dataset['group'].str.contains('A', na=False)]\ndataset_ohe = pd.get_dummies(dataset, columns=['department'])\ntrain, test = train_test_split(dataset_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read CSV file 'logfile.log' into DataFrame df1, drop duplicates, fill missing values in 'count' with median, group by 'group' and plot average 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('logfile.log')\ndf1 = df1.drop_duplicates()\ndf1['count'] = df1['count'].fillna(df1['count'].median())\navg = df1.groupby('group')['sales'].mean()\navg.plot(kind='bar')\nplt.title('Average sales by group')\nplt.show()"}
{"instruction": "Load JSON file 'input.json' into DataFrame df_data, normalize nested records, filter rows where 'speed' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndf_data = pd.read_json('input.json')\ndf_data = pd.json_normalize(df_data.to_dict(orient='records'))\ndf_data = df_data[df_data['speed'] > 0]\ndf_data.to_parquet('clean_data.csv')"}
{"instruction": "Parse 'timestamp' in df2 as datetime, extract year and month, compute monthly sum of 'sales', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2['timestamp'] = pd.to_datetime(df2['timestamp'])\ndf2['year_month'] = df2['timestamp'].dt.to_period('M')\nmonthly = df2.groupby('year_month')['sales'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of sales')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['time_series.csv', 'logfile.log', 'input.json'] into df2, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['time_series.csv', 'logfile.log', 'input.json'])\ndf2 = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf2.columns = df2.columns.str.lower()\nnull_frac = df2.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf2.drop(columns=cols_to_drop, inplace=True)\ndf2.to_csv('clean_data.csv', index=False)"}
{"instruction": "Load JSON file 'input.json' into DataFrame data_clean, normalize nested records, filter rows where 'duration' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndata_clean = pd.read_json('input.json')\ndata_clean = pd.json_normalize(data_clean.to_dict(orient='records'))\ndata_clean = data_clean[data_clean['duration'] > 0]\ndata_clean.to_parquet('clean_data.csv')"}
{"instruction": "Read Excel file 'data.xlsx' sheet 'Report', concatenate sheets into dataset, pivot on 'class' and 'duration', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.xlsx', sheet_name=None)\ndataset = pd.concat(sheets.values(), ignore_index=True)\npivot = dataset.pivot_table(index='class', columns='duration', values='duration', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Filter data_raw for rows where 'group' contains 'A', then encode 'city' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata_raw = data_raw[data_raw['group'].str.contains('A', na=False)]\ndata_raw_ohe = pd.get_dummies(data_raw, columns=['city'])\ntrain, test = train_test_split(data_raw_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read CSV file 'logfile.log' into DataFrame data_raw, drop duplicates, fill missing values in 'duration' with median, group by 'department' and plot average 'weight'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('logfile.log')\ndata_raw = data_raw.drop_duplicates()\ndata_raw['duration'] = data_raw['duration'].fillna(data_raw['duration'].median())\navg = data_raw.groupby('department')['weight'].mean()\navg.plot(kind='bar')\nplt.title('Average weight by department')\nplt.show()"}
{"instruction": "Filter df1 for rows where 'price' contains 'A', then encode 'city' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf1 = df1[df1['price'].str.contains('A', na=False)]\ndf1_ohe = pd.get_dummies(df1, columns=['city'])\ntrain, test = train_test_split(df1_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read Excel file 'sales.csv' sheet 'Report', concatenate sheets into data_clean, pivot on 'department' and 'age', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('sales.csv', sheet_name=None)\ndata_clean = pd.concat(sheets.values(), ignore_index=True)\npivot = data_clean.pivot_table(index='department', columns='age', values='age', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read Excel file 'logfile.log' sheet 'Sheet1', concatenate sheets into data_clean, pivot on 'status' and 'income', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('logfile.log', sheet_name=None)\ndata_clean = pd.concat(sheets.values(), ignore_index=True)\npivot = data_clean.pivot_table(index='status', columns='income', values='income', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read CSV file 'logfile.log' into DataFrame df_data, drop duplicates, fill missing values in 'sales' with median, group by 'class' and plot average 'value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('logfile.log')\ndf_data = df_data.drop_duplicates()\ndf_data['sales'] = df_data['sales'].fillna(df_data['sales'].median())\navg = df_data.groupby('class')['value'].mean()\navg.plot(kind='bar')\nplt.title('Average value by class')\nplt.show()"}
{"instruction": "Load JSON file 'input.json' into DataFrame df_sales, normalize nested records, filter rows where 'age' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndf_sales = pd.read_json('input.json')\ndf_sales = pd.json_normalize(df_sales.to_dict(orient='records'))\ndf_sales = df_sales[df_sales['age'] > 0]\ndf_sales.to_parquet('output.parquet')"}
{"instruction": "Read log file 'logfile.log' with custom delimiter '|', parse into sales_df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('logfile.log', sep='|', names=['timestamp','level','message'])\nsales_df['timestamp'] = pd.to_datetime(sales_df['timestamp'])\nerrors = sales_df[sales_df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['time_series.csv', 'sales.csv', 'data.xlsx'] into data_raw, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['time_series.csv', 'sales.csv', 'data.xlsx'])\ndata_raw = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata_raw.columns = data_raw.columns.str.lower()\nnull_frac = data_raw.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata_raw.drop(columns=cols_to_drop, inplace=True)\ndata_raw.to_csv('clean_data.csv', index=False)"}
{"instruction": "Filter data_raw for rows where 'age' contains 'A', then encode 'category' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata_raw = data_raw[data_raw['age'].str.contains('A', na=False)]\ndata_raw_ohe = pd.get_dummies(data_raw, columns=['category'])\ntrain, test = train_test_split(data_raw_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read CSV file 'data.csv' into DataFrame df2, drop duplicates, fill missing values in 'city' with median, group by 'status' and plot average 'quantity'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('data.csv')\ndf2 = df2.drop_duplicates()\ndf2['city'] = df2['city'].fillna(df2['city'].median())\navg = df2.groupby('status')['quantity'].mean()\navg.plot(kind='bar')\nplt.title('Average quantity by status')\nplt.show()"}
{"instruction": "Filter data_clean for rows where 'sale_date' contains 'A', then encode 'category' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata_clean = data_clean[data_clean['sale_date'].str.contains('A', na=False)]\ndata_clean_ohe = pd.get_dummies(data_clean, columns=['category'])\ntrain, test = train_test_split(data_clean_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Parse 'sale_date' in df2 as datetime, extract year and month, compute monthly sum of 'revenue', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2['sale_date'] = pd.to_datetime(df2['sale_date'])\ndf2['year_month'] = df2['sale_date'].dt.to_period('M')\nmonthly = df2.groupby('year_month')['revenue'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of revenue')\nplt.show()"}
{"instruction": "Filter df2 for rows where 'sale_date' contains 'A', then encode 'city' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf2 = df2[df2['sale_date'].str.contains('A', na=False)]\ndf2_ohe = pd.get_dummies(df2, columns=['city'])\ntrain, test = train_test_split(df2_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read CSV file 'data.csv' into DataFrame df_sales, drop duplicates, fill missing values in 'group' with median, group by 'city' and plot average 'value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('data.csv')\ndf_sales = df_sales.drop_duplicates()\ndf_sales['group'] = df_sales['group'].fillna(df_sales['group'].median())\navg = df_sales.groupby('city')['value'].mean()\navg.plot(kind='bar')\nplt.title('Average value by city')\nplt.show()"}
{"instruction": "Read CSV file 'input.json' into DataFrame dataset, drop duplicates, fill missing values in 'age' with median, group by 'city' and plot average 'value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('input.json')\ndataset = dataset.drop_duplicates()\ndataset['age'] = dataset['age'].fillna(dataset['age'].median())\navg = dataset.groupby('city')['value'].mean()\navg.plot(kind='bar')\nplt.title('Average value by city')\nplt.show()"}
{"instruction": "Load JSON file 'data.xlsx' into DataFrame sales_df, normalize nested records, filter rows where 'city' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\nsales_df = pd.read_json('data.xlsx')\nsales_df = pd.json_normalize(sales_df.to_dict(orient='records'))\nsales_df = sales_df[sales_df['city'] > 0]\nsales_df.to_parquet('clean_data.csv')"}
{"instruction": "Read CSV file 'data.xlsx' into DataFrame df2, drop duplicates, fill missing values in 'price' with median, group by 'group' and plot average 'revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('data.xlsx')\ndf2 = df2.drop_duplicates()\ndf2['price'] = df2['price'].fillna(df2['price'].median())\navg = df2.groupby('group')['revenue'].mean()\navg.plot(kind='bar')\nplt.title('Average revenue by group')\nplt.show()"}
{"instruction": "Read log file 'sales.csv' with custom delimiter '|', parse into df1, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('sales.csv', sep='|', names=['timestamp','level','message'])\ndf1['timestamp'] = pd.to_datetime(df1['timestamp'])\nerrors = df1[df1['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load time series CSV 'logfile.log', set 'timestamp' as index, resample weekly average of 'weight', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('logfile.log', parse_dates=['timestamp'], index_col='timestamp')\nweekly = data_raw['weight'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly weight with Anomalies')\nplt.show()"}
{"instruction": "Load JSON file 'sales.csv' into DataFrame sales_df, normalize nested records, filter rows where 'type' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\nsales_df = pd.read_json('sales.csv')\nsales_df = pd.json_normalize(sales_df.to_dict(orient='records'))\nsales_df = sales_df[sales_df['type'] > 0]\nsales_df.to_parquet('output.parquet')"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into data_raw, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\ndata_raw['timestamp'] = pd.to_datetime(data_raw['timestamp'])\nerrors = data_raw[data_raw['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Filter df1 for rows where 'region' contains 'A', then encode 'group' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf1 = df1[df1['region'].str.contains('A', na=False)]\ndf1_ohe = pd.get_dummies(df1, columns=['group'])\ntrain, test = train_test_split(df1_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Parse 'order_date' in data as datetime, extract year and month, compute monthly sum of 'duration', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata['order_date'] = pd.to_datetime(data['order_date'])\ndata['year_month'] = data['order_date'].dt.to_period('M')\nmonthly = data.groupby('year_month')['duration'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of duration')\nplt.show()"}
{"instruction": "Load time series CSV 'data.csv', set 'timestamp' as index, resample weekly average of 'income', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('data.csv', parse_dates=['timestamp'], index_col='timestamp')\nweekly = df1['income'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly income with Anomalies')\nplt.show()"}
{"instruction": "Load JSON file 'input.json' into DataFrame sales_df, normalize nested records, filter rows where 'revenue' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\nsales_df = pd.read_json('input.json')\nsales_df = pd.json_normalize(sales_df.to_dict(orient='records'))\nsales_df = sales_df[sales_df['revenue'] > 0]\nsales_df.to_parquet('output.parquet')"}
{"instruction": "Concatenate multiple CSV files ['sales.csv', 'logfile.log', 'data.xlsx'] into df2, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['sales.csv', 'logfile.log', 'data.xlsx'])\ndf2 = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf2.columns = df2.columns.str.lower()\nnull_frac = df2.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf2.drop(columns=cols_to_drop, inplace=True)\ndf2.to_csv('clean_data.csv', index=False)"}
{"instruction": "Parse 'order_date' in df as datetime, extract year and month, compute monthly sum of 'price', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf['order_date'] = pd.to_datetime(df['order_date'])\ndf['year_month'] = df['order_date'].dt.to_period('M')\nmonthly = df.groupby('year_month')['price'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of price')\nplt.show()"}
{"instruction": "Parse 'sale_date' in data as datetime, extract year and month, compute monthly sum of 'quantity', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata['sale_date'] = pd.to_datetime(data['sale_date'])\ndata['year_month'] = data['sale_date'].dt.to_period('M')\nmonthly = data.groupby('year_month')['quantity'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of quantity')\nplt.show()"}
{"instruction": "Load time series CSV 'sales.csv', set 'sale_date' as index, resample weekly average of 'score', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('sales.csv', parse_dates=['sale_date'], index_col='sale_date')\nweekly = data['score'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly score with Anomalies')\nplt.show()"}
{"instruction": "Read CSV file 'sales.csv' into DataFrame df, drop duplicates, fill missing values in 'duration' with median, group by 'group' and plot average 'duration'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales.csv')\ndf = df.drop_duplicates()\ndf['duration'] = df['duration'].fillna(df['duration'].median())\navg = df.groupby('group')['duration'].mean()\navg.plot(kind='bar')\nplt.title('Average duration by group')\nplt.show()"}
{"instruction": "Load time series CSV 'logfile.log', set 'date' as index, resample weekly average of 'duration', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('logfile.log', parse_dates=['date'], index_col='date')\nweekly = df['duration'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly duration with Anomalies')\nplt.show()"}
{"instruction": "Load JSON file 'data.xlsx' into DataFrame df_data, normalize nested records, filter rows where 'sale_date' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndf_data = pd.read_json('data.xlsx')\ndf_data = pd.json_normalize(df_data.to_dict(orient='records'))\ndf_data = df_data[df_data['sale_date'] > 0]\ndf_data.to_parquet('results.parquet')"}
{"instruction": "Read log file 'sales.csv' with custom delimiter '|', parse into df1, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('sales.csv', sep='|', names=['timestamp','level','message'])\ndf1['timestamp'] = pd.to_datetime(df1['timestamp'])\nerrors = df1[df1['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read Excel file 'logfile.log' sheet 'Report', concatenate sheets into df_sales, pivot on 'status' and 'duration', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('logfile.log', sheet_name=None)\ndf_sales = pd.concat(sheets.values(), ignore_index=True)\npivot = df_sales.pivot_table(index='status', columns='duration', values='duration', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Parse 'sale_date' in df1 as datetime, extract year and month, compute monthly sum of 'score', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1['sale_date'] = pd.to_datetime(df1['sale_date'])\ndf1['year_month'] = df1['sale_date'].dt.to_period('M')\nmonthly = df1.groupby('year_month')['score'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of score')\nplt.show()"}
{"instruction": "Read CSV file 'input.json' into DataFrame data_raw, drop duplicates, fill missing values in 'group' with median, group by 'category' and plot average 'height'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('input.json')\ndata_raw = data_raw.drop_duplicates()\ndata_raw['group'] = data_raw['group'].fillna(data_raw['group'].median())\navg = data_raw.groupby('category')['height'].mean()\navg.plot(kind='bar')\nplt.title('Average height by category')\nplt.show()"}
{"instruction": "Read CSV file 'time_series.csv' into DataFrame df_data, drop duplicates, fill missing values in 'income' with median, group by 'category' and plot average 'salary'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('time_series.csv')\ndf_data = df_data.drop_duplicates()\ndf_data['income'] = df_data['income'].fillna(df_data['income'].median())\navg = df_data.groupby('category')['salary'].mean()\navg.plot(kind='bar')\nplt.title('Average salary by category')\nplt.show()"}
{"instruction": "Load JSON file 'data.xlsx' into DataFrame df1, normalize nested records, filter rows where 'speed' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndf1 = pd.read_json('data.xlsx')\ndf1 = pd.json_normalize(df1.to_dict(orient='records'))\ndf1 = df1[df1['speed'] > 0]\ndf1.to_parquet('clean_data.csv')"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into sales_df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\nsales_df['timestamp'] = pd.to_datetime(sales_df['timestamp'])\nerrors = sales_df[sales_df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read Excel file 'data.xlsx' sheet 'Sheet1', concatenate sheets into df, pivot on 'category' and 'sales', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.xlsx', sheet_name=None)\ndf = pd.concat(sheets.values(), ignore_index=True)\npivot = df.pivot_table(index='category', columns='sales', values='sales', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Filter df1 for rows where 'group' contains 'A', then encode 'class' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf1 = df1[df1['group'].str.contains('A', na=False)]\ndf1_ohe = pd.get_dummies(df1, columns=['class'])\ntrain, test = train_test_split(df1_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Filter df_sales for rows where 'status' contains 'A', then encode 'department' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_sales = df_sales[df_sales['status'].str.contains('A', na=False)]\ndf_sales_ohe = pd.get_dummies(df_sales, columns=['department'])\ntrain, test = train_test_split(df_sales_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read CSV file 'data.csv' into DataFrame data_clean, drop duplicates, fill missing values in 'rating' with median, group by 'category' and plot average 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('data.csv')\ndata_clean = data_clean.drop_duplicates()\ndata_clean['rating'] = data_clean['rating'].fillna(data_clean['rating'].median())\navg = data_clean.groupby('category')['sales'].mean()\navg.plot(kind='bar')\nplt.title('Average sales by category')\nplt.show()"}
{"instruction": "Filter data for rows where 'score' contains 'A', then encode 'group' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata = data[data['score'].str.contains('A', na=False)]\ndata_ohe = pd.get_dummies(data, columns=['group'])\ntrain, test = train_test_split(data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\nerrors = df[df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read CSV file 'data.csv' into DataFrame df, drop duplicates, fill missing values in 'weight' with median, group by 'region' and plot average 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('data.csv')\ndf = df.drop_duplicates()\ndf['weight'] = df['weight'].fillna(df['weight'].median())\navg = df.groupby('region')['sales'].mean()\navg.plot(kind='bar')\nplt.title('Average sales by region')\nplt.show()"}
{"instruction": "Read log file 'logfile.log' with custom delimiter '|', parse into dataset, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('logfile.log', sep='|', names=['timestamp','level','message'])\ndataset['timestamp'] = pd.to_datetime(dataset['timestamp'])\nerrors = dataset[dataset['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read Excel file 'time_series.csv' sheet 'Sheet1', concatenate sheets into df_data, pivot on 'city' and 'age', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('time_series.csv', sheet_name=None)\ndf_data = pd.concat(sheets.values(), ignore_index=True)\npivot = df_data.pivot_table(index='city', columns='age', values='age', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read Excel file 'time_series.csv' sheet 'Sheet1', concatenate sheets into df2, pivot on 'category' and 'revenue', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('time_series.csv', sheet_name=None)\ndf2 = pd.concat(sheets.values(), ignore_index=True)\npivot = df2.pivot_table(index='category', columns='revenue', values='revenue', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read CSV file 'time_series.csv' into DataFrame sales_df, drop duplicates, fill missing values in 'height' with median, group by 'region' and plot average 'value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('time_series.csv')\nsales_df = sales_df.drop_duplicates()\nsales_df['height'] = sales_df['height'].fillna(sales_df['height'].median())\navg = sales_df.groupby('region')['value'].mean()\navg.plot(kind='bar')\nplt.title('Average value by region')\nplt.show()"}
{"instruction": "Load JSON file 'logfile.log' into DataFrame data, normalize nested records, filter rows where 'city' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndata = pd.read_json('logfile.log')\ndata = pd.json_normalize(data.to_dict(orient='records'))\ndata = data[data['city'] > 0]\ndata.to_parquet('clean_data.csv')"}
{"instruction": "Read log file 'data.csv' with custom delimiter '|', parse into df_data, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('data.csv', sep='|', names=['timestamp','level','message'])\ndf_data['timestamp'] = pd.to_datetime(df_data['timestamp'])\nerrors = df_data[df_data['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['input.json', 'time_series.csv', 'data.csv'] into data_raw, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['input.json', 'time_series.csv', 'data.csv'])\ndata_raw = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata_raw.columns = data_raw.columns.str.lower()\nnull_frac = data_raw.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata_raw.drop(columns=cols_to_drop, inplace=True)\ndata_raw.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read CSV file 'sales.csv' into DataFrame df_sales, drop duplicates, fill missing values in 'price' with median, group by 'department' and plot average 'score'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('sales.csv')\ndf_sales = df_sales.drop_duplicates()\ndf_sales['price'] = df_sales['price'].fillna(df_sales['price'].median())\navg = df_sales.groupby('department')['score'].mean()\navg.plot(kind='bar')\nplt.title('Average score by department')\nplt.show()"}
{"instruction": "Filter df2 for rows where 'sale_date' contains 'A', then encode 'class' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf2 = df2[df2['sale_date'].str.contains('A', na=False)]\ndf2_ohe = pd.get_dummies(df2, columns=['class'])\ntrain, test = train_test_split(df2_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read CSV file 'input.json' into DataFrame df1, drop duplicates, fill missing values in 'order_date' with median, group by 'department' and plot average 'price'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('input.json')\ndf1 = df1.drop_duplicates()\ndf1['order_date'] = df1['order_date'].fillna(df1['order_date'].median())\navg = df1.groupby('department')['price'].mean()\navg.plot(kind='bar')\nplt.title('Average price by department')\nplt.show()"}
{"instruction": "Read CSV file 'data.csv' into DataFrame data, drop duplicates, fill missing values in 'age' with median, group by 'department' and plot average 'weight'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('data.csv')\ndata = data.drop_duplicates()\ndata['age'] = data['age'].fillna(data['age'].median())\navg = data.groupby('department')['weight'].mean()\navg.plot(kind='bar')\nplt.title('Average weight by department')\nplt.show()"}
{"instruction": "Read Excel file 'data.xlsx' sheet 'Sheet1', concatenate sheets into df, pivot on 'category' and 'income', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.xlsx', sheet_name=None)\ndf = pd.concat(sheets.values(), ignore_index=True)\npivot = df.pivot_table(index='category', columns='income', values='income', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read log file 'time_series.csv' with custom delimiter '|', parse into data, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('time_series.csv', sep='|', names=['timestamp','level','message'])\ndata['timestamp'] = pd.to_datetime(data['timestamp'])\nerrors = data[data['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load time series CSV 'data.csv', set 'sale_date' as index, resample weekly average of 'score', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('data.csv', parse_dates=['sale_date'], index_col='sale_date')\nweekly = df2['score'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly score with Anomalies')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['logfile.log', 'input.json', 'time_series.csv'] into df_sales, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['logfile.log', 'input.json', 'time_series.csv'])\ndf_sales = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf_sales.columns = df_sales.columns.str.lower()\nnull_frac = df_sales.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf_sales.drop(columns=cols_to_drop, inplace=True)\ndf_sales.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read CSV file 'data.xlsx' into DataFrame df_sales, drop duplicates, fill missing values in 'city' with median, group by 'group' and plot average 'age'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('data.xlsx')\ndf_sales = df_sales.drop_duplicates()\ndf_sales['city'] = df_sales['city'].fillna(df_sales['city'].median())\navg = df_sales.groupby('group')['age'].mean()\navg.plot(kind='bar')\nplt.title('Average age by group')\nplt.show()"}
{"instruction": "Read log file 'sales.csv' with custom delimiter '|', parse into dataset, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('sales.csv', sep='|', names=['timestamp','level','message'])\ndataset['timestamp'] = pd.to_datetime(dataset['timestamp'])\nerrors = dataset[dataset['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read CSV file 'input.json' into DataFrame df_sales, drop duplicates, fill missing values in 'category' with median, group by 'group' and plot average 'quantity'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('input.json')\ndf_sales = df_sales.drop_duplicates()\ndf_sales['category'] = df_sales['category'].fillna(df_sales['category'].median())\navg = df_sales.groupby('group')['quantity'].mean()\navg.plot(kind='bar')\nplt.title('Average quantity by group')\nplt.show()"}
{"instruction": "Read CSV file 'time_series.csv' into DataFrame dataset, drop duplicates, fill missing values in 'score' with median, group by 'category' and plot average 'age'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('time_series.csv')\ndataset = dataset.drop_duplicates()\ndataset['score'] = dataset['score'].fillna(dataset['score'].median())\navg = dataset.groupby('category')['age'].mean()\navg.plot(kind='bar')\nplt.title('Average age by category')\nplt.show()"}
{"instruction": "Parse 'sale_date' in df1 as datetime, extract year and month, compute monthly sum of 'duration', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1['sale_date'] = pd.to_datetime(df1['sale_date'])\ndf1['year_month'] = df1['sale_date'].dt.to_period('M')\nmonthly = df1.groupby('year_month')['duration'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of duration')\nplt.show()"}
{"instruction": "Load time series CSV 'time_series.csv', set 'order_date' as index, resample weekly average of 'weight', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('time_series.csv', parse_dates=['order_date'], index_col='order_date')\nweekly = data_clean['weight'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly weight with Anomalies')\nplt.show()"}
{"instruction": "Load JSON file 'logfile.log' into DataFrame data_raw, normalize nested records, filter rows where 'price' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndata_raw = pd.read_json('logfile.log')\ndata_raw = pd.json_normalize(data_raw.to_dict(orient='records'))\ndata_raw = data_raw[data_raw['price'] > 0]\ndata_raw.to_parquet('clean_data.csv')"}
{"instruction": "Filter df1 for rows where 'salary' contains 'A', then encode 'region' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf1 = df1[df1['salary'].str.contains('A', na=False)]\ndf1_ohe = pd.get_dummies(df1, columns=['region'])\ntrain, test = train_test_split(df1_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Filter data_raw for rows where 'duration' contains 'A', then encode 'status' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata_raw = data_raw[data_raw['duration'].str.contains('A', na=False)]\ndata_raw_ohe = pd.get_dummies(data_raw, columns=['status'])\ntrain, test = train_test_split(data_raw_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Concatenate multiple CSV files ['logfile.log', 'data.xlsx', 'sales.csv'] into sales_df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['logfile.log', 'data.xlsx', 'sales.csv'])\nsales_df = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\nsales_df.columns = sales_df.columns.str.lower()\nnull_frac = sales_df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\nsales_df.drop(columns=cols_to_drop, inplace=True)\nsales_df.to_csv('clean_data.csv', index=False)"}
{"instruction": "Filter df_data for rows where 'price' contains 'A', then encode 'region' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_data = df_data[df_data['price'].str.contains('A', na=False)]\ndf_data_ohe = pd.get_dummies(df_data, columns=['region'])\ntrain, test = train_test_split(df_data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Load JSON file 'input.json' into DataFrame data_clean, normalize nested records, filter rows where 'rating' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndata_clean = pd.read_json('input.json')\ndata_clean = pd.json_normalize(data_clean.to_dict(orient='records'))\ndata_clean = data_clean[data_clean['rating'] > 0]\ndata_clean.to_parquet('results.parquet')"}
{"instruction": "Concatenate multiple CSV files ['data.csv', 'sales.csv', 'time_series.csv'] into df_sales, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.csv', 'sales.csv', 'time_series.csv'])\ndf_sales = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf_sales.columns = df_sales.columns.str.lower()\nnull_frac = df_sales.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf_sales.drop(columns=cols_to_drop, inplace=True)\ndf_sales.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read CSV file 'data.xlsx' into DataFrame df, drop duplicates, fill missing values in 'rating' with median, group by 'type' and plot average 'revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('data.xlsx')\ndf = df.drop_duplicates()\ndf['rating'] = df['rating'].fillna(df['rating'].median())\navg = df.groupby('type')['revenue'].mean()\navg.plot(kind='bar')\nplt.title('Average revenue by type')\nplt.show()"}
{"instruction": "Filter data_clean for rows where 'date' contains 'A', then encode 'type' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata_clean = data_clean[data_clean['date'].str.contains('A', na=False)]\ndata_clean_ohe = pd.get_dummies(data_clean, columns=['type'])\ntrain, test = train_test_split(data_clean_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read Excel file 'logfile.log' sheet 'Data', concatenate sheets into df1, pivot on 'department' and 'quantity', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('logfile.log', sheet_name=None)\ndf1 = pd.concat(sheets.values(), ignore_index=True)\npivot = df1.pivot_table(index='department', columns='quantity', values='quantity', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['sales.csv', 'time_series.csv', 'input.json'] into dataset, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['sales.csv', 'time_series.csv', 'input.json'])\ndataset = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndataset.columns = dataset.columns.str.lower()\nnull_frac = dataset.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndataset.drop(columns=cols_to_drop, inplace=True)\ndataset.to_csv('clean_data.csv', index=False)"}
{"instruction": "Concatenate multiple CSV files ['input.json', 'data.xlsx', 'data.csv'] into data, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['input.json', 'data.xlsx', 'data.csv'])\ndata = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata.columns = data.columns.str.lower()\nnull_frac = data.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata.drop(columns=cols_to_drop, inplace=True)\ndata.to_csv('clean_data.csv', index=False)"}
{"instruction": "Parse 'date' in data_clean as datetime, extract year and month, compute monthly sum of 'rating', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean['date'] = pd.to_datetime(data_clean['date'])\ndata_clean['year_month'] = data_clean['date'].dt.to_period('M')\nmonthly = data_clean.groupby('year_month')['rating'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of rating')\nplt.show()"}
{"instruction": "Load JSON file 'input.json' into DataFrame data_raw, normalize nested records, filter rows where 'sales' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndata_raw = pd.read_json('input.json')\ndata_raw = pd.json_normalize(data_raw.to_dict(orient='records'))\ndata_raw = data_raw[data_raw['sales'] > 0]\ndata_raw.to_parquet('results.parquet')"}
{"instruction": "Read Excel file 'data.xlsx' sheet 'Data', concatenate sheets into dataset, pivot on 'class' and 'duration', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.xlsx', sheet_name=None)\ndataset = pd.concat(sheets.values(), ignore_index=True)\npivot = dataset.pivot_table(index='class', columns='duration', values='duration', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Load time series CSV 'data.xlsx', set 'timestamp' as index, resample weekly average of 'quantity', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('data.xlsx', parse_dates=['timestamp'], index_col='timestamp')\nweekly = data_clean['quantity'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly quantity with Anomalies')\nplt.show()"}
{"instruction": "Load time series CSV 'data.csv', set 'order_date' as index, resample weekly average of 'income', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('data.csv', parse_dates=['order_date'], index_col='order_date')\nweekly = data['income'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly income with Anomalies')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['sales.csv', 'logfile.log', 'time_series.csv'] into data_raw, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['sales.csv', 'logfile.log', 'time_series.csv'])\ndata_raw = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata_raw.columns = data_raw.columns.str.lower()\nnull_frac = data_raw.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata_raw.drop(columns=cols_to_drop, inplace=True)\ndata_raw.to_csv('clean_data.csv', index=False)"}
{"instruction": "Load JSON file 'time_series.csv' into DataFrame df, normalize nested records, filter rows where 'date' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndf = pd.read_json('time_series.csv')\ndf = pd.json_normalize(df.to_dict(orient='records'))\ndf = df[df['date'] > 0]\ndf.to_parquet('output.parquet')"}
{"instruction": "Concatenate multiple CSV files ['time_series.csv', 'input.json', 'data.csv'] into df_data, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['time_series.csv', 'input.json', 'data.csv'])\ndf_data = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf_data.columns = df_data.columns.str.lower()\nnull_frac = df_data.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf_data.drop(columns=cols_to_drop, inplace=True)\ndf_data.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read Excel file 'data.csv' sheet 'Data', concatenate sheets into sales_df, pivot on 'type' and 'speed', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.csv', sheet_name=None)\nsales_df = pd.concat(sheets.values(), ignore_index=True)\npivot = sales_df.pivot_table(index='type', columns='speed', values='speed', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Load time series CSV 'sales.csv', set 'date' as index, resample weekly average of 'income', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('sales.csv', parse_dates=['date'], index_col='date')\nweekly = df2['income'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly income with Anomalies')\nplt.show()"}
{"instruction": "Filter data for rows where 'value' contains 'A', then encode 'status' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata = data[data['value'].str.contains('A', na=False)]\ndata_ohe = pd.get_dummies(data, columns=['status'])\ntrain, test = train_test_split(data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read CSV file 'sales.csv' into DataFrame dataset, drop duplicates, fill missing values in 'order_date' with median, group by 'region' and plot average 'quantity'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('sales.csv')\ndataset = dataset.drop_duplicates()\ndataset['order_date'] = dataset['order_date'].fillna(dataset['order_date'].median())\navg = dataset.groupby('region')['quantity'].mean()\navg.plot(kind='bar')\nplt.title('Average quantity by region')\nplt.show()"}
{"instruction": "Filter df_data for rows where 'quantity' contains 'A', then encode 'status' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_data = df_data[df_data['quantity'].str.contains('A', na=False)]\ndf_data_ohe = pd.get_dummies(df_data, columns=['status'])\ntrain, test = train_test_split(df_data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Parse 'date' in data_raw as datetime, extract year and month, compute monthly sum of 'sales', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw['date'] = pd.to_datetime(data_raw['date'])\ndata_raw['year_month'] = data_raw['date'].dt.to_period('M')\nmonthly = data_raw.groupby('year_month')['sales'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of sales')\nplt.show()"}
{"instruction": "Filter df_data for rows where 'type' contains 'A', then encode 'group' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_data = df_data[df_data['type'].str.contains('A', na=False)]\ndf_data_ohe = pd.get_dummies(df_data, columns=['group'])\ntrain, test = train_test_split(df_data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read CSV file 'input.json' into DataFrame df_sales, drop duplicates, fill missing values in 'rating' with median, group by 'class' and plot average 'revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('input.json')\ndf_sales = df_sales.drop_duplicates()\ndf_sales['rating'] = df_sales['rating'].fillna(df_sales['rating'].median())\navg = df_sales.groupby('class')['revenue'].mean()\navg.plot(kind='bar')\nplt.title('Average revenue by class')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['sales.csv', 'data.xlsx', 'input.json'] into df1, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['sales.csv', 'data.xlsx', 'input.json'])\ndf1 = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf1.columns = df1.columns.str.lower()\nnull_frac = df1.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf1.drop(columns=cols_to_drop, inplace=True)\ndf1.to_csv('clean_data.csv', index=False)"}
{"instruction": "Parse 'order_date' in sales_df as datetime, extract year and month, compute monthly sum of 'price', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df['order_date'] = pd.to_datetime(sales_df['order_date'])\nsales_df['year_month'] = sales_df['order_date'].dt.to_period('M')\nmonthly = sales_df.groupby('year_month')['price'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of price')\nplt.show()"}
{"instruction": "Filter dataset for rows where 'sale_date' contains 'A', then encode 'group' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndataset = dataset[dataset['sale_date'].str.contains('A', na=False)]\ndataset_ohe = pd.get_dummies(dataset, columns=['group'])\ntrain, test = train_test_split(dataset_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Filter df_data for rows where 'value' contains 'A', then encode 'type' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_data = df_data[df_data['value'].str.contains('A', na=False)]\ndf_data_ohe = pd.get_dummies(df_data, columns=['type'])\ntrain, test = train_test_split(df_data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read log file 'sales.csv' with custom delimiter '|', parse into df_data, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('sales.csv', sep='|', names=['timestamp','level','message'])\ndf_data['timestamp'] = pd.to_datetime(df_data['timestamp'])\nerrors = df_data[df_data['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Parse 'timestamp' in data as datetime, extract year and month, compute monthly sum of 'salary', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata['timestamp'] = pd.to_datetime(data['timestamp'])\ndata['year_month'] = data['timestamp'].dt.to_period('M')\nmonthly = data.groupby('year_month')['salary'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of salary')\nplt.show()"}
{"instruction": "Read CSV file 'data.csv' into DataFrame data, drop duplicates, fill missing values in 'timestamp' with median, group by 'department' and plot average 'revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('data.csv')\ndata = data.drop_duplicates()\ndata['timestamp'] = data['timestamp'].fillna(data['timestamp'].median())\navg = data.groupby('department')['revenue'].mean()\navg.plot(kind='bar')\nplt.title('Average revenue by department')\nplt.show()"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into data, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\ndata['timestamp'] = pd.to_datetime(data['timestamp'])\nerrors = data[data['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['input.json', 'logfile.log', 'time_series.csv'] into data_clean, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['input.json', 'logfile.log', 'time_series.csv'])\ndata_clean = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata_clean.columns = data_clean.columns.str.lower()\nnull_frac = data_clean.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata_clean.drop(columns=cols_to_drop, inplace=True)\ndata_clean.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read Excel file 'input.json' sheet 'Sheet1', concatenate sheets into sales_df, pivot on 'status' and 'age', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('input.json', sheet_name=None)\nsales_df = pd.concat(sheets.values(), ignore_index=True)\npivot = sales_df.pivot_table(index='status', columns='age', values='age', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Filter df_sales for rows where 'speed' contains 'A', then encode 'group' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_sales = df_sales[df_sales['speed'].str.contains('A', na=False)]\ndf_sales_ohe = pd.get_dummies(df_sales, columns=['group'])\ntrain, test = train_test_split(df_sales_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Load JSON file 'data.xlsx' into DataFrame data_raw, normalize nested records, filter rows where 'count' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndata_raw = pd.read_json('data.xlsx')\ndata_raw = pd.json_normalize(data_raw.to_dict(orient='records'))\ndata_raw = data_raw[data_raw['count'] > 0]\ndata_raw.to_parquet('clean_data.csv')"}
{"instruction": "Filter data for rows where 'count' contains 'A', then encode 'class' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata = data[data['count'].str.contains('A', na=False)]\ndata_ohe = pd.get_dummies(data, columns=['class'])\ntrain, test = train_test_split(data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Filter df_data for rows where 'department' contains 'A', then encode 'group' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_data = df_data[df_data['department'].str.contains('A', na=False)]\ndf_data_ohe = pd.get_dummies(df_data, columns=['group'])\ntrain, test = train_test_split(df_data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read CSV file 'input.json' into DataFrame dataset, drop duplicates, fill missing values in 'class' with median, group by 'status' and plot average 'duration'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('input.json')\ndataset = dataset.drop_duplicates()\ndataset['class'] = dataset['class'].fillna(dataset['class'].median())\navg = dataset.groupby('status')['duration'].mean()\navg.plot(kind='bar')\nplt.title('Average duration by status')\nplt.show()"}
{"instruction": "Load JSON file 'time_series.csv' into DataFrame df_sales, normalize nested records, filter rows where 'count' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndf_sales = pd.read_json('time_series.csv')\ndf_sales = pd.json_normalize(df_sales.to_dict(orient='records'))\ndf_sales = df_sales[df_sales['count'] > 0]\ndf_sales.to_parquet('clean_data.csv')"}
{"instruction": "Read Excel file 'time_series.csv' sheet 'Report', concatenate sheets into dataset, pivot on 'status' and 'count', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('time_series.csv', sheet_name=None)\ndataset = pd.concat(sheets.values(), ignore_index=True)\npivot = dataset.pivot_table(index='status', columns='count', values='count', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Load JSON file 'data.csv' into DataFrame df, normalize nested records, filter rows where 'duration' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndf = pd.read_json('data.csv')\ndf = pd.json_normalize(df.to_dict(orient='records'))\ndf = df[df['duration'] > 0]\ndf.to_parquet('output.parquet')"}
{"instruction": "Filter dataset for rows where 'class' contains 'A', then encode 'group' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndataset = dataset[dataset['class'].str.contains('A', na=False)]\ndataset_ohe = pd.get_dummies(dataset, columns=['group'])\ntrain, test = train_test_split(dataset_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Parse 'sale_date' in df1 as datetime, extract year and month, compute monthly sum of 'age', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1['sale_date'] = pd.to_datetime(df1['sale_date'])\ndf1['year_month'] = df1['sale_date'].dt.to_period('M')\nmonthly = df1.groupby('year_month')['age'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of age')\nplt.show()"}
{"instruction": "Read log file 'data.csv' with custom delimiter '|', parse into data, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('data.csv', sep='|', names=['timestamp','level','message'])\ndata['timestamp'] = pd.to_datetime(data['timestamp'])\nerrors = data[data['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read CSV file 'data.csv' into DataFrame dataset, drop duplicates, fill missing values in 'revenue' with median, group by 'status' and plot average 'speed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('data.csv')\ndataset = dataset.drop_duplicates()\ndataset['revenue'] = dataset['revenue'].fillna(dataset['revenue'].median())\navg = dataset.groupby('status')['speed'].mean()\navg.plot(kind='bar')\nplt.title('Average speed by status')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['logfile.log', 'input.json', 'time_series.csv'] into df1, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['logfile.log', 'input.json', 'time_series.csv'])\ndf1 = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf1.columns = df1.columns.str.lower()\nnull_frac = df1.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf1.drop(columns=cols_to_drop, inplace=True)\ndf1.to_csv('clean_data.csv', index=False)"}
{"instruction": "Concatenate multiple CSV files ['logfile.log', 'data.xlsx', 'data.csv'] into df_sales, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['logfile.log', 'data.xlsx', 'data.csv'])\ndf_sales = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf_sales.columns = df_sales.columns.str.lower()\nnull_frac = df_sales.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf_sales.drop(columns=cols_to_drop, inplace=True)\ndf_sales.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read log file 'data.csv' with custom delimiter '|', parse into dataset, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('data.csv', sep='|', names=['timestamp','level','message'])\ndataset['timestamp'] = pd.to_datetime(dataset['timestamp'])\nerrors = dataset[dataset['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read log file 'input.json' with custom delimiter '|', parse into df_data, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('input.json', sep='|', names=['timestamp','level','message'])\ndf_data['timestamp'] = pd.to_datetime(df_data['timestamp'])\nerrors = df_data[df_data['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load JSON file 'sales.csv' into DataFrame df_data, normalize nested records, filter rows where 'department' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndf_data = pd.read_json('sales.csv')\ndf_data = pd.json_normalize(df_data.to_dict(orient='records'))\ndf_data = df_data[df_data['department'] > 0]\ndf_data.to_parquet('clean_data.csv')"}
{"instruction": "Read Excel file 'data.xlsx' sheet 'Sheet1', concatenate sheets into df, pivot on 'class' and 'height', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.xlsx', sheet_name=None)\ndf = pd.concat(sheets.values(), ignore_index=True)\npivot = df.pivot_table(index='class', columns='height', values='height', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Load time series CSV 'data.xlsx', set 'order_date' as index, resample weekly average of 'duration', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('data.xlsx', parse_dates=['order_date'], index_col='order_date')\nweekly = data['duration'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly duration with Anomalies')\nplt.show()"}
{"instruction": "Read CSV file 'sales.csv' into DataFrame df_data, drop duplicates, fill missing values in 'score' with median, group by 'city' and plot average 'income'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('sales.csv')\ndf_data = df_data.drop_duplicates()\ndf_data['score'] = df_data['score'].fillna(df_data['score'].median())\navg = df_data.groupby('city')['income'].mean()\navg.plot(kind='bar')\nplt.title('Average income by city')\nplt.show()"}
{"instruction": "Read Excel file 'input.json' sheet 'Sheet1', concatenate sheets into df_data, pivot on 'category' and 'value', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('input.json', sheet_name=None)\ndf_data = pd.concat(sheets.values(), ignore_index=True)\npivot = df_data.pivot_table(index='category', columns='value', values='value', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['sales.csv', 'data.csv', 'input.json'] into data_raw, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['sales.csv', 'data.csv', 'input.json'])\ndata_raw = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata_raw.columns = data_raw.columns.str.lower()\nnull_frac = data_raw.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata_raw.drop(columns=cols_to_drop, inplace=True)\ndata_raw.to_csv('clean_data.csv', index=False)"}
{"instruction": "Load JSON file 'data.xlsx' into DataFrame df1, normalize nested records, filter rows where 'revenue' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndf1 = pd.read_json('data.xlsx')\ndf1 = pd.json_normalize(df1.to_dict(orient='records'))\ndf1 = df1[df1['revenue'] > 0]\ndf1.to_parquet('clean_data.csv')"}
{"instruction": "Filter df2 for rows where 'status' contains 'A', then encode 'group' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf2 = df2[df2['status'].str.contains('A', na=False)]\ndf2_ohe = pd.get_dummies(df2, columns=['group'])\ntrain, test = train_test_split(df2_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read log file 'time_series.csv' with custom delimiter '|', parse into data_raw, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('time_series.csv', sep='|', names=['timestamp','level','message'])\ndata_raw['timestamp'] = pd.to_datetime(data_raw['timestamp'])\nerrors = data_raw[data_raw['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into dataset, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\ndataset['timestamp'] = pd.to_datetime(dataset['timestamp'])\nerrors = dataset[dataset['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['time_series.csv', 'data.csv', 'input.json'] into sales_df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['time_series.csv', 'data.csv', 'input.json'])\nsales_df = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\nsales_df.columns = sales_df.columns.str.lower()\nnull_frac = sales_df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\nsales_df.drop(columns=cols_to_drop, inplace=True)\nsales_df.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read log file 'time_series.csv' with custom delimiter '|', parse into df_data, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('time_series.csv', sep='|', names=['timestamp','level','message'])\ndf_data['timestamp'] = pd.to_datetime(df_data['timestamp'])\nerrors = df_data[df_data['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['data.csv', 'input.json', 'time_series.csv'] into df_sales, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.csv', 'input.json', 'time_series.csv'])\ndf_sales = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf_sales.columns = df_sales.columns.str.lower()\nnull_frac = df_sales.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf_sales.drop(columns=cols_to_drop, inplace=True)\ndf_sales.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read CSV file 'sales.csv' into DataFrame sales_df, drop duplicates, fill missing values in 'group' with median, group by 'status' and plot average 'height'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('sales.csv')\nsales_df = sales_df.drop_duplicates()\nsales_df['group'] = sales_df['group'].fillna(sales_df['group'].median())\navg = sales_df.groupby('status')['height'].mean()\navg.plot(kind='bar')\nplt.title('Average height by status')\nplt.show()"}
{"instruction": "Filter df for rows where 'department' contains 'A', then encode 'class' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf = df[df['department'].str.contains('A', na=False)]\ndf_ohe = pd.get_dummies(df, columns=['class'])\ntrain, test = train_test_split(df_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Parse 'timestamp' in df as datetime, extract year and month, compute monthly sum of 'score', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\ndf['year_month'] = df['timestamp'].dt.to_period('M')\nmonthly = df.groupby('year_month')['score'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of score')\nplt.show()"}
{"instruction": "Read Excel file 'sales.csv' sheet 'Sheet1', concatenate sheets into data_clean, pivot on 'status' and 'score', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('sales.csv', sheet_name=None)\ndata_clean = pd.concat(sheets.values(), ignore_index=True)\npivot = data_clean.pivot_table(index='status', columns='score', values='score', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read CSV file 'data.xlsx' into DataFrame dataset, drop duplicates, fill missing values in 'count' with median, group by 'city' and plot average 'score'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('data.xlsx')\ndataset = dataset.drop_duplicates()\ndataset['count'] = dataset['count'].fillna(dataset['count'].median())\navg = dataset.groupby('city')['score'].mean()\navg.plot(kind='bar')\nplt.title('Average score by city')\nplt.show()"}
{"instruction": "Parse 'order_date' in df_sales as datetime, extract year and month, compute monthly sum of 'quantity', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales['order_date'] = pd.to_datetime(df_sales['order_date'])\ndf_sales['year_month'] = df_sales['order_date'].dt.to_period('M')\nmonthly = df_sales.groupby('year_month')['quantity'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of quantity')\nplt.show()"}
{"instruction": "Read log file 'sales.csv' with custom delimiter '|', parse into sales_df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('sales.csv', sep='|', names=['timestamp','level','message'])\nsales_df['timestamp'] = pd.to_datetime(sales_df['timestamp'])\nerrors = sales_df[sales_df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into data_clean, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\ndata_clean['timestamp'] = pd.to_datetime(data_clean['timestamp'])\nerrors = data_clean[data_clean['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load JSON file 'logfile.log' into DataFrame dataset, normalize nested records, filter rows where 'order_date' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndataset = pd.read_json('logfile.log')\ndataset = pd.json_normalize(dataset.to_dict(orient='records'))\ndataset = dataset[dataset['order_date'] > 0]\ndataset.to_parquet('output.parquet')"}
{"instruction": "Load JSON file 'data.csv' into DataFrame data_raw, normalize nested records, filter rows where 'timestamp' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndata_raw = pd.read_json('data.csv')\ndata_raw = pd.json_normalize(data_raw.to_dict(orient='records'))\ndata_raw = data_raw[data_raw['timestamp'] > 0]\ndata_raw.to_parquet('clean_data.csv')"}
{"instruction": "Parse 'timestamp' in data_clean as datetime, extract year and month, compute monthly sum of 'score', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean['timestamp'] = pd.to_datetime(data_clean['timestamp'])\ndata_clean['year_month'] = data_clean['timestamp'].dt.to_period('M')\nmonthly = data_clean.groupby('year_month')['score'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of score')\nplt.show()"}
{"instruction": "Read log file 'sales.csv' with custom delimiter '|', parse into sales_df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('sales.csv', sep='|', names=['timestamp','level','message'])\nsales_df['timestamp'] = pd.to_datetime(sales_df['timestamp'])\nerrors = sales_df[sales_df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Parse 'sale_date' in df_sales as datetime, extract year and month, compute monthly sum of 'count', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales['sale_date'] = pd.to_datetime(df_sales['sale_date'])\ndf_sales['year_month'] = df_sales['sale_date'].dt.to_period('M')\nmonthly = df_sales.groupby('year_month')['count'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of count')\nplt.show()"}
{"instruction": "Read CSV file 'data.csv' into DataFrame dataset, drop duplicates, fill missing values in 'sales' with median, group by 'type' and plot average 'score'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('data.csv')\ndataset = dataset.drop_duplicates()\ndataset['sales'] = dataset['sales'].fillna(dataset['sales'].median())\navg = dataset.groupby('type')['score'].mean()\navg.plot(kind='bar')\nplt.title('Average score by type')\nplt.show()"}
{"instruction": "Read CSV file 'data.xlsx' into DataFrame dataset, drop duplicates, fill missing values in 'duration' with median, group by 'category' and plot average 'count'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('data.xlsx')\ndataset = dataset.drop_duplicates()\ndataset['duration'] = dataset['duration'].fillna(dataset['duration'].median())\navg = dataset.groupby('category')['count'].mean()\navg.plot(kind='bar')\nplt.title('Average count by category')\nplt.show()"}
{"instruction": "Filter data_clean for rows where 'type' contains 'A', then encode 'class' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata_clean = data_clean[data_clean['type'].str.contains('A', na=False)]\ndata_clean_ohe = pd.get_dummies(data_clean, columns=['class'])\ntrain, test = train_test_split(data_clean_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read CSV file 'logfile.log' into DataFrame df_sales, drop duplicates, fill missing values in 'timestamp' with median, group by 'type' and plot average 'revenue'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('logfile.log')\ndf_sales = df_sales.drop_duplicates()\ndf_sales['timestamp'] = df_sales['timestamp'].fillna(df_sales['timestamp'].median())\navg = df_sales.groupby('type')['revenue'].mean()\navg.plot(kind='bar')\nplt.title('Average revenue by type')\nplt.show()"}
{"instruction": "Load time series CSV 'data.csv', set 'order_date' as index, resample weekly average of 'weight', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('data.csv', parse_dates=['order_date'], index_col='order_date')\nweekly = dataset['weight'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly weight with Anomalies')\nplt.show()"}
{"instruction": "Read log file 'logfile.log' with custom delimiter '|', parse into df_sales, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('logfile.log', sep='|', names=['timestamp','level','message'])\ndf_sales['timestamp'] = pd.to_datetime(df_sales['timestamp'])\nerrors = df_sales[df_sales['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load time series CSV 'input.json', set 'sale_date' as index, resample weekly average of 'score', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('input.json', parse_dates=['sale_date'], index_col='sale_date')\nweekly = df_data['score'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly score with Anomalies')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['logfile.log', 'data.xlsx', 'sales.csv'] into sales_df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['logfile.log', 'data.xlsx', 'sales.csv'])\nsales_df = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\nsales_df.columns = sales_df.columns.str.lower()\nnull_frac = sales_df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\nsales_df.drop(columns=cols_to_drop, inplace=True)\nsales_df.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read log file 'data.csv' with custom delimiter '|', parse into df_sales, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('data.csv', sep='|', names=['timestamp','level','message'])\ndf_sales['timestamp'] = pd.to_datetime(df_sales['timestamp'])\nerrors = df_sales[df_sales['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['data.xlsx', 'sales.csv', 'input.json'] into data_raw, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.xlsx', 'sales.csv', 'input.json'])\ndata_raw = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata_raw.columns = data_raw.columns.str.lower()\nnull_frac = data_raw.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata_raw.drop(columns=cols_to_drop, inplace=True)\ndata_raw.to_csv('clean_data.csv', index=False)"}
{"instruction": "Parse 'sale_date' in sales_df as datetime, extract year and month, compute monthly sum of 'revenue', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df['sale_date'] = pd.to_datetime(sales_df['sale_date'])\nsales_df['year_month'] = sales_df['sale_date'].dt.to_period('M')\nmonthly = sales_df.groupby('year_month')['revenue'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of revenue')\nplt.show()"}
{"instruction": "Read CSV file 'input.json' into DataFrame df_sales, drop duplicates, fill missing values in 'status' with median, group by 'city' and plot average 'speed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('input.json')\ndf_sales = df_sales.drop_duplicates()\ndf_sales['status'] = df_sales['status'].fillna(df_sales['status'].median())\navg = df_sales.groupby('city')['speed'].mean()\navg.plot(kind='bar')\nplt.title('Average speed by city')\nplt.show()"}
{"instruction": "Read Excel file 'input.json' sheet 'Data', concatenate sheets into dataset, pivot on 'status' and 'revenue', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('input.json', sheet_name=None)\ndataset = pd.concat(sheets.values(), ignore_index=True)\npivot = dataset.pivot_table(index='status', columns='revenue', values='revenue', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read log file 'input.json' with custom delimiter '|', parse into sales_df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('input.json', sep='|', names=['timestamp','level','message'])\nsales_df['timestamp'] = pd.to_datetime(sales_df['timestamp'])\nerrors = sales_df[sales_df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read CSV file 'time_series.csv' into DataFrame df1, drop duplicates, fill missing values in 'score' with median, group by 'status' and plot average 'age'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('time_series.csv')\ndf1 = df1.drop_duplicates()\ndf1['score'] = df1['score'].fillna(df1['score'].median())\navg = df1.groupby('status')['age'].mean()\navg.plot(kind='bar')\nplt.title('Average age by status')\nplt.show()"}
{"instruction": "Load time series CSV 'input.json', set 'order_date' as index, resample weekly average of 'price', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('input.json', parse_dates=['order_date'], index_col='order_date')\nweekly = data_clean['price'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly price with Anomalies')\nplt.show()"}
{"instruction": "Read Excel file 'logfile.log' sheet 'Data', concatenate sheets into data_clean, pivot on 'category' and 'score', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('logfile.log', sheet_name=None)\ndata_clean = pd.concat(sheets.values(), ignore_index=True)\npivot = data_clean.pivot_table(index='category', columns='score', values='score', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read Excel file 'time_series.csv' sheet 'Data', concatenate sheets into data, pivot on 'status' and 'speed', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('time_series.csv', sheet_name=None)\ndata = pd.concat(sheets.values(), ignore_index=True)\npivot = data.pivot_table(index='status', columns='speed', values='speed', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['logfile.log', 'sales.csv', 'data.xlsx'] into sales_df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['logfile.log', 'sales.csv', 'data.xlsx'])\nsales_df = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\nsales_df.columns = sales_df.columns.str.lower()\nnull_frac = sales_df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\nsales_df.drop(columns=cols_to_drop, inplace=True)\nsales_df.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read log file 'sales.csv' with custom delimiter '|', parse into df1, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('sales.csv', sep='|', names=['timestamp','level','message'])\ndf1['timestamp'] = pd.to_datetime(df1['timestamp'])\nerrors = df1[df1['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read CSV file 'logfile.log' into DataFrame data, drop duplicates, fill missing values in 'sale_date' with median, group by 'type' and plot average 'salary'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('logfile.log')\ndata = data.drop_duplicates()\ndata['sale_date'] = data['sale_date'].fillna(data['sale_date'].median())\navg = data.groupby('type')['salary'].mean()\navg.plot(kind='bar')\nplt.title('Average salary by type')\nplt.show()"}
{"instruction": "Read Excel file 'data.csv' sheet 'Data', concatenate sheets into df2, pivot on 'group' and 'duration', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.csv', sheet_name=None)\ndf2 = pd.concat(sheets.values(), ignore_index=True)\npivot = df2.pivot_table(index='group', columns='duration', values='duration', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Load JSON file 'time_series.csv' into DataFrame data, normalize nested records, filter rows where 'sale_date' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndata = pd.read_json('time_series.csv')\ndata = pd.json_normalize(data.to_dict(orient='records'))\ndata = data[data['sale_date'] > 0]\ndata.to_parquet('clean_data.csv')"}
{"instruction": "Read CSV file 'logfile.log' into DataFrame dataset, drop duplicates, fill missing values in 'type' with median, group by 'category' and plot average 'income'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('logfile.log')\ndataset = dataset.drop_duplicates()\ndataset['type'] = dataset['type'].fillna(dataset['type'].median())\navg = dataset.groupby('category')['income'].mean()\navg.plot(kind='bar')\nplt.title('Average income by category')\nplt.show()"}
{"instruction": "Parse 'order_date' in df2 as datetime, extract year and month, compute monthly sum of 'revenue', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2['order_date'] = pd.to_datetime(df2['order_date'])\ndf2['year_month'] = df2['order_date'].dt.to_period('M')\nmonthly = df2.groupby('year_month')['revenue'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of revenue')\nplt.show()"}
{"instruction": "Read Excel file 'sales.csv' sheet 'Report', concatenate sheets into df2, pivot on 'group' and 'quantity', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('sales.csv', sheet_name=None)\ndf2 = pd.concat(sheets.values(), ignore_index=True)\npivot = df2.pivot_table(index='group', columns='quantity', values='quantity', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Parse 'sale_date' in data as datetime, extract year and month, compute monthly sum of 'value', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata['sale_date'] = pd.to_datetime(data['sale_date'])\ndata['year_month'] = data['sale_date'].dt.to_period('M')\nmonthly = data.groupby('year_month')['value'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of value')\nplt.show()"}
{"instruction": "Read log file 'data.csv' with custom delimiter '|', parse into data_raw, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('data.csv', sep='|', names=['timestamp','level','message'])\ndata_raw['timestamp'] = pd.to_datetime(data_raw['timestamp'])\nerrors = data_raw[data_raw['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Filter df2 for rows where 'type' contains 'A', then encode 'class' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf2 = df2[df2['type'].str.contains('A', na=False)]\ndf2_ohe = pd.get_dummies(df2, columns=['class'])\ntrain, test = train_test_split(df2_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read Excel file 'logfile.log' sheet 'Sheet1', concatenate sheets into df_sales, pivot on 'region' and 'sales', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('logfile.log', sheet_name=None)\ndf_sales = pd.concat(sheets.values(), ignore_index=True)\npivot = df_sales.pivot_table(index='region', columns='sales', values='sales', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Parse 'date' in data_clean as datetime, extract year and month, compute monthly sum of 'income', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean['date'] = pd.to_datetime(data_clean['date'])\ndata_clean['year_month'] = data_clean['date'].dt.to_period('M')\nmonthly = data_clean.groupby('year_month')['income'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of income')\nplt.show()"}
{"instruction": "Read Excel file 'sales.csv' sheet 'Report', concatenate sheets into df_data, pivot on 'category' and 'salary', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('sales.csv', sheet_name=None)\ndf_data = pd.concat(sheets.values(), ignore_index=True)\npivot = df_data.pivot_table(index='category', columns='salary', values='salary', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read CSV file 'data.xlsx' into DataFrame df, drop duplicates, fill missing values in 'department' with median, group by 'group' and plot average 'speed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('data.xlsx')\ndf = df.drop_duplicates()\ndf['department'] = df['department'].fillna(df['department'].median())\navg = df.groupby('group')['speed'].mean()\navg.plot(kind='bar')\nplt.title('Average speed by group')\nplt.show()"}
{"instruction": "Read Excel file 'data.csv' sheet 'Data', concatenate sheets into df1, pivot on 'city' and 'height', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.csv', sheet_name=None)\ndf1 = pd.concat(sheets.values(), ignore_index=True)\npivot = df1.pivot_table(index='city', columns='height', values='height', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read log file 'data.csv' with custom delimiter '|', parse into df_data, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('data.csv', sep='|', names=['timestamp','level','message'])\ndf_data['timestamp'] = pd.to_datetime(df_data['timestamp'])\nerrors = df_data[df_data['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['time_series.csv', 'input.json', 'logfile.log'] into data_raw, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['time_series.csv', 'input.json', 'logfile.log'])\ndata_raw = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata_raw.columns = data_raw.columns.str.lower()\nnull_frac = data_raw.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata_raw.drop(columns=cols_to_drop, inplace=True)\ndata_raw.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into sales_df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\nsales_df['timestamp'] = pd.to_datetime(sales_df['timestamp'])\nerrors = sales_df[sales_df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Filter sales_df for rows where 'rating' contains 'A', then encode 'region' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nsales_df = sales_df[sales_df['rating'].str.contains('A', na=False)]\nsales_df_ohe = pd.get_dummies(sales_df, columns=['region'])\ntrain, test = train_test_split(sales_df_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Concatenate multiple CSV files ['logfile.log', 'sales.csv', 'input.json'] into sales_df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['logfile.log', 'sales.csv', 'input.json'])\nsales_df = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\nsales_df.columns = sales_df.columns.str.lower()\nnull_frac = sales_df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\nsales_df.drop(columns=cols_to_drop, inplace=True)\nsales_df.to_csv('clean_data.csv', index=False)"}
{"instruction": "Load time series CSV 'input.json', set 'date' as index, resample weekly average of 'score', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('input.json', parse_dates=['date'], index_col='date')\nweekly = df1['score'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly score with Anomalies')\nplt.show()"}
{"instruction": "Parse 'date' in data_clean as datetime, extract year and month, compute monthly sum of 'quantity', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean['date'] = pd.to_datetime(data_clean['date'])\ndata_clean['year_month'] = data_clean['date'].dt.to_period('M')\nmonthly = data_clean.groupby('year_month')['quantity'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of quantity')\nplt.show()"}
{"instruction": "Filter df_data for rows where 'duration' contains 'A', then encode 'region' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_data = df_data[df_data['duration'].str.contains('A', na=False)]\ndf_data_ohe = pd.get_dummies(df_data, columns=['region'])\ntrain, test = train_test_split(df_data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read log file 'data.csv' with custom delimiter '|', parse into df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('data.csv', sep='|', names=['timestamp','level','message'])\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\nerrors = df[df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load JSON file 'data.xlsx' into DataFrame data_clean, normalize nested records, filter rows where 'category' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndata_clean = pd.read_json('data.xlsx')\ndata_clean = pd.json_normalize(data_clean.to_dict(orient='records'))\ndata_clean = data_clean[data_clean['category'] > 0]\ndata_clean.to_parquet('clean_data.csv')"}
{"instruction": "Read log file 'logfile.log' with custom delimiter '|', parse into df_data, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('logfile.log', sep='|', names=['timestamp','level','message'])\ndf_data['timestamp'] = pd.to_datetime(df_data['timestamp'])\nerrors = df_data[df_data['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read Excel file 'data.xlsx' sheet 'Data', concatenate sheets into df_sales, pivot on 'group' and 'speed', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.xlsx', sheet_name=None)\ndf_sales = pd.concat(sheets.values(), ignore_index=True)\npivot = df_sales.pivot_table(index='group', columns='speed', values='speed', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['data.xlsx', 'time_series.csv', 'logfile.log'] into df_data, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.xlsx', 'time_series.csv', 'logfile.log'])\ndf_data = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf_data.columns = df_data.columns.str.lower()\nnull_frac = df_data.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf_data.drop(columns=cols_to_drop, inplace=True)\ndf_data.to_csv('clean_data.csv', index=False)"}
{"instruction": "Load time series CSV 'data.csv', set 'sale_date' as index, resample weekly average of 'sales', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('data.csv', parse_dates=['sale_date'], index_col='sale_date')\nweekly = df_data['sales'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly sales with Anomalies')\nplt.show()"}
{"instruction": "Read log file 'logfile.log' with custom delimiter '|', parse into data, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('logfile.log', sep='|', names=['timestamp','level','message'])\ndata['timestamp'] = pd.to_datetime(data['timestamp'])\nerrors = data[data['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Parse 'timestamp' in df1 as datetime, extract year and month, compute monthly sum of 'sales', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1['timestamp'] = pd.to_datetime(df1['timestamp'])\ndf1['year_month'] = df1['timestamp'].dt.to_period('M')\nmonthly = df1.groupby('year_month')['sales'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of sales')\nplt.show()"}
{"instruction": "Read Excel file 'input.json' sheet 'Data', concatenate sheets into df_sales, pivot on 'city' and 'rating', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('input.json', sheet_name=None)\ndf_sales = pd.concat(sheets.values(), ignore_index=True)\npivot = df_sales.pivot_table(index='city', columns='rating', values='rating', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read Excel file 'input.json' sheet 'Sheet1', concatenate sheets into df_sales, pivot on 'region' and 'price', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('input.json', sheet_name=None)\ndf_sales = pd.concat(sheets.values(), ignore_index=True)\npivot = df_sales.pivot_table(index='region', columns='price', values='price', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read Excel file 'logfile.log' sheet 'Report', concatenate sheets into df2, pivot on 'department' and 'speed', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('logfile.log', sheet_name=None)\ndf2 = pd.concat(sheets.values(), ignore_index=True)\npivot = df2.pivot_table(index='department', columns='speed', values='speed', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read log file 'data.csv' with custom delimiter '|', parse into sales_df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('data.csv', sep='|', names=['timestamp','level','message'])\nsales_df['timestamp'] = pd.to_datetime(sales_df['timestamp'])\nerrors = sales_df[sales_df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read Excel file 'logfile.log' sheet 'Sheet1', concatenate sheets into df, pivot on 'status' and 'value', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('logfile.log', sheet_name=None)\ndf = pd.concat(sheets.values(), ignore_index=True)\npivot = df.pivot_table(index='status', columns='value', values='value', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Filter df1 for rows where 'sales' contains 'A', then encode 'category' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf1 = df1[df1['sales'].str.contains('A', na=False)]\ndf1_ohe = pd.get_dummies(df1, columns=['category'])\ntrain, test = train_test_split(df1_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Load JSON file 'data.xlsx' into DataFrame df_sales, normalize nested records, filter rows where 'city' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndf_sales = pd.read_json('data.xlsx')\ndf_sales = pd.json_normalize(df_sales.to_dict(orient='records'))\ndf_sales = df_sales[df_sales['city'] > 0]\ndf_sales.to_parquet('clean_data.csv')"}
{"instruction": "Read Excel file 'time_series.csv' sheet 'Sheet1', concatenate sheets into df_sales, pivot on 'city' and 'income', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('time_series.csv', sheet_name=None)\ndf_sales = pd.concat(sheets.values(), ignore_index=True)\npivot = df_sales.pivot_table(index='city', columns='income', values='income', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read log file 'time_series.csv' with custom delimiter '|', parse into data, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('time_series.csv', sep='|', names=['timestamp','level','message'])\ndata['timestamp'] = pd.to_datetime(data['timestamp'])\nerrors = data[data['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load JSON file 'data.csv' into DataFrame data, normalize nested records, filter rows where 'sale_date' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndata = pd.read_json('data.csv')\ndata = pd.json_normalize(data.to_dict(orient='records'))\ndata = data[data['sale_date'] > 0]\ndata.to_parquet('results.parquet')"}
{"instruction": "Load time series CSV 'sales.csv', set 'date' as index, resample weekly average of 'rating', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('sales.csv', parse_dates=['date'], index_col='date')\nweekly = df_data['rating'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly rating with Anomalies')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['data.csv', 'logfile.log', 'time_series.csv'] into df1, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.csv', 'logfile.log', 'time_series.csv'])\ndf1 = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf1.columns = df1.columns.str.lower()\nnull_frac = df1.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf1.drop(columns=cols_to_drop, inplace=True)\ndf1.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read log file 'data.csv' with custom delimiter '|', parse into sales_df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('data.csv', sep='|', names=['timestamp','level','message'])\nsales_df['timestamp'] = pd.to_datetime(sales_df['timestamp'])\nerrors = sales_df[sales_df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['data.xlsx', 'logfile.log', 'time_series.csv'] into data_clean, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.xlsx', 'logfile.log', 'time_series.csv'])\ndata_clean = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata_clean.columns = data_clean.columns.str.lower()\nnull_frac = data_clean.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata_clean.drop(columns=cols_to_drop, inplace=True)\ndata_clean.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read CSV file 'input.json' into DataFrame data_raw, drop duplicates, fill missing values in 'price' with median, group by 'type' and plot average 'height'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('input.json')\ndata_raw = data_raw.drop_duplicates()\ndata_raw['price'] = data_raw['price'].fillna(data_raw['price'].median())\navg = data_raw.groupby('type')['height'].mean()\navg.plot(kind='bar')\nplt.title('Average height by type')\nplt.show()"}
{"instruction": "Filter df1 for rows where 'timestamp' contains 'A', then encode 'class' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf1 = df1[df1['timestamp'].str.contains('A', na=False)]\ndf1_ohe = pd.get_dummies(df1, columns=['class'])\ntrain, test = train_test_split(df1_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Parse 'timestamp' in data_clean as datetime, extract year and month, compute monthly sum of 'count', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean['timestamp'] = pd.to_datetime(data_clean['timestamp'])\ndata_clean['year_month'] = data_clean['timestamp'].dt.to_period('M')\nmonthly = data_clean.groupby('year_month')['count'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of count')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['sales.csv', 'logfile.log', 'data.csv'] into dataset, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['sales.csv', 'logfile.log', 'data.csv'])\ndataset = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndataset.columns = dataset.columns.str.lower()\nnull_frac = dataset.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndataset.drop(columns=cols_to_drop, inplace=True)\ndataset.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read Excel file 'input.json' sheet 'Sheet1', concatenate sheets into dataset, pivot on 'city' and 'sales', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('input.json', sheet_name=None)\ndataset = pd.concat(sheets.values(), ignore_index=True)\npivot = dataset.pivot_table(index='city', columns='sales', values='sales', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Parse 'timestamp' in data_raw as datetime, extract year and month, compute monthly sum of 'income', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw['timestamp'] = pd.to_datetime(data_raw['timestamp'])\ndata_raw['year_month'] = data_raw['timestamp'].dt.to_period('M')\nmonthly = data_raw.groupby('year_month')['income'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of income')\nplt.show()"}
{"instruction": "Load time series CSV 'time_series.csv', set 'date' as index, resample weekly average of 'revenue', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('time_series.csv', parse_dates=['date'], index_col='date')\nweekly = df_data['revenue'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly revenue with Anomalies')\nplt.show()"}
{"instruction": "Load time series CSV 'data.csv', set 'sale_date' as index, resample weekly average of 'price', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('data.csv', parse_dates=['sale_date'], index_col='sale_date')\nweekly = df1['price'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly price with Anomalies')\nplt.show()"}
{"instruction": "Filter sales_df for rows where 'sales' contains 'A', then encode 'department' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nsales_df = sales_df[sales_df['sales'].str.contains('A', na=False)]\nsales_df_ohe = pd.get_dummies(sales_df, columns=['department'])\ntrain, test = train_test_split(sales_df_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Parse 'date' in data_raw as datetime, extract year and month, compute monthly sum of 'weight', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw['date'] = pd.to_datetime(data_raw['date'])\ndata_raw['year_month'] = data_raw['date'].dt.to_period('M')\nmonthly = data_raw.groupby('year_month')['weight'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of weight')\nplt.show()"}
{"instruction": "Parse 'sale_date' in data as datetime, extract year and month, compute monthly sum of 'speed', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata['sale_date'] = pd.to_datetime(data['sale_date'])\ndata['year_month'] = data['sale_date'].dt.to_period('M')\nmonthly = data.groupby('year_month')['speed'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of speed')\nplt.show()"}
{"instruction": "Filter data_raw for rows where 'quantity' contains 'A', then encode 'city' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata_raw = data_raw[data_raw['quantity'].str.contains('A', na=False)]\ndata_raw_ohe = pd.get_dummies(data_raw, columns=['city'])\ntrain, test = train_test_split(data_raw_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Load time series CSV 'sales.csv', set 'date' as index, resample weekly average of 'count', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('sales.csv', parse_dates=['date'], index_col='date')\nweekly = data_clean['count'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly count with Anomalies')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['sales.csv', 'input.json', 'logfile.log'] into dataset, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['sales.csv', 'input.json', 'logfile.log'])\ndataset = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndataset.columns = dataset.columns.str.lower()\nnull_frac = dataset.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndataset.drop(columns=cols_to_drop, inplace=True)\ndataset.to_csv('clean_data.csv', index=False)"}
{"instruction": "Filter df for rows where 'speed' contains 'A', then encode 'class' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf = df[df['speed'].str.contains('A', na=False)]\ndf_ohe = pd.get_dummies(df, columns=['class'])\ntrain, test = train_test_split(df_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Parse 'date' in data_raw as datetime, extract year and month, compute monthly sum of 'count', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw['date'] = pd.to_datetime(data_raw['date'])\ndata_raw['year_month'] = data_raw['date'].dt.to_period('M')\nmonthly = data_raw.groupby('year_month')['count'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of count')\nplt.show()"}
{"instruction": "Filter data for rows where 'age' contains 'A', then encode 'type' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata = data[data['age'].str.contains('A', na=False)]\ndata_ohe = pd.get_dummies(data, columns=['type'])\ntrain, test = train_test_split(data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Concatenate multiple CSV files ['time_series.csv', 'data.xlsx', 'logfile.log'] into sales_df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['time_series.csv', 'data.xlsx', 'logfile.log'])\nsales_df = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\nsales_df.columns = sales_df.columns.str.lower()\nnull_frac = sales_df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\nsales_df.drop(columns=cols_to_drop, inplace=True)\nsales_df.to_csv('clean_data.csv', index=False)"}
{"instruction": "Load JSON file 'data.csv' into DataFrame df_data, normalize nested records, filter rows where 'duration' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndf_data = pd.read_json('data.csv')\ndf_data = pd.json_normalize(df_data.to_dict(orient='records'))\ndf_data = df_data[df_data['duration'] > 0]\ndf_data.to_parquet('output.parquet')"}
{"instruction": "Read log file 'time_series.csv' with custom delimiter '|', parse into data_raw, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('time_series.csv', sep='|', names=['timestamp','level','message'])\ndata_raw['timestamp'] = pd.to_datetime(data_raw['timestamp'])\nerrors = data_raw[data_raw['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read Excel file 'data.csv' sheet 'Report', concatenate sheets into data, pivot on 'department' and 'price', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.csv', sheet_name=None)\ndata = pd.concat(sheets.values(), ignore_index=True)\npivot = data.pivot_table(index='department', columns='price', values='price', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Load time series CSV 'input.json', set 'date' as index, resample weekly average of 'salary', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('input.json', parse_dates=['date'], index_col='date')\nweekly = data_clean['salary'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly salary with Anomalies')\nplt.show()"}
{"instruction": "Read log file 'time_series.csv' with custom delimiter '|', parse into df_sales, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('time_series.csv', sep='|', names=['timestamp','level','message'])\ndf_sales['timestamp'] = pd.to_datetime(df_sales['timestamp'])\nerrors = df_sales[df_sales['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read log file 'sales.csv' with custom delimiter '|', parse into df_data, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('sales.csv', sep='|', names=['timestamp','level','message'])\ndf_data['timestamp'] = pd.to_datetime(df_data['timestamp'])\nerrors = df_data[df_data['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load JSON file 'data.xlsx' into DataFrame df, normalize nested records, filter rows where 'date' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndf = pd.read_json('data.xlsx')\ndf = pd.json_normalize(df.to_dict(orient='records'))\ndf = df[df['date'] > 0]\ndf.to_parquet('clean_data.csv')"}
{"instruction": "Load time series CSV 'time_series.csv', set 'sale_date' as index, resample weekly average of 'quantity', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('time_series.csv', parse_dates=['sale_date'], index_col='sale_date')\nweekly = df1['quantity'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly quantity with Anomalies')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['data.csv', 'logfile.log', 'input.json'] into sales_df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.csv', 'logfile.log', 'input.json'])\nsales_df = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\nsales_df.columns = sales_df.columns.str.lower()\nnull_frac = sales_df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\nsales_df.drop(columns=cols_to_drop, inplace=True)\nsales_df.to_csv('clean_data.csv', index=False)"}
{"instruction": "Parse 'order_date' in data_clean as datetime, extract year and month, compute monthly sum of 'duration', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean['order_date'] = pd.to_datetime(data_clean['order_date'])\ndata_clean['year_month'] = data_clean['order_date'].dt.to_period('M')\nmonthly = data_clean.groupby('year_month')['duration'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of duration')\nplt.show()"}
{"instruction": "Read CSV file 'data.csv' into DataFrame data_clean, drop duplicates, fill missing values in 'sale_date' with median, group by 'category' and plot average 'weight'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('data.csv')\ndata_clean = data_clean.drop_duplicates()\ndata_clean['sale_date'] = data_clean['sale_date'].fillna(data_clean['sale_date'].median())\navg = data_clean.groupby('category')['weight'].mean()\navg.plot(kind='bar')\nplt.title('Average weight by category')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['data.xlsx', 'sales.csv', 'logfile.log'] into data, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.xlsx', 'sales.csv', 'logfile.log'])\ndata = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata.columns = data.columns.str.lower()\nnull_frac = data.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata.drop(columns=cols_to_drop, inplace=True)\ndata.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read Excel file 'sales.csv' sheet 'Report', concatenate sheets into data_raw, pivot on 'status' and 'height', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('sales.csv', sheet_name=None)\ndata_raw = pd.concat(sheets.values(), ignore_index=True)\npivot = data_raw.pivot_table(index='status', columns='height', values='height', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['input.json', 'sales.csv', 'data.csv'] into df1, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['input.json', 'sales.csv', 'data.csv'])\ndf1 = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf1.columns = df1.columns.str.lower()\nnull_frac = df1.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf1.drop(columns=cols_to_drop, inplace=True)\ndf1.to_csv('clean_data.csv', index=False)"}
{"instruction": "Load time series CSV 'sales.csv', set 'order_date' as index, resample weekly average of 'rating', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('sales.csv', parse_dates=['order_date'], index_col='order_date')\nweekly = df_data['rating'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly rating with Anomalies')\nplt.show()"}
{"instruction": "Read log file 'time_series.csv' with custom delimiter '|', parse into data_raw, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('time_series.csv', sep='|', names=['timestamp','level','message'])\ndata_raw['timestamp'] = pd.to_datetime(data_raw['timestamp'])\nerrors = data_raw[data_raw['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load JSON file 'sales.csv' into DataFrame df2, normalize nested records, filter rows where 'category' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndf2 = pd.read_json('sales.csv')\ndf2 = pd.json_normalize(df2.to_dict(orient='records'))\ndf2 = df2[df2['category'] > 0]\ndf2.to_parquet('results.parquet')"}
{"instruction": "Parse 'sale_date' in df_sales as datetime, extract year and month, compute monthly sum of 'price', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales['sale_date'] = pd.to_datetime(df_sales['sale_date'])\ndf_sales['year_month'] = df_sales['sale_date'].dt.to_period('M')\nmonthly = df_sales.groupby('year_month')['price'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of price')\nplt.show()"}
{"instruction": "Load time series CSV 'data.xlsx', set 'timestamp' as index, resample weekly average of 'rating', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('data.xlsx', parse_dates=['timestamp'], index_col='timestamp')\nweekly = df2['rating'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly rating with Anomalies')\nplt.show()"}
{"instruction": "Load JSON file 'input.json' into DataFrame df_sales, normalize nested records, filter rows where 'class' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndf_sales = pd.read_json('input.json')\ndf_sales = pd.json_normalize(df_sales.to_dict(orient='records'))\ndf_sales = df_sales[df_sales['class'] > 0]\ndf_sales.to_parquet('results.parquet')"}
{"instruction": "Read CSV file 'data.csv' into DataFrame df, drop duplicates, fill missing values in 'age' with median, group by 'type' and plot average 'height'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('data.csv')\ndf = df.drop_duplicates()\ndf['age'] = df['age'].fillna(df['age'].median())\navg = df.groupby('type')['height'].mean()\navg.plot(kind='bar')\nplt.title('Average height by type')\nplt.show()"}
{"instruction": "Read CSV file 'time_series.csv' into DataFrame data_raw, drop duplicates, fill missing values in 'order_date' with median, group by 'type' and plot average 'income'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('time_series.csv')\ndata_raw = data_raw.drop_duplicates()\ndata_raw['order_date'] = data_raw['order_date'].fillna(data_raw['order_date'].median())\navg = data_raw.groupby('type')['income'].mean()\navg.plot(kind='bar')\nplt.title('Average income by type')\nplt.show()"}
{"instruction": "Parse 'timestamp' in sales_df as datetime, extract year and month, compute monthly sum of 'price', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df['timestamp'] = pd.to_datetime(sales_df['timestamp'])\nsales_df['year_month'] = sales_df['timestamp'].dt.to_period('M')\nmonthly = sales_df.groupby('year_month')['price'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of price')\nplt.show()"}
{"instruction": "Load time series CSV 'time_series.csv', set 'sale_date' as index, resample weekly average of 'speed', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('time_series.csv', parse_dates=['sale_date'], index_col='sale_date')\nweekly = df2['speed'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly speed with Anomalies')\nplt.show()"}
{"instruction": "Read CSV file 'logfile.log' into DataFrame data, drop duplicates, fill missing values in 'score' with median, group by 'city' and plot average 'price'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('logfile.log')\ndata = data.drop_duplicates()\ndata['score'] = data['score'].fillna(data['score'].median())\navg = data.groupby('city')['price'].mean()\navg.plot(kind='bar')\nplt.title('Average price by city')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['logfile.log', 'input.json', 'data.csv'] into df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['logfile.log', 'input.json', 'data.csv'])\ndf = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf.columns = df.columns.str.lower()\nnull_frac = df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf.drop(columns=cols_to_drop, inplace=True)\ndf.to_csv('clean_data.csv', index=False)"}
{"instruction": "Filter df1 for rows where 'income' contains 'A', then encode 'class' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf1 = df1[df1['income'].str.contains('A', na=False)]\ndf1_ohe = pd.get_dummies(df1, columns=['class'])\ntrain, test = train_test_split(df1_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read Excel file 'input.json' sheet 'Sheet1', concatenate sheets into df_data, pivot on 'department' and 'height', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('input.json', sheet_name=None)\ndf_data = pd.concat(sheets.values(), ignore_index=True)\npivot = df_data.pivot_table(index='department', columns='height', values='height', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read log file 'sales.csv' with custom delimiter '|', parse into df_data, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('sales.csv', sep='|', names=['timestamp','level','message'])\ndf_data['timestamp'] = pd.to_datetime(df_data['timestamp'])\nerrors = df_data[df_data['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Filter data for rows where 'height' contains 'A', then encode 'status' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata = data[data['height'].str.contains('A', na=False)]\ndata_ohe = pd.get_dummies(data, columns=['status'])\ntrain, test = train_test_split(data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Concatenate multiple CSV files ['data.xlsx', 'input.json', 'data.csv'] into dataset, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.xlsx', 'input.json', 'data.csv'])\ndataset = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndataset.columns = dataset.columns.str.lower()\nnull_frac = dataset.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndataset.drop(columns=cols_to_drop, inplace=True)\ndataset.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read log file 'sales.csv' with custom delimiter '|', parse into df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales.csv', sep='|', names=['timestamp','level','message'])\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\nerrors = df[df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read Excel file 'data.xlsx' sheet 'Data', concatenate sheets into data_clean, pivot on 'class' and 'height', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.xlsx', sheet_name=None)\ndata_clean = pd.concat(sheets.values(), ignore_index=True)\npivot = data_clean.pivot_table(index='class', columns='height', values='height', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['logfile.log', 'input.json', 'time_series.csv'] into sales_df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['logfile.log', 'input.json', 'time_series.csv'])\nsales_df = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\nsales_df.columns = sales_df.columns.str.lower()\nnull_frac = sales_df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\nsales_df.drop(columns=cols_to_drop, inplace=True)\nsales_df.to_csv('clean_data.csv', index=False)"}
{"instruction": "Concatenate multiple CSV files ['data.csv', 'time_series.csv', 'input.json'] into df1, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.csv', 'time_series.csv', 'input.json'])\ndf1 = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf1.columns = df1.columns.str.lower()\nnull_frac = df1.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf1.drop(columns=cols_to_drop, inplace=True)\ndf1.to_csv('clean_data.csv', index=False)"}
{"instruction": "Concatenate multiple CSV files ['input.json', 'data.xlsx', 'data.csv'] into df1, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['input.json', 'data.xlsx', 'data.csv'])\ndf1 = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf1.columns = df1.columns.str.lower()\nnull_frac = df1.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf1.drop(columns=cols_to_drop, inplace=True)\ndf1.to_csv('clean_data.csv', index=False)"}
{"instruction": "Load time series CSV 'sales.csv', set 'sale_date' as index, resample weekly average of 'quantity', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('sales.csv', parse_dates=['sale_date'], index_col='sale_date')\nweekly = data['quantity'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly quantity with Anomalies')\nplt.show()"}
{"instruction": "Parse 'date' in df2 as datetime, extract year and month, compute monthly sum of 'age', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2['date'] = pd.to_datetime(df2['date'])\ndf2['year_month'] = df2['date'].dt.to_period('M')\nmonthly = df2.groupby('year_month')['age'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of age')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['data.csv', 'sales.csv', 'input.json'] into df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.csv', 'sales.csv', 'input.json'])\ndf = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf.columns = df.columns.str.lower()\nnull_frac = df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf.drop(columns=cols_to_drop, inplace=True)\ndf.to_csv('clean_data.csv', index=False)"}
{"instruction": "Load time series CSV 'time_series.csv', set 'timestamp' as index, resample weekly average of 'age', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('time_series.csv', parse_dates=['timestamp'], index_col='timestamp')\nweekly = data_raw['age'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly age with Anomalies')\nplt.show()"}
{"instruction": "Parse 'date' in df as datetime, extract year and month, compute monthly sum of 'revenue', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf['date'] = pd.to_datetime(df['date'])\ndf['year_month'] = df['date'].dt.to_period('M')\nmonthly = df.groupby('year_month')['revenue'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of revenue')\nplt.show()"}
{"instruction": "Read Excel file 'sales.csv' sheet 'Sheet1', concatenate sheets into df_sales, pivot on 'category' and 'speed', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('sales.csv', sheet_name=None)\ndf_sales = pd.concat(sheets.values(), ignore_index=True)\npivot = df_sales.pivot_table(index='category', columns='speed', values='speed', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['data.csv', 'sales.csv', 'input.json'] into df_data, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.csv', 'sales.csv', 'input.json'])\ndf_data = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf_data.columns = df_data.columns.str.lower()\nnull_frac = df_data.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf_data.drop(columns=cols_to_drop, inplace=True)\ndf_data.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read log file 'time_series.csv' with custom delimiter '|', parse into data_clean, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('time_series.csv', sep='|', names=['timestamp','level','message'])\ndata_clean['timestamp'] = pd.to_datetime(data_clean['timestamp'])\nerrors = data_clean[data_clean['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read Excel file 'time_series.csv' sheet 'Report', concatenate sheets into data_clean, pivot on 'type' and 'count', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('time_series.csv', sheet_name=None)\ndata_clean = pd.concat(sheets.values(), ignore_index=True)\npivot = data_clean.pivot_table(index='type', columns='count', values='count', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read log file 'data.csv' with custom delimiter '|', parse into df_data, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('data.csv', sep='|', names=['timestamp','level','message'])\ndf_data['timestamp'] = pd.to_datetime(df_data['timestamp'])\nerrors = df_data[df_data['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Parse 'order_date' in df_sales as datetime, extract year and month, compute monthly sum of 'weight', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales['order_date'] = pd.to_datetime(df_sales['order_date'])\ndf_sales['year_month'] = df_sales['order_date'].dt.to_period('M')\nmonthly = df_sales.groupby('year_month')['weight'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of weight')\nplt.show()"}
{"instruction": "Read Excel file 'sales.csv' sheet 'Report', concatenate sheets into data, pivot on 'class' and 'score', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('sales.csv', sheet_name=None)\ndata = pd.concat(sheets.values(), ignore_index=True)\npivot = data.pivot_table(index='class', columns='score', values='score', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Parse 'date' in df_sales as datetime, extract year and month, compute monthly sum of 'age', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales['date'] = pd.to_datetime(df_sales['date'])\ndf_sales['year_month'] = df_sales['date'].dt.to_period('M')\nmonthly = df_sales.groupby('year_month')['age'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of age')\nplt.show()"}
{"instruction": "Load JSON file 'logfile.log' into DataFrame df_data, normalize nested records, filter rows where 'timestamp' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndf_data = pd.read_json('logfile.log')\ndf_data = pd.json_normalize(df_data.to_dict(orient='records'))\ndf_data = df_data[df_data['timestamp'] > 0]\ndf_data.to_parquet('results.parquet')"}
{"instruction": "Read log file 'input.json' with custom delimiter '|', parse into data, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('input.json', sep='|', names=['timestamp','level','message'])\ndata['timestamp'] = pd.to_datetime(data['timestamp'])\nerrors = data[data['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load JSON file 'input.json' into DataFrame df, normalize nested records, filter rows where 'price' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndf = pd.read_json('input.json')\ndf = pd.json_normalize(df.to_dict(orient='records'))\ndf = df[df['price'] > 0]\ndf.to_parquet('output.parquet')"}
{"instruction": "Read CSV file 'data.xlsx' into DataFrame df2, drop duplicates, fill missing values in 'count' with median, group by 'category' and plot average 'age'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('data.xlsx')\ndf2 = df2.drop_duplicates()\ndf2['count'] = df2['count'].fillna(df2['count'].median())\navg = df2.groupby('category')['age'].mean()\navg.plot(kind='bar')\nplt.title('Average age by category')\nplt.show()"}
{"instruction": "Read Excel file 'input.json' sheet 'Data', concatenate sheets into df_sales, pivot on 'department' and 'duration', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('input.json', sheet_name=None)\ndf_sales = pd.concat(sheets.values(), ignore_index=True)\npivot = df_sales.pivot_table(index='department', columns='duration', values='duration', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into data_clean, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\ndata_clean['timestamp'] = pd.to_datetime(data_clean['timestamp'])\nerrors = data_clean[data_clean['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load JSON file 'data.xlsx' into DataFrame df2, normalize nested records, filter rows where 'region' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndf2 = pd.read_json('data.xlsx')\ndf2 = pd.json_normalize(df2.to_dict(orient='records'))\ndf2 = df2[df2['region'] > 0]\ndf2.to_parquet('results.parquet')"}
{"instruction": "Filter df_sales for rows where 'type' contains 'A', then encode 'type' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_sales = df_sales[df_sales['type'].str.contains('A', na=False)]\ndf_sales_ohe = pd.get_dummies(df_sales, columns=['type'])\ntrain, test = train_test_split(df_sales_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Load time series CSV 'input.json', set 'sale_date' as index, resample weekly average of 'height', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('input.json', parse_dates=['sale_date'], index_col='sale_date')\nweekly = data_clean['height'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly height with Anomalies')\nplt.show()"}
{"instruction": "Read CSV file 'sales.csv' into DataFrame df, drop duplicates, fill missing values in 'type' with median, group by 'status' and plot average 'score'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales.csv')\ndf = df.drop_duplicates()\ndf['type'] = df['type'].fillna(df['type'].median())\navg = df.groupby('status')['score'].mean()\navg.plot(kind='bar')\nplt.title('Average score by status')\nplt.show()"}
{"instruction": "Read CSV file 'data.csv' into DataFrame data, drop duplicates, fill missing values in 'class' with median, group by 'class' and plot average 'duration'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('data.csv')\ndata = data.drop_duplicates()\ndata['class'] = data['class'].fillna(data['class'].median())\navg = data.groupby('class')['duration'].mean()\navg.plot(kind='bar')\nplt.title('Average duration by class')\nplt.show()"}
{"instruction": "Read log file 'sales.csv' with custom delimiter '|', parse into data_raw, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('sales.csv', sep='|', names=['timestamp','level','message'])\ndata_raw['timestamp'] = pd.to_datetime(data_raw['timestamp'])\nerrors = data_raw[data_raw['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read log file 'time_series.csv' with custom delimiter '|', parse into data_clean, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('time_series.csv', sep='|', names=['timestamp','level','message'])\ndata_clean['timestamp'] = pd.to_datetime(data_clean['timestamp'])\nerrors = data_clean[data_clean['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load time series CSV 'data.xlsx', set 'date' as index, resample weekly average of 'quantity', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('data.xlsx', parse_dates=['date'], index_col='date')\nweekly = data_clean['quantity'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly quantity with Anomalies')\nplt.show()"}
{"instruction": "Load JSON file 'input.json' into DataFrame dataset, normalize nested records, filter rows where 'type' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndataset = pd.read_json('input.json')\ndataset = pd.json_normalize(dataset.to_dict(orient='records'))\ndataset = dataset[dataset['type'] > 0]\ndataset.to_parquet('results.parquet')"}
{"instruction": "Read CSV file 'logfile.log' into DataFrame data_clean, drop duplicates, fill missing values in 'group' with median, group by 'class' and plot average 'quantity'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('logfile.log')\ndata_clean = data_clean.drop_duplicates()\ndata_clean['group'] = data_clean['group'].fillna(data_clean['group'].median())\navg = data_clean.groupby('class')['quantity'].mean()\navg.plot(kind='bar')\nplt.title('Average quantity by class')\nplt.show()"}
{"instruction": "Filter data_raw for rows where 'revenue' contains 'A', then encode 'type' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata_raw = data_raw[data_raw['revenue'].str.contains('A', na=False)]\ndata_raw_ohe = pd.get_dummies(data_raw, columns=['type'])\ntrain, test = train_test_split(data_raw_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Concatenate multiple CSV files ['logfile.log', 'input.json', 'data.csv'] into data_clean, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['logfile.log', 'input.json', 'data.csv'])\ndata_clean = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata_clean.columns = data_clean.columns.str.lower()\nnull_frac = data_clean.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata_clean.drop(columns=cols_to_drop, inplace=True)\ndata_clean.to_csv('clean_data.csv', index=False)"}
{"instruction": "Filter sales_df for rows where 'date' contains 'A', then encode 'category' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nsales_df = sales_df[sales_df['date'].str.contains('A', na=False)]\nsales_df_ohe = pd.get_dummies(sales_df, columns=['category'])\ntrain, test = train_test_split(sales_df_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Parse 'order_date' in dataset as datetime, extract year and month, compute monthly sum of 'revenue', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset['order_date'] = pd.to_datetime(dataset['order_date'])\ndataset['year_month'] = dataset['order_date'].dt.to_period('M')\nmonthly = dataset.groupby('year_month')['revenue'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of revenue')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['data.csv', 'time_series.csv', 'data.xlsx'] into data_clean, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.csv', 'time_series.csv', 'data.xlsx'])\ndata_clean = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata_clean.columns = data_clean.columns.str.lower()\nnull_frac = data_clean.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata_clean.drop(columns=cols_to_drop, inplace=True)\ndata_clean.to_csv('clean_data.csv', index=False)"}
{"instruction": "Load JSON file 'logfile.log' into DataFrame df_sales, normalize nested records, filter rows where 'rating' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndf_sales = pd.read_json('logfile.log')\ndf_sales = pd.json_normalize(df_sales.to_dict(orient='records'))\ndf_sales = df_sales[df_sales['rating'] > 0]\ndf_sales.to_parquet('clean_data.csv')"}
{"instruction": "Read Excel file 'logfile.log' sheet 'Report', concatenate sheets into df1, pivot on 'city' and 'value', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('logfile.log', sheet_name=None)\ndf1 = pd.concat(sheets.values(), ignore_index=True)\npivot = df1.pivot_table(index='city', columns='value', values='value', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read CSV file 'logfile.log' into DataFrame sales_df, drop duplicates, fill missing values in 'department' with median, group by 'region' and plot average 'height'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('logfile.log')\nsales_df = sales_df.drop_duplicates()\nsales_df['department'] = sales_df['department'].fillna(sales_df['department'].median())\navg = sales_df.groupby('region')['height'].mean()\navg.plot(kind='bar')\nplt.title('Average height by region')\nplt.show()"}
{"instruction": "Read CSV file 'sales.csv' into DataFrame data_raw, drop duplicates, fill missing values in 'weight' with median, group by 'group' and plot average 'rating'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('sales.csv')\ndata_raw = data_raw.drop_duplicates()\ndata_raw['weight'] = data_raw['weight'].fillna(data_raw['weight'].median())\navg = data_raw.groupby('group')['rating'].mean()\navg.plot(kind='bar')\nplt.title('Average rating by group')\nplt.show()"}
{"instruction": "Read CSV file 'time_series.csv' into DataFrame df_data, drop duplicates, fill missing values in 'timestamp' with median, group by 'region' and plot average 'duration'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('time_series.csv')\ndf_data = df_data.drop_duplicates()\ndf_data['timestamp'] = df_data['timestamp'].fillna(df_data['timestamp'].median())\navg = df_data.groupby('region')['duration'].mean()\navg.plot(kind='bar')\nplt.title('Average duration by region')\nplt.show()"}
{"instruction": "Read CSV file 'sales.csv' into DataFrame sales_df, drop duplicates, fill missing values in 'sales' with median, group by 'department' and plot average 'speed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('sales.csv')\nsales_df = sales_df.drop_duplicates()\nsales_df['sales'] = sales_df['sales'].fillna(sales_df['sales'].median())\navg = sales_df.groupby('department')['speed'].mean()\navg.plot(kind='bar')\nplt.title('Average speed by department')\nplt.show()"}
{"instruction": "Filter df_data for rows where 'revenue' contains 'A', then encode 'category' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_data = df_data[df_data['revenue'].str.contains('A', na=False)]\ndf_data_ohe = pd.get_dummies(df_data, columns=['category'])\ntrain, test = train_test_split(df_data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Load JSON file 'time_series.csv' into DataFrame df_data, normalize nested records, filter rows where 'weight' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndf_data = pd.read_json('time_series.csv')\ndf_data = pd.json_normalize(df_data.to_dict(orient='records'))\ndf_data = df_data[df_data['weight'] > 0]\ndf_data.to_parquet('clean_data.csv')"}
{"instruction": "Read log file 'sales.csv' with custom delimiter '|', parse into sales_df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('sales.csv', sep='|', names=['timestamp','level','message'])\nsales_df['timestamp'] = pd.to_datetime(sales_df['timestamp'])\nerrors = sales_df[sales_df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read Excel file 'data.csv' sheet 'Sheet1', concatenate sheets into df1, pivot on 'department' and 'duration', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.csv', sheet_name=None)\ndf1 = pd.concat(sheets.values(), ignore_index=True)\npivot = df1.pivot_table(index='department', columns='duration', values='duration', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into df_sales, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\ndf_sales['timestamp'] = pd.to_datetime(df_sales['timestamp'])\nerrors = df_sales[df_sales['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Filter data for rows where 'region' contains 'A', then encode 'group' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata = data[data['region'].str.contains('A', na=False)]\ndata_ohe = pd.get_dummies(data, columns=['group'])\ntrain, test = train_test_split(data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Concatenate multiple CSV files ['sales.csv', 'input.json', 'data.csv'] into dataset, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['sales.csv', 'input.json', 'data.csv'])\ndataset = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndataset.columns = dataset.columns.str.lower()\nnull_frac = dataset.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndataset.drop(columns=cols_to_drop, inplace=True)\ndataset.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into data_clean, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\ndata_clean['timestamp'] = pd.to_datetime(data_clean['timestamp'])\nerrors = data_clean[data_clean['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['logfile.log', 'input.json', 'data.xlsx'] into dataset, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['logfile.log', 'input.json', 'data.xlsx'])\ndataset = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndataset.columns = dataset.columns.str.lower()\nnull_frac = dataset.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndataset.drop(columns=cols_to_drop, inplace=True)\ndataset.to_csv('clean_data.csv', index=False)"}
{"instruction": "Parse 'date' in dataset as datetime, extract year and month, compute monthly sum of 'quantity', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset['date'] = pd.to_datetime(dataset['date'])\ndataset['year_month'] = dataset['date'].dt.to_period('M')\nmonthly = dataset.groupby('year_month')['quantity'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of quantity')\nplt.show()"}
{"instruction": "Read log file 'data.csv' with custom delimiter '|', parse into data, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('data.csv', sep='|', names=['timestamp','level','message'])\ndata['timestamp'] = pd.to_datetime(data['timestamp'])\nerrors = data[data['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Filter df2 for rows where 'department' contains 'A', then encode 'type' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf2 = df2[df2['department'].str.contains('A', na=False)]\ndf2_ohe = pd.get_dummies(df2, columns=['type'])\ntrain, test = train_test_split(df2_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read log file 'input.json' with custom delimiter '|', parse into data, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('input.json', sep='|', names=['timestamp','level','message'])\ndata['timestamp'] = pd.to_datetime(data['timestamp'])\nerrors = data[data['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read Excel file 'input.json' sheet 'Data', concatenate sheets into dataset, pivot on 'group' and 'height', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('input.json', sheet_name=None)\ndataset = pd.concat(sheets.values(), ignore_index=True)\npivot = dataset.pivot_table(index='group', columns='height', values='height', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read CSV file 'data.xlsx' into DataFrame data_raw, drop duplicates, fill missing values in 'price' with median, group by 'status' and plot average 'value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('data.xlsx')\ndata_raw = data_raw.drop_duplicates()\ndata_raw['price'] = data_raw['price'].fillna(data_raw['price'].median())\navg = data_raw.groupby('status')['value'].mean()\navg.plot(kind='bar')\nplt.title('Average value by status')\nplt.show()"}
{"instruction": "Load time series CSV 'data.csv', set 'date' as index, resample weekly average of 'rating', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('data.csv', parse_dates=['date'], index_col='date')\nweekly = df2['rating'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly rating with Anomalies')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['logfile.log', 'data.csv', 'sales.csv'] into df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['logfile.log', 'data.csv', 'sales.csv'])\ndf = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf.columns = df.columns.str.lower()\nnull_frac = df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf.drop(columns=cols_to_drop, inplace=True)\ndf.to_csv('clean_data.csv', index=False)"}
{"instruction": "Parse 'order_date' in df_sales as datetime, extract year and month, compute monthly sum of 'score', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales['order_date'] = pd.to_datetime(df_sales['order_date'])\ndf_sales['year_month'] = df_sales['order_date'].dt.to_period('M')\nmonthly = df_sales.groupby('year_month')['score'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of score')\nplt.show()"}
{"instruction": "Load time series CSV 'sales.csv', set 'order_date' as index, resample weekly average of 'salary', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('sales.csv', parse_dates=['order_date'], index_col='order_date')\nweekly = data_clean['salary'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly salary with Anomalies')\nplt.show()"}
{"instruction": "Read CSV file 'logfile.log' into DataFrame data_raw, drop duplicates, fill missing values in 'salary' with median, group by 'city' and plot average 'count'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('logfile.log')\ndata_raw = data_raw.drop_duplicates()\ndata_raw['salary'] = data_raw['salary'].fillna(data_raw['salary'].median())\navg = data_raw.groupby('city')['count'].mean()\navg.plot(kind='bar')\nplt.title('Average count by city')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['input.json', 'data.xlsx', 'sales.csv'] into dataset, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['input.json', 'data.xlsx', 'sales.csv'])\ndataset = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndataset.columns = dataset.columns.str.lower()\nnull_frac = dataset.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndataset.drop(columns=cols_to_drop, inplace=True)\ndataset.to_csv('clean_data.csv', index=False)"}
{"instruction": "Concatenate multiple CSV files ['input.json', 'data.csv', 'logfile.log'] into data, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['input.json', 'data.csv', 'logfile.log'])\ndata = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata.columns = data.columns.str.lower()\nnull_frac = data.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata.drop(columns=cols_to_drop, inplace=True)\ndata.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read Excel file 'sales.csv' sheet 'Sheet1', concatenate sheets into df, pivot on 'city' and 'value', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('sales.csv', sheet_name=None)\ndf = pd.concat(sheets.values(), ignore_index=True)\npivot = df.pivot_table(index='city', columns='value', values='value', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read Excel file 'time_series.csv' sheet 'Report', concatenate sheets into data, pivot on 'category' and 'duration', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('time_series.csv', sheet_name=None)\ndata = pd.concat(sheets.values(), ignore_index=True)\npivot = data.pivot_table(index='category', columns='duration', values='duration', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read Excel file 'input.json' sheet 'Data', concatenate sheets into df2, pivot on 'department' and 'rating', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('input.json', sheet_name=None)\ndf2 = pd.concat(sheets.values(), ignore_index=True)\npivot = df2.pivot_table(index='department', columns='rating', values='rating', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['data.xlsx', 'logfile.log', 'data.csv'] into dataset, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.xlsx', 'logfile.log', 'data.csv'])\ndataset = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndataset.columns = dataset.columns.str.lower()\nnull_frac = dataset.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndataset.drop(columns=cols_to_drop, inplace=True)\ndataset.to_csv('clean_data.csv', index=False)"}
{"instruction": "Filter df_sales for rows where 'timestamp' contains 'A', then encode 'department' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_sales = df_sales[df_sales['timestamp'].str.contains('A', na=False)]\ndf_sales_ohe = pd.get_dummies(df_sales, columns=['department'])\ntrain, test = train_test_split(df_sales_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Load JSON file 'data.xlsx' into DataFrame df2, normalize nested records, filter rows where 'date' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndf2 = pd.read_json('data.xlsx')\ndf2 = pd.json_normalize(df2.to_dict(orient='records'))\ndf2 = df2[df2['date'] > 0]\ndf2.to_parquet('output.parquet')"}
{"instruction": "Concatenate multiple CSV files ['sales.csv', 'time_series.csv', 'data.xlsx'] into sales_df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['sales.csv', 'time_series.csv', 'data.xlsx'])\nsales_df = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\nsales_df.columns = sales_df.columns.str.lower()\nnull_frac = sales_df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\nsales_df.drop(columns=cols_to_drop, inplace=True)\nsales_df.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read CSV file 'data.xlsx' into DataFrame data_raw, drop duplicates, fill missing values in 'department' with median, group by 'status' and plot average 'price'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('data.xlsx')\ndata_raw = data_raw.drop_duplicates()\ndata_raw['department'] = data_raw['department'].fillna(data_raw['department'].median())\navg = data_raw.groupby('status')['price'].mean()\navg.plot(kind='bar')\nplt.title('Average price by status')\nplt.show()"}
{"instruction": "Read log file 'logfile.log' with custom delimiter '|', parse into df2, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('logfile.log', sep='|', names=['timestamp','level','message'])\ndf2['timestamp'] = pd.to_datetime(df2['timestamp'])\nerrors = df2[df2['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read CSV file 'logfile.log' into DataFrame df1, drop duplicates, fill missing values in 'city' with median, group by 'class' and plot average 'score'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('logfile.log')\ndf1 = df1.drop_duplicates()\ndf1['city'] = df1['city'].fillna(df1['city'].median())\navg = df1.groupby('class')['score'].mean()\navg.plot(kind='bar')\nplt.title('Average score by class')\nplt.show()"}
{"instruction": "Parse 'date' in dataset as datetime, extract year and month, compute monthly sum of 'weight', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset['date'] = pd.to_datetime(dataset['date'])\ndataset['year_month'] = dataset['date'].dt.to_period('M')\nmonthly = dataset.groupby('year_month')['weight'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of weight')\nplt.show()"}
{"instruction": "Read log file 'time_series.csv' with custom delimiter '|', parse into data_raw, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('time_series.csv', sep='|', names=['timestamp','level','message'])\ndata_raw['timestamp'] = pd.to_datetime(data_raw['timestamp'])\nerrors = data_raw[data_raw['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['data.csv', 'time_series.csv', 'data.xlsx'] into df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.csv', 'time_series.csv', 'data.xlsx'])\ndf = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf.columns = df.columns.str.lower()\nnull_frac = df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf.drop(columns=cols_to_drop, inplace=True)\ndf.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read CSV file 'logfile.log' into DataFrame df2, drop duplicates, fill missing values in 'category' with median, group by 'status' and plot average 'salary'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('logfile.log')\ndf2 = df2.drop_duplicates()\ndf2['category'] = df2['category'].fillna(df2['category'].median())\navg = df2.groupby('status')['salary'].mean()\navg.plot(kind='bar')\nplt.title('Average salary by status')\nplt.show()"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into df1, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\ndf1['timestamp'] = pd.to_datetime(df1['timestamp'])\nerrors = df1[df1['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['data.xlsx', 'sales.csv', 'input.json'] into df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.xlsx', 'sales.csv', 'input.json'])\ndf = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf.columns = df.columns.str.lower()\nnull_frac = df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf.drop(columns=cols_to_drop, inplace=True)\ndf.to_csv('clean_data.csv', index=False)"}
{"instruction": "Load JSON file 'data.xlsx' into DataFrame data_clean, normalize nested records, filter rows where 'type' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndata_clean = pd.read_json('data.xlsx')\ndata_clean = pd.json_normalize(data_clean.to_dict(orient='records'))\ndata_clean = data_clean[data_clean['type'] > 0]\ndata_clean.to_parquet('results.parquet')"}
{"instruction": "Read log file 'logfile.log' with custom delimiter '|', parse into df2, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('logfile.log', sep='|', names=['timestamp','level','message'])\ndf2['timestamp'] = pd.to_datetime(df2['timestamp'])\nerrors = df2[df2['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read CSV file 'sales.csv' into DataFrame df1, drop duplicates, fill missing values in 'sales' with median, group by 'class' and plot average 'value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('sales.csv')\ndf1 = df1.drop_duplicates()\ndf1['sales'] = df1['sales'].fillna(df1['sales'].median())\navg = df1.groupby('class')['value'].mean()\navg.plot(kind='bar')\nplt.title('Average value by class')\nplt.show()"}
{"instruction": "Read log file 'logfile.log' with custom delimiter '|', parse into dataset, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('logfile.log', sep='|', names=['timestamp','level','message'])\ndataset['timestamp'] = pd.to_datetime(dataset['timestamp'])\nerrors = dataset[dataset['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load JSON file 'data.xlsx' into DataFrame df_data, normalize nested records, filter rows where 'height' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndf_data = pd.read_json('data.xlsx')\ndf_data = pd.json_normalize(df_data.to_dict(orient='records'))\ndf_data = df_data[df_data['height'] > 0]\ndf_data.to_parquet('results.parquet')"}
{"instruction": "Filter df_data for rows where 'department' contains 'A', then encode 'group' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_data = df_data[df_data['department'].str.contains('A', na=False)]\ndf_data_ohe = pd.get_dummies(df_data, columns=['group'])\ntrain, test = train_test_split(df_data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Filter df_data for rows where 'count' contains 'A', then encode 'category' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_data = df_data[df_data['count'].str.contains('A', na=False)]\ndf_data_ohe = pd.get_dummies(df_data, columns=['category'])\ntrain, test = train_test_split(df_data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Parse 'order_date' in df2 as datetime, extract year and month, compute monthly sum of 'rating', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2['order_date'] = pd.to_datetime(df2['order_date'])\ndf2['year_month'] = df2['order_date'].dt.to_period('M')\nmonthly = df2.groupby('year_month')['rating'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of rating')\nplt.show()"}
{"instruction": "Parse 'order_date' in df as datetime, extract year and month, compute monthly sum of 'revenue', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf['order_date'] = pd.to_datetime(df['order_date'])\ndf['year_month'] = df['order_date'].dt.to_period('M')\nmonthly = df.groupby('year_month')['revenue'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of revenue')\nplt.show()"}
{"instruction": "Read Excel file 'logfile.log' sheet 'Sheet1', concatenate sheets into data_clean, pivot on 'city' and 'quantity', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('logfile.log', sheet_name=None)\ndata_clean = pd.concat(sheets.values(), ignore_index=True)\npivot = data_clean.pivot_table(index='city', columns='quantity', values='quantity', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read log file 'time_series.csv' with custom delimiter '|', parse into data, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('time_series.csv', sep='|', names=['timestamp','level','message'])\ndata['timestamp'] = pd.to_datetime(data['timestamp'])\nerrors = data[data['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Parse 'date' in data_raw as datetime, extract year and month, compute monthly sum of 'speed', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw['date'] = pd.to_datetime(data_raw['date'])\ndata_raw['year_month'] = data_raw['date'].dt.to_period('M')\nmonthly = data_raw.groupby('year_month')['speed'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of speed')\nplt.show()"}
{"instruction": "Load time series CSV 'data.csv', set 'timestamp' as index, resample weekly average of 'height', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('data.csv', parse_dates=['timestamp'], index_col='timestamp')\nweekly = data['height'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly height with Anomalies')\nplt.show()"}
{"instruction": "Read Excel file 'logfile.log' sheet 'Data', concatenate sheets into data_clean, pivot on 'type' and 'speed', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('logfile.log', sheet_name=None)\ndata_clean = pd.concat(sheets.values(), ignore_index=True)\npivot = data_clean.pivot_table(index='type', columns='speed', values='speed', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read CSV file 'data.csv' into DataFrame df2, drop duplicates, fill missing values in 'price' with median, group by 'class' and plot average 'score'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('data.csv')\ndf2 = df2.drop_duplicates()\ndf2['price'] = df2['price'].fillna(df2['price'].median())\navg = df2.groupby('class')['score'].mean()\navg.plot(kind='bar')\nplt.title('Average score by class')\nplt.show()"}
{"instruction": "Read log file 'time_series.csv' with custom delimiter '|', parse into df1, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('time_series.csv', sep='|', names=['timestamp','level','message'])\ndf1['timestamp'] = pd.to_datetime(df1['timestamp'])\nerrors = df1[df1['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read log file 'input.json' with custom delimiter '|', parse into dataset, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('input.json', sep='|', names=['timestamp','level','message'])\ndataset['timestamp'] = pd.to_datetime(dataset['timestamp'])\nerrors = dataset[dataset['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load JSON file 'data.csv' into DataFrame dataset, normalize nested records, filter rows where 'sale_date' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndataset = pd.read_json('data.csv')\ndataset = pd.json_normalize(dataset.to_dict(orient='records'))\ndataset = dataset[dataset['sale_date'] > 0]\ndataset.to_parquet('results.parquet')"}
{"instruction": "Read Excel file 'time_series.csv' sheet 'Sheet1', concatenate sheets into data, pivot on 'category' and 'rating', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('time_series.csv', sheet_name=None)\ndata = pd.concat(sheets.values(), ignore_index=True)\npivot = data.pivot_table(index='category', columns='rating', values='rating', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read CSV file 'logfile.log' into DataFrame df_sales, drop duplicates, fill missing values in 'revenue' with median, group by 'status' and plot average 'quantity'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('logfile.log')\ndf_sales = df_sales.drop_duplicates()\ndf_sales['revenue'] = df_sales['revenue'].fillna(df_sales['revenue'].median())\navg = df_sales.groupby('status')['quantity'].mean()\navg.plot(kind='bar')\nplt.title('Average quantity by status')\nplt.show()"}
{"instruction": "Read Excel file 'logfile.log' sheet 'Report', concatenate sheets into data_raw, pivot on 'city' and 'income', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('logfile.log', sheet_name=None)\ndata_raw = pd.concat(sheets.values(), ignore_index=True)\npivot = data_raw.pivot_table(index='city', columns='income', values='income', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read CSV file 'data.xlsx' into DataFrame data_raw, drop duplicates, fill missing values in 'status' with median, group by 'group' and plot average 'quantity'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('data.xlsx')\ndata_raw = data_raw.drop_duplicates()\ndata_raw['status'] = data_raw['status'].fillna(data_raw['status'].median())\navg = data_raw.groupby('group')['quantity'].mean()\navg.plot(kind='bar')\nplt.title('Average quantity by group')\nplt.show()"}
{"instruction": "Load JSON file 'sales.csv' into DataFrame df_data, normalize nested records, filter rows where 'age' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndf_data = pd.read_json('sales.csv')\ndf_data = pd.json_normalize(df_data.to_dict(orient='records'))\ndf_data = df_data[df_data['age'] > 0]\ndf_data.to_parquet('clean_data.csv')"}
{"instruction": "Concatenate multiple CSV files ['sales.csv', 'logfile.log', 'input.json'] into dataset, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['sales.csv', 'logfile.log', 'input.json'])\ndataset = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndataset.columns = dataset.columns.str.lower()\nnull_frac = dataset.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndataset.drop(columns=cols_to_drop, inplace=True)\ndataset.to_csv('clean_data.csv', index=False)"}
{"instruction": "Parse 'sale_date' in df as datetime, extract year and month, compute monthly sum of 'sales', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf['sale_date'] = pd.to_datetime(df['sale_date'])\ndf['year_month'] = df['sale_date'].dt.to_period('M')\nmonthly = df.groupby('year_month')['sales'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of sales')\nplt.show()"}
{"instruction": "Load JSON file 'sales.csv' into DataFrame data_raw, normalize nested records, filter rows where 'duration' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndata_raw = pd.read_json('sales.csv')\ndata_raw = pd.json_normalize(data_raw.to_dict(orient='records'))\ndata_raw = data_raw[data_raw['duration'] > 0]\ndata_raw.to_parquet('output.parquet')"}
{"instruction": "Read log file 'time_series.csv' with custom delimiter '|', parse into df2, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('time_series.csv', sep='|', names=['timestamp','level','message'])\ndf2['timestamp'] = pd.to_datetime(df2['timestamp'])\nerrors = df2[df2['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load JSON file 'time_series.csv' into DataFrame df_data, normalize nested records, filter rows where 'order_date' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndf_data = pd.read_json('time_series.csv')\ndf_data = pd.json_normalize(df_data.to_dict(orient='records'))\ndf_data = df_data[df_data['order_date'] > 0]\ndf_data.to_parquet('output.parquet')"}
{"instruction": "Read CSV file 'time_series.csv' into DataFrame data_raw, drop duplicates, fill missing values in 'revenue' with median, group by 'department' and plot average 'value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('time_series.csv')\ndata_raw = data_raw.drop_duplicates()\ndata_raw['revenue'] = data_raw['revenue'].fillna(data_raw['revenue'].median())\navg = data_raw.groupby('department')['value'].mean()\navg.plot(kind='bar')\nplt.title('Average value by department')\nplt.show()"}
{"instruction": "Load time series CSV 'input.json', set 'date' as index, resample weekly average of 'rating', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('input.json', parse_dates=['date'], index_col='date')\nweekly = df_sales['rating'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly rating with Anomalies')\nplt.show()"}
{"instruction": "Filter df_data for rows where 'class' contains 'A', then encode 'status' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_data = df_data[df_data['class'].str.contains('A', na=False)]\ndf_data_ohe = pd.get_dummies(df_data, columns=['status'])\ntrain, test = train_test_split(df_data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read Excel file 'time_series.csv' sheet 'Sheet1', concatenate sheets into df_data, pivot on 'city' and 'rating', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('time_series.csv', sheet_name=None)\ndf_data = pd.concat(sheets.values(), ignore_index=True)\npivot = df_data.pivot_table(index='city', columns='rating', values='rating', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['data.xlsx', 'input.json', 'sales.csv'] into df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.xlsx', 'input.json', 'sales.csv'])\ndf = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf.columns = df.columns.str.lower()\nnull_frac = df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf.drop(columns=cols_to_drop, inplace=True)\ndf.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read Excel file 'input.json' sheet 'Data', concatenate sheets into df1, pivot on 'category' and 'duration', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('input.json', sheet_name=None)\ndf1 = pd.concat(sheets.values(), ignore_index=True)\npivot = df1.pivot_table(index='category', columns='duration', values='duration', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read Excel file 'time_series.csv' sheet 'Data', concatenate sheets into data, pivot on 'status' and 'score', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('time_series.csv', sheet_name=None)\ndata = pd.concat(sheets.values(), ignore_index=True)\npivot = data.pivot_table(index='status', columns='score', values='score', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Load JSON file 'input.json' into DataFrame data_raw, normalize nested records, filter rows where 'region' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndata_raw = pd.read_json('input.json')\ndata_raw = pd.json_normalize(data_raw.to_dict(orient='records'))\ndata_raw = data_raw[data_raw['region'] > 0]\ndata_raw.to_parquet('results.parquet')"}
{"instruction": "Read CSV file 'input.json' into DataFrame df_sales, drop duplicates, fill missing values in 'timestamp' with median, group by 'city' and plot average 'value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('input.json')\ndf_sales = df_sales.drop_duplicates()\ndf_sales['timestamp'] = df_sales['timestamp'].fillna(df_sales['timestamp'].median())\navg = df_sales.groupby('city')['value'].mean()\navg.plot(kind='bar')\nplt.title('Average value by city')\nplt.show()"}
{"instruction": "Parse 'timestamp' in data as datetime, extract year and month, compute monthly sum of 'income', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata['timestamp'] = pd.to_datetime(data['timestamp'])\ndata['year_month'] = data['timestamp'].dt.to_period('M')\nmonthly = data.groupby('year_month')['income'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of income')\nplt.show()"}
{"instruction": "Parse 'order_date' in df_data as datetime, extract year and month, compute monthly sum of 'duration', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data['order_date'] = pd.to_datetime(df_data['order_date'])\ndf_data['year_month'] = df_data['order_date'].dt.to_period('M')\nmonthly = df_data.groupby('year_month')['duration'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of duration')\nplt.show()"}
{"instruction": "Parse 'sale_date' in df1 as datetime, extract year and month, compute monthly sum of 'quantity', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1['sale_date'] = pd.to_datetime(df1['sale_date'])\ndf1['year_month'] = df1['sale_date'].dt.to_period('M')\nmonthly = df1.groupby('year_month')['quantity'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of quantity')\nplt.show()"}
{"instruction": "Load time series CSV 'data.xlsx', set 'order_date' as index, resample weekly average of 'value', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('data.xlsx', parse_dates=['order_date'], index_col='order_date')\nweekly = data_clean['value'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly value with Anomalies')\nplt.show()"}
{"instruction": "Load time series CSV 'data.csv', set 'timestamp' as index, resample weekly average of 'duration', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('data.csv', parse_dates=['timestamp'], index_col='timestamp')\nweekly = dataset['duration'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly duration with Anomalies')\nplt.show()"}
{"instruction": "Filter df_data for rows where 'class' contains 'A', then encode 'class' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_data = df_data[df_data['class'].str.contains('A', na=False)]\ndf_data_ohe = pd.get_dummies(df_data, columns=['class'])\ntrain, test = train_test_split(df_data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Load JSON file 'sales.csv' into DataFrame data_raw, normalize nested records, filter rows where 'category' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndata_raw = pd.read_json('sales.csv')\ndata_raw = pd.json_normalize(data_raw.to_dict(orient='records'))\ndata_raw = data_raw[data_raw['category'] > 0]\ndata_raw.to_parquet('output.parquet')"}
{"instruction": "Load JSON file 'time_series.csv' into DataFrame sales_df, normalize nested records, filter rows where 'height' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\nsales_df = pd.read_json('time_series.csv')\nsales_df = pd.json_normalize(sales_df.to_dict(orient='records'))\nsales_df = sales_df[sales_df['height'] > 0]\nsales_df.to_parquet('clean_data.csv')"}
{"instruction": "Read log file 'logfile.log' with custom delimiter '|', parse into sales_df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('logfile.log', sep='|', names=['timestamp','level','message'])\nsales_df['timestamp'] = pd.to_datetime(sales_df['timestamp'])\nerrors = sales_df[sales_df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into data_clean, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\ndata_clean['timestamp'] = pd.to_datetime(data_clean['timestamp'])\nerrors = data_clean[data_clean['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load JSON file 'time_series.csv' into DataFrame data, normalize nested records, filter rows where 'count' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndata = pd.read_json('time_series.csv')\ndata = pd.json_normalize(data.to_dict(orient='records'))\ndata = data[data['count'] > 0]\ndata.to_parquet('output.parquet')"}
{"instruction": "Read Excel file 'sales.csv' sheet 'Data', concatenate sheets into data, pivot on 'department' and 'score', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('sales.csv', sheet_name=None)\ndata = pd.concat(sheets.values(), ignore_index=True)\npivot = data.pivot_table(index='department', columns='score', values='score', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read Excel file 'logfile.log' sheet 'Report', concatenate sheets into data_clean, pivot on 'region' and 'price', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('logfile.log', sheet_name=None)\ndata_clean = pd.concat(sheets.values(), ignore_index=True)\npivot = data_clean.pivot_table(index='region', columns='price', values='price', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read CSV file 'time_series.csv' into DataFrame df_data, drop duplicates, fill missing values in 'status' with median, group by 'city' and plot average 'rating'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data = pd.read_csv('time_series.csv')\ndf_data = df_data.drop_duplicates()\ndf_data['status'] = df_data['status'].fillna(df_data['status'].median())\navg = df_data.groupby('city')['rating'].mean()\navg.plot(kind='bar')\nplt.title('Average rating by city')\nplt.show()"}
{"instruction": "Read log file 'sales.csv' with custom delimiter '|', parse into data, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('sales.csv', sep='|', names=['timestamp','level','message'])\ndata['timestamp'] = pd.to_datetime(data['timestamp'])\nerrors = data[data['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read CSV file 'input.json' into DataFrame dataset, drop duplicates, fill missing values in 'rating' with median, group by 'group' and plot average 'income'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('input.json')\ndataset = dataset.drop_duplicates()\ndataset['rating'] = dataset['rating'].fillna(dataset['rating'].median())\navg = dataset.groupby('group')['income'].mean()\navg.plot(kind='bar')\nplt.title('Average income by group')\nplt.show()"}
{"instruction": "Read CSV file 'sales.csv' into DataFrame dataset, drop duplicates, fill missing values in 'status' with median, group by 'region' and plot average 'duration'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('sales.csv')\ndataset = dataset.drop_duplicates()\ndataset['status'] = dataset['status'].fillna(dataset['status'].median())\navg = dataset.groupby('region')['duration'].mean()\navg.plot(kind='bar')\nplt.title('Average duration by region')\nplt.show()"}
{"instruction": "Parse 'timestamp' in df2 as datetime, extract year and month, compute monthly sum of 'income', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2['timestamp'] = pd.to_datetime(df2['timestamp'])\ndf2['year_month'] = df2['timestamp'].dt.to_period('M')\nmonthly = df2.groupby('year_month')['income'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of income')\nplt.show()"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into dataset, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\ndataset['timestamp'] = pd.to_datetime(dataset['timestamp'])\nerrors = dataset[dataset['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Parse 'order_date' in df_data as datetime, extract year and month, compute monthly sum of 'price', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data['order_date'] = pd.to_datetime(df_data['order_date'])\ndf_data['year_month'] = df_data['order_date'].dt.to_period('M')\nmonthly = df_data.groupby('year_month')['price'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of price')\nplt.show()"}
{"instruction": "Read Excel file 'sales.csv' sheet 'Data', concatenate sheets into df_data, pivot on 'group' and 'score', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('sales.csv', sheet_name=None)\ndf_data = pd.concat(sheets.values(), ignore_index=True)\npivot = df_data.pivot_table(index='group', columns='score', values='score', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['time_series.csv', 'input.json', 'logfile.log'] into data_clean, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['time_series.csv', 'input.json', 'logfile.log'])\ndata_clean = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata_clean.columns = data_clean.columns.str.lower()\nnull_frac = data_clean.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata_clean.drop(columns=cols_to_drop, inplace=True)\ndata_clean.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read CSV file 'sales.csv' into DataFrame data_clean, drop duplicates, fill missing values in 'date' with median, group by 'city' and plot average 'salary'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('sales.csv')\ndata_clean = data_clean.drop_duplicates()\ndata_clean['date'] = data_clean['date'].fillna(data_clean['date'].median())\navg = data_clean.groupby('city')['salary'].mean()\navg.plot(kind='bar')\nplt.title('Average salary by city')\nplt.show()"}
{"instruction": "Load JSON file 'time_series.csv' into DataFrame df2, normalize nested records, filter rows where 'class' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndf2 = pd.read_json('time_series.csv')\ndf2 = pd.json_normalize(df2.to_dict(orient='records'))\ndf2 = df2[df2['class'] > 0]\ndf2.to_parquet('results.parquet')"}
{"instruction": "Filter data_clean for rows where 'duration' contains 'A', then encode 'group' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata_clean = data_clean[data_clean['duration'].str.contains('A', na=False)]\ndata_clean_ohe = pd.get_dummies(data_clean, columns=['group'])\ntrain, test = train_test_split(data_clean_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Load time series CSV 'data.csv', set 'sale_date' as index, resample weekly average of 'duration', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('data.csv', parse_dates=['sale_date'], index_col='sale_date')\nweekly = data_clean['duration'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly duration with Anomalies')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['data.xlsx', 'sales.csv', 'time_series.csv'] into df2, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.xlsx', 'sales.csv', 'time_series.csv'])\ndf2 = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf2.columns = df2.columns.str.lower()\nnull_frac = df2.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf2.drop(columns=cols_to_drop, inplace=True)\ndf2.to_csv('clean_data.csv', index=False)"}
{"instruction": "Filter df_sales for rows where 'speed' contains 'A', then encode 'group' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_sales = df_sales[df_sales['speed'].str.contains('A', na=False)]\ndf_sales_ohe = pd.get_dummies(df_sales, columns=['group'])\ntrain, test = train_test_split(df_sales_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Parse 'timestamp' in sales_df as datetime, extract year and month, compute monthly sum of 'score', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df['timestamp'] = pd.to_datetime(sales_df['timestamp'])\nsales_df['year_month'] = sales_df['timestamp'].dt.to_period('M')\nmonthly = sales_df.groupby('year_month')['score'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of score')\nplt.show()"}
{"instruction": "Read CSV file 'sales.csv' into DataFrame data_raw, drop duplicates, fill missing values in 'quantity' with median, group by 'region' and plot average 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('sales.csv')\ndata_raw = data_raw.drop_duplicates()\ndata_raw['quantity'] = data_raw['quantity'].fillna(data_raw['quantity'].median())\navg = data_raw.groupby('region')['sales'].mean()\navg.plot(kind='bar')\nplt.title('Average sales by region')\nplt.show()"}
{"instruction": "Read log file 'time_series.csv' with custom delimiter '|', parse into df_sales, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('time_series.csv', sep='|', names=['timestamp','level','message'])\ndf_sales['timestamp'] = pd.to_datetime(df_sales['timestamp'])\nerrors = df_sales[df_sales['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read log file 'time_series.csv' with custom delimiter '|', parse into data_clean, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('time_series.csv', sep='|', names=['timestamp','level','message'])\ndata_clean['timestamp'] = pd.to_datetime(data_clean['timestamp'])\nerrors = data_clean[data_clean['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Parse 'order_date' in df2 as datetime, extract year and month, compute monthly sum of 'quantity', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2['order_date'] = pd.to_datetime(df2['order_date'])\ndf2['year_month'] = df2['order_date'].dt.to_period('M')\nmonthly = df2.groupby('year_month')['quantity'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of quantity')\nplt.show()"}
{"instruction": "Load time series CSV 'sales.csv', set 'timestamp' as index, resample weekly average of 'height', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('sales.csv', parse_dates=['timestamp'], index_col='timestamp')\nweekly = dataset['height'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly height with Anomalies')\nplt.show()"}
{"instruction": "Filter df_data for rows where 'score' contains 'A', then encode 'type' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_data = df_data[df_data['score'].str.contains('A', na=False)]\ndf_data_ohe = pd.get_dummies(df_data, columns=['type'])\ntrain, test = train_test_split(df_data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Concatenate multiple CSV files ['time_series.csv', 'data.xlsx', 'input.json'] into df2, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['time_series.csv', 'data.xlsx', 'input.json'])\ndf2 = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf2.columns = df2.columns.str.lower()\nnull_frac = df2.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf2.drop(columns=cols_to_drop, inplace=True)\ndf2.to_csv('clean_data.csv', index=False)"}
{"instruction": "Load time series CSV 'time_series.csv', set 'sale_date' as index, resample weekly average of 'salary', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('time_series.csv', parse_dates=['sale_date'], index_col='sale_date')\nweekly = df2['salary'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly salary with Anomalies')\nplt.show()"}
{"instruction": "Read log file 'time_series.csv' with custom delimiter '|', parse into dataset, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('time_series.csv', sep='|', names=['timestamp','level','message'])\ndataset['timestamp'] = pd.to_datetime(dataset['timestamp'])\nerrors = dataset[dataset['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read CSV file 'time_series.csv' into DataFrame data, drop duplicates, fill missing values in 'category' with median, group by 'group' and plot average 'duration'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('time_series.csv')\ndata = data.drop_duplicates()\ndata['category'] = data['category'].fillna(data['category'].median())\navg = data.groupby('group')['duration'].mean()\navg.plot(kind='bar')\nplt.title('Average duration by group')\nplt.show()"}
{"instruction": "Read log file 'logfile.log' with custom delimiter '|', parse into dataset, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('logfile.log', sep='|', names=['timestamp','level','message'])\ndataset['timestamp'] = pd.to_datetime(dataset['timestamp'])\nerrors = dataset[dataset['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read log file 'time_series.csv' with custom delimiter '|', parse into df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('time_series.csv', sep='|', names=['timestamp','level','message'])\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\nerrors = df[df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Load JSON file 'logfile.log' into DataFrame df2, normalize nested records, filter rows where 'speed' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndf2 = pd.read_json('logfile.log')\ndf2 = pd.json_normalize(df2.to_dict(orient='records'))\ndf2 = df2[df2['speed'] > 0]\ndf2.to_parquet('output.parquet')"}
{"instruction": "Concatenate multiple CSV files ['sales.csv', 'input.json', 'data.xlsx'] into dataset, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['sales.csv', 'input.json', 'data.xlsx'])\ndataset = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndataset.columns = dataset.columns.str.lower()\nnull_frac = dataset.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndataset.drop(columns=cols_to_drop, inplace=True)\ndataset.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read CSV file 'input.json' into DataFrame data_clean, drop duplicates, fill missing values in 'count' with median, group by 'group' and plot average 'speed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean = pd.read_csv('input.json')\ndata_clean = data_clean.drop_duplicates()\ndata_clean['count'] = data_clean['count'].fillna(data_clean['count'].median())\navg = data_clean.groupby('group')['speed'].mean()\navg.plot(kind='bar')\nplt.title('Average speed by group')\nplt.show()"}
{"instruction": "Parse 'timestamp' in sales_df as datetime, extract year and month, compute monthly sum of 'height', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df['timestamp'] = pd.to_datetime(sales_df['timestamp'])\nsales_df['year_month'] = sales_df['timestamp'].dt.to_period('M')\nmonthly = sales_df.groupby('year_month')['height'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of height')\nplt.show()"}
{"instruction": "Load time series CSV 'logfile.log', set 'sale_date' as index, resample weekly average of 'count', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('logfile.log', parse_dates=['sale_date'], index_col='sale_date')\nweekly = df_sales['count'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly count with Anomalies')\nplt.show()"}
{"instruction": "Load JSON file 'data.xlsx' into DataFrame data_raw, normalize nested records, filter rows where 'group' > 0, and save to Parquet 'clean_data.csv'.", "output": "import pandas as pd\ndata_raw = pd.read_json('data.xlsx')\ndata_raw = pd.json_normalize(data_raw.to_dict(orient='records'))\ndata_raw = data_raw[data_raw['group'] > 0]\ndata_raw.to_parquet('clean_data.csv')"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into df_sales, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_sales = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\ndf_sales['timestamp'] = pd.to_datetime(df_sales['timestamp'])\nerrors = df_sales[df_sales['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['sales.csv', 'input.json', 'data.csv'] into df2, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['sales.csv', 'input.json', 'data.csv'])\ndf2 = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf2.columns = df2.columns.str.lower()\nnull_frac = df2.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf2.drop(columns=cols_to_drop, inplace=True)\ndf2.to_csv('clean_data.csv', index=False)"}
{"instruction": "Filter df_data for rows where 'timestamp' contains 'A', then encode 'group' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_data = df_data[df_data['timestamp'].str.contains('A', na=False)]\ndf_data_ohe = pd.get_dummies(df_data, columns=['group'])\ntrain, test = train_test_split(df_data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Load JSON file 'logfile.log' into DataFrame df2, normalize nested records, filter rows where 'sales' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndf2 = pd.read_json('logfile.log')\ndf2 = pd.json_normalize(df2.to_dict(orient='records'))\ndf2 = df2[df2['sales'] > 0]\ndf2.to_parquet('output.parquet')"}
{"instruction": "Parse 'sale_date' in sales_df as datetime, extract year and month, compute monthly sum of 'speed', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df['sale_date'] = pd.to_datetime(sales_df['sale_date'])\nsales_df['year_month'] = sales_df['sale_date'].dt.to_period('M')\nmonthly = sales_df.groupby('year_month')['speed'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of speed')\nplt.show()"}
{"instruction": "Filter sales_df for rows where 'category' contains 'A', then encode 'category' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nsales_df = sales_df[sales_df['category'].str.contains('A', na=False)]\nsales_df_ohe = pd.get_dummies(sales_df, columns=['category'])\ntrain, test = train_test_split(sales_df_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read Excel file 'logfile.log' sheet 'Report', concatenate sheets into df1, pivot on 'city' and 'quantity', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('logfile.log', sheet_name=None)\ndf1 = pd.concat(sheets.values(), ignore_index=True)\npivot = df1.pivot_table(index='city', columns='quantity', values='quantity', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Read Excel file 'sales.csv' sheet 'Report', concatenate sheets into df, pivot on 'class' and 'weight', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('sales.csv', sheet_name=None)\ndf = pd.concat(sheets.values(), ignore_index=True)\npivot = df.pivot_table(index='class', columns='weight', values='weight', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['data.xlsx', 'input.json', 'logfile.log'] into df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.xlsx', 'input.json', 'logfile.log'])\ndf = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf.columns = df.columns.str.lower()\nnull_frac = df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf.drop(columns=cols_to_drop, inplace=True)\ndf.to_csv('clean_data.csv', index=False)"}
{"instruction": "Filter df_sales for rows where 'region' contains 'A', then encode 'status' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_sales = df_sales[df_sales['region'].str.contains('A', na=False)]\ndf_sales_ohe = pd.get_dummies(df_sales, columns=['status'])\ntrain, test = train_test_split(df_sales_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read CSV file 'sales.csv' into DataFrame data, drop duplicates, fill missing values in 'quantity' with median, group by 'city' and plot average 'quantity'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('sales.csv')\ndata = data.drop_duplicates()\ndata['quantity'] = data['quantity'].fillna(data['quantity'].median())\navg = data.groupby('city')['quantity'].mean()\navg.plot(kind='bar')\nplt.title('Average quantity by city')\nplt.show()"}
{"instruction": "Parse 'sale_date' in data as datetime, extract year and month, compute monthly sum of 'height', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata['sale_date'] = pd.to_datetime(data['sale_date'])\ndata['year_month'] = data['sale_date'].dt.to_period('M')\nmonthly = data.groupby('year_month')['height'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of height')\nplt.show()"}
{"instruction": "Load time series CSV 'input.json', set 'order_date' as index, resample weekly average of 'income', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('input.json', parse_dates=['order_date'], index_col='order_date')\nweekly = data_raw['income'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly income with Anomalies')\nplt.show()"}
{"instruction": "Read Excel file 'logfile.log' sheet 'Report', concatenate sheets into data_clean, pivot on 'city' and 'sales', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('logfile.log', sheet_name=None)\ndata_clean = pd.concat(sheets.values(), ignore_index=True)\npivot = data_clean.pivot_table(index='city', columns='sales', values='sales', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['data.csv', 'sales.csv', 'data.xlsx'] into df_data, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.csv', 'sales.csv', 'data.xlsx'])\ndf_data = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf_data.columns = df_data.columns.str.lower()\nnull_frac = df_data.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf_data.drop(columns=cols_to_drop, inplace=True)\ndf_data.to_csv('clean_data.csv', index=False)"}
{"instruction": "Parse 'date' in df1 as datetime, extract year and month, compute monthly sum of 'speed', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1['date'] = pd.to_datetime(df1['date'])\ndf1['year_month'] = df1['date'].dt.to_period('M')\nmonthly = df1.groupby('year_month')['speed'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of speed')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['data.xlsx', 'sales.csv', 'input.json'] into data, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.xlsx', 'sales.csv', 'input.json'])\ndata = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndata.columns = data.columns.str.lower()\nnull_frac = data.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndata.drop(columns=cols_to_drop, inplace=True)\ndata.to_csv('clean_data.csv', index=False)"}
{"instruction": "Load JSON file 'input.json' into DataFrame data_clean, normalize nested records, filter rows where 'rating' > 0, and save to Parquet 'results.parquet'.", "output": "import pandas as pd\ndata_clean = pd.read_json('input.json')\ndata_clean = pd.json_normalize(data_clean.to_dict(orient='records'))\ndata_clean = data_clean[data_clean['rating'] > 0]\ndata_clean.to_parquet('results.parquet')"}
{"instruction": "Parse 'order_date' in sales_df as datetime, extract year and month, compute monthly sum of 'income', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df['order_date'] = pd.to_datetime(sales_df['order_date'])\nsales_df['year_month'] = sales_df['order_date'].dt.to_period('M')\nmonthly = sales_df.groupby('year_month')['income'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of income')\nplt.show()"}
{"instruction": "Read CSV file 'logfile.log' into DataFrame df1, drop duplicates, fill missing values in 'city' with median, group by 'city' and plot average 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('logfile.log')\ndf1 = df1.drop_duplicates()\ndf1['city'] = df1['city'].fillna(df1['city'].median())\navg = df1.groupby('city')['sales'].mean()\navg.plot(kind='bar')\nplt.title('Average sales by city')\nplt.show()"}
{"instruction": "Read CSV file 'time_series.csv' into DataFrame sales_df, drop duplicates, fill missing values in 'revenue' with median, group by 'category' and plot average 'sales'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('time_series.csv')\nsales_df = sales_df.drop_duplicates()\nsales_df['revenue'] = sales_df['revenue'].fillna(sales_df['revenue'].median())\navg = sales_df.groupby('category')['sales'].mean()\navg.plot(kind='bar')\nplt.title('Average sales by category')\nplt.show()"}
{"instruction": "Parse 'sale_date' in data_clean as datetime, extract year and month, compute monthly sum of 'quantity', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean['sale_date'] = pd.to_datetime(data_clean['sale_date'])\ndata_clean['year_month'] = data_clean['sale_date'].dt.to_period('M')\nmonthly = data_clean.groupby('year_month')['quantity'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of quantity')\nplt.show()"}
{"instruction": "Read CSV file 'sales.csv' into DataFrame data, drop duplicates, fill missing values in 'group' with median, group by 'type' and plot average 'speed'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('sales.csv')\ndata = data.drop_duplicates()\ndata['group'] = data['group'].fillna(data['group'].median())\navg = data.groupby('type')['speed'].mean()\navg.plot(kind='bar')\nplt.title('Average speed by type')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['time_series.csv', 'input.json', 'data.xlsx'] into df_data, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['time_series.csv', 'input.json', 'data.xlsx'])\ndf_data = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf_data.columns = df_data.columns.str.lower()\nnull_frac = df_data.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf_data.drop(columns=cols_to_drop, inplace=True)\ndf_data.to_csv('clean_data.csv', index=False)"}
{"instruction": "Load time series CSV 'data.csv', set 'date' as index, resample weekly average of 'quantity', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('data.csv', parse_dates=['date'], index_col='date')\nweekly = sales_df['quantity'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly quantity with Anomalies')\nplt.show()"}
{"instruction": "Read log file 'data.xlsx' with custom delimiter '|', parse into df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('data.xlsx', sep='|', names=['timestamp','level','message'])\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\nerrors = df[df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['time_series.csv', 'data.csv', 'logfile.log'] into dataset, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['time_series.csv', 'data.csv', 'logfile.log'])\ndataset = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndataset.columns = dataset.columns.str.lower()\nnull_frac = dataset.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndataset.drop(columns=cols_to_drop, inplace=True)\ndataset.to_csv('clean_data.csv', index=False)"}
{"instruction": "Filter df2 for rows where 'group' contains 'A', then encode 'category' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf2 = df2[df2['group'].str.contains('A', na=False)]\ndf2_ohe = pd.get_dummies(df2, columns=['category'])\ntrain, test = train_test_split(df2_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Read log file 'logfile.log' with custom delimiter '|', parse into sales_df, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nsales_df = pd.read_csv('logfile.log', sep='|', names=['timestamp','level','message'])\nsales_df['timestamp'] = pd.to_datetime(sales_df['timestamp'])\nerrors = sales_df[sales_df['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Read Excel file 'logfile.log' sheet 'Report', concatenate sheets into data_raw, pivot on 'status' and 'height', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('logfile.log', sheet_name=None)\ndata_raw = pd.concat(sheets.values(), ignore_index=True)\npivot = data_raw.pivot_table(index='status', columns='height', values='height', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Parse 'order_date' in data_clean as datetime, extract year and month, compute monthly sum of 'sales', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_clean['order_date'] = pd.to_datetime(data_clean['order_date'])\ndata_clean['year_month'] = data_clean['order_date'].dt.to_period('M')\nmonthly = data_clean.groupby('year_month')['sales'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of sales')\nplt.show()"}
{"instruction": "Filter df for rows where 'age' contains 'A', then encode 'type' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf = df[df['age'].str.contains('A', na=False)]\ndf_ohe = pd.get_dummies(df, columns=['type'])\ntrain, test = train_test_split(df_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Concatenate multiple CSV files ['data.csv', 'logfile.log', 'input.json'] into df_data, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.csv', 'logfile.log', 'input.json'])\ndf_data = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf_data.columns = df_data.columns.str.lower()\nnull_frac = df_data.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf_data.drop(columns=cols_to_drop, inplace=True)\ndf_data.to_csv('clean_data.csv', index=False)"}
{"instruction": "Load JSON file 'input.json' into DataFrame df_data, normalize nested records, filter rows where 'group' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndf_data = pd.read_json('input.json')\ndf_data = pd.json_normalize(df_data.to_dict(orient='records'))\ndf_data = df_data[df_data['group'] > 0]\ndf_data.to_parquet('output.parquet')"}
{"instruction": "Read CSV file 'input.json' into DataFrame df1, drop duplicates, fill missing values in 'quantity' with median, group by 'status' and plot average 'price'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('input.json')\ndf1 = df1.drop_duplicates()\ndf1['quantity'] = df1['quantity'].fillna(df1['quantity'].median())\navg = df1.groupby('status')['price'].mean()\navg.plot(kind='bar')\nplt.title('Average price by status')\nplt.show()"}
{"instruction": "Load JSON file 'data.xlsx' into DataFrame sales_df, normalize nested records, filter rows where 'value' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\nsales_df = pd.read_json('data.xlsx')\nsales_df = pd.json_normalize(sales_df.to_dict(orient='records'))\nsales_df = sales_df[sales_df['value'] > 0]\nsales_df.to_parquet('output.parquet')"}
{"instruction": "Concatenate multiple CSV files ['input.json', 'data.csv', 'sales.csv'] into sales_df, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['input.json', 'data.csv', 'sales.csv'])\nsales_df = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\nsales_df.columns = sales_df.columns.str.lower()\nnull_frac = sales_df.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\nsales_df.drop(columns=cols_to_drop, inplace=True)\nsales_df.to_csv('clean_data.csv', index=False)"}
{"instruction": "Load time series CSV 'sales.csv', set 'order_date' as index, resample weekly average of 'revenue', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('sales.csv', parse_dates=['order_date'], index_col='order_date')\nweekly = df['revenue'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly revenue with Anomalies')\nplt.show()"}
{"instruction": "Filter sales_df for rows where 'date' contains 'A', then encode 'region' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nsales_df = sales_df[sales_df['date'].str.contains('A', na=False)]\nsales_df_ohe = pd.get_dummies(sales_df, columns=['region'])\ntrain, test = train_test_split(sales_df_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Filter df_data for rows where 'duration' contains 'A', then encode 'department' with one-hot, and train/test split with 80-20%.", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_data = df_data[df_data['duration'].str.contains('A', na=False)]\ndf_data_ohe = pd.get_dummies(df_data, columns=['department'])\ntrain, test = train_test_split(df_data_ohe, test_size=0.2, random_state=42)\nprint(train.shape, test.shape)"}
{"instruction": "Load time series CSV 'data.csv', set 'date' as index, resample weekly average of 'sales', detect points > mean+2*std and annotate plot.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('data.csv', parse_dates=['date'], index_col='date')\nweekly = df1['sales'].resample('W').mean()\nmean = weekly.mean(); std = weekly.std()\nsignal = weekly[weekly > mean + 2*std]\nplt.plot(weekly.index, weekly)\nplt.scatter(signal.index, signal, color='red')\nplt.title('Weekly sales with Anomalies')\nplt.show()"}
{"instruction": "Read log file 'sales.csv' with custom delimiter '|', parse into data_raw, extract error rows containing 'ERROR', count occurrences by date, and plot bar chart.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndata_raw = pd.read_csv('sales.csv', sep='|', names=['timestamp','level','message'])\ndata_raw['timestamp'] = pd.to_datetime(data_raw['timestamp'])\nerrors = data_raw[data_raw['level']=='ERROR']\nerrors['date'] = errors['timestamp'].dt.date\ncounts = errors['date'].value_counts().sort_index()\ncounts.plot(kind='bar')\nplt.title('Errors per Day')\nplt.show()"}
{"instruction": "Parse 'order_date' in df1 as datetime, extract year and month, compute monthly sum of 'price', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf1['order_date'] = pd.to_datetime(df1['order_date'])\ndf1['year_month'] = df1['order_date'].dt.to_period('M')\nmonthly = df1.groupby('year_month')['price'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of price')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['data.csv', 'input.json', 'sales.csv'] into df_data, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['data.csv', 'input.json', 'sales.csv'])\ndf_data = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf_data.columns = df_data.columns.str.lower()\nnull_frac = df_data.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf_data.drop(columns=cols_to_drop, inplace=True)\ndf_data.to_csv('clean_data.csv', index=False)"}
{"instruction": "Load JSON file 'input.json' into DataFrame df1, normalize nested records, filter rows where 'age' > 0, and save to Parquet 'output.parquet'.", "output": "import pandas as pd\ndf1 = pd.read_json('input.json')\ndf1 = pd.json_normalize(df1.to_dict(orient='records'))\ndf1 = df1[df1['age'] > 0]\ndf1.to_parquet('output.parquet')"}
{"instruction": "Read CSV file 'data.csv' into DataFrame df2, drop duplicates, fill missing values in 'salary' with median, group by 'status' and plot average 'income'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf2 = pd.read_csv('data.csv')\ndf2 = df2.drop_duplicates()\ndf2['salary'] = df2['salary'].fillna(df2['salary'].median())\navg = df2.groupby('status')['income'].mean()\navg.plot(kind='bar')\nplt.title('Average income by status')\nplt.show()"}
{"instruction": "Concatenate multiple CSV files ['time_series.csv', 'data.xlsx', 'input.json'] into df_sales, clean column names to lowercase, drop columns with >50% nulls, and save to 'clean_data.csv'.", "output": "import pandas as pd\nimport glob\nfiles = glob.glob(['time_series.csv', 'data.xlsx', 'input.json'])\ndf_sales = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\ndf_sales.columns = df_sales.columns.str.lower()\nnull_frac = df_sales.isnull().mean()\ncols_to_drop = null_frac[null_frac > 0.5].index\ndf_sales.drop(columns=cols_to_drop, inplace=True)\ndf_sales.to_csv('clean_data.csv', index=False)"}
{"instruction": "Read CSV file 'data.xlsx' into DataFrame dataset, drop duplicates, fill missing values in 'quantity' with median, group by 'class' and plot average 'value'.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('data.xlsx')\ndataset = dataset.drop_duplicates()\ndataset['quantity'] = dataset['quantity'].fillna(dataset['quantity'].median())\navg = dataset.groupby('class')['value'].mean()\navg.plot(kind='bar')\nplt.title('Average value by class')\nplt.show()"}
{"instruction": "Read Excel file 'data.csv' sheet 'Data', concatenate sheets into df_sales, pivot on 'group' and 'speed', then plot heatmap.", "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsheets = pd.read_excel('data.csv', sheet_name=None)\ndf_sales = pd.concat(sheets.values(), ignore_index=True)\npivot = df_sales.pivot_table(index='group', columns='speed', values='speed', aggfunc='mean')\nsns.heatmap(pivot, annot=True)\nplt.show()"}
{"instruction": "Parse 'sale_date' in df_data as datetime, extract year and month, compute monthly sum of 'income', and plot line chart with markers.", "output": "import pandas as pd\nimport matplotlib.pyplot as plt\ndf_data['sale_date'] = pd.to_datetime(df_data['sale_date'])\ndf_data['year_month'] = df_data['sale_date'].dt.to_period('M')\nmonthly = df_data.groupby('year_month')['income'].sum()\nmonthly.plot(marker='o')\nplt.title('Monthly Sum of income')\nplt.show()"}
